

#Structure de CUDA 

##Structure de Programmation

Un programme CUDA est composé de deux composants primaire : un hôte (host) et un device (le GPU). 
Le code Host tourne sur le CPU, tandis que les fonctions kernel tournent sur le GPU.
L'execution du kernel peut être réalisée totalement indépendament de l'execution de l'host.

![](/img/host_device.png "Host and device")	

Une application commence par executer le concernant l'host CPU, ensuite l'host invoque un kernel GPU sur le device GPU.Le GPu execute ce kernel en parallel en utilisant de multiples threads.Lorsque le kernel completeson exécution, le CPU reprend son execution du programme d'origine.
(il est possible d'executer plusieur kernel ou bien de ne pas attendre sa fin pour continuer l'exeuction du host, nous détaillerons ceci dans la partie Stream et synchronisation).

##Kernel

CUDA C hérite du C en permettant aux programmers de définir des fonction C, appelées kernels, qui lorsqu'elle sont appellées, sont executées N fois en parallele par N différents threads CUDA, contrainrement à une fois seulement en C.

###Définir un kernel

Un kenerl est définit en utilisant le mot-clé __global__.

~~~

// Definition du Kernel
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

~~~

###Invoquer un Kernel
Invoquer un kernel GPU est très similaire à l'appel d'une fonction. CUDA utilise une synthaxe à base de chevron afin de configurer et d'exectuer un kernel. 
Il faut donc lui indiquer le nombre de blocs et le nombre de threads à utiliser.

~~~

int main()
{
    ...
    // Invocation de Kernel avec N threads sur 1 bloc
    VecAdd<<<1, N>>>(A, B, C);
    ...
}

~~~

Ici chaqu'un des N threads executant VecAdd() performent une addition.


##Grid

La grid (grille) est le nom d'un kernel actif. Lorsque l'on lance plusieurs kernels, chacun à son propre espace de travail, sa grid

![](/img/grid.png "Grid")

##Blocs
Un bloc est un ensemble limité de threads (multiple de 2).
Les blocs composant une grid sont totalement indépendants. Différents blocs sont assignés à différents microprocesseurs (SM pour streaming multiprocesors).
Plusieurs blocs peuvent être présent sur le même SM, mais un bloc ne peut pas être distribué sur plusieurs SM en même temps.
Les threads utilisés dans un bloc peuvent :
    *Se synchroniser ( à l'aide de la fonction __syncthreads qui agit comme une barriere tant que tout les threads du même bloc ne l'ont pas atteinte)
    *Partager de la mémoire (shared memory)
    *Communiquer

##Parraléllisme

L'indépendance des blocs et des kernel permet à CUDA d'être plus évolutifs et plus flexibles. CUDA peut donc supporter plusieurs formes de parralélisme tel que :
*Parraléllisme au niveau des threads : Différents threads exectuent différentes tâches.
*Parraléllisme au niveau des blocs et des grids : Différents blocs ou grids executent différentes tâches.
*Parraléllisme de données : Différents threads and blocks travaillent sur différentes parties de la mémoire.

##Thread ID

Chaque bloc et chaque thread disposent d'un index qui leur est propre, uniquement accesible depuis un kernel actif.
Les variables blockIdx et threadIdx contiennent ces index.
threadIdx est un vecteur à trois dimension, tandis que blockIdx est un vecteur à 2 dimensions. L'index threadIdx est donc lui-même le bloc de threads.
La taille de la grid et d'un bloc est définie respectivement par les variables gridDim et blockDim.

##Warp

Les warps sont des groupes de threads consécutifs. Un warp est executé par un seul coeur CUDA.
A l'execution, un bloc de threads est divisé en un nombre de warp égal au nombre de coeur CUDA dans le SM.
La taille des warps dépend du matériel, par exemple pour la jetson TK1 chaque warp contient 32 threads.
Ce sont ces warps qui nous permettent d'obtenir un style d'execution SMT (Single Instruction Multiple-Thread).
Contrairement au grids et aux blocs, l'implémentation des warps n'est pas accesible aux programmeurs. Cependant, ce modèle d'execution influe sur les performances (plus de détail dans la section Optimisation).

## Hierarchie de la mémoire 

Les threads CUDA ont accès à différents espaces de la mémoire lors de leurs éxecutions. 
Chaque thread dispose d'un espace mémoire privé qui lui est propre.
Chaque thread peut aussi accéder à un espace de mémoire partagée avec tout les threads du même bloc ainsi qu'à la mémoire globale (partagée par tout les kernels).

![](/img/)

L'hote et le device maintiennent leurs propres espaces mémoires en DRAM, réfférés en temps qu'host memory et device memory.

##Thrust vector

Thrust est une librarie basée sur la STL (Standard Template Library). Elle offre une interface haut-niveau permettant de programmer plus facilement des applications CUDA.

Thrust fournit deux containeurs vectoriels : host_vector (stocké sur la mémoiré de l'host) et device_vector (stocké sur la mémoire du GPU).
Ces vecteurs fonctionnent de la même manière que les std::vector disponibles en C++ STL : 
ce sont des conteneurs géneriques (tout type de donnée) pouvant être redimensionnés dynamiquement.
Lorsqu'une fonction Thrust est appellée il vérifie automatique le type d'itérateur utilisé pour déterminer si il doit utiliser une implémentation host ou device.
Ce processus est connu sous le nom de "static dispatching" car  l"implémentation host/device est determinée durant la compilation.

Nous pouvons utiliser des raw_pointers pour accèder aux données sans utiliser les itérateurs.

##Capacité de calcul

La capacité de calcul d'un appareil est représenté par son numéro de version (parfois appellé "SM version").
Ce numéro de version permet d'identifier le nombre de feature supporté par le hardware GPU et est utilisé par les application à l'éxecution pour déterminer les outils et instructions diposnibles pour le GPU.

Les appareils ayant la même architecture diposent du même niveau de revision.
Liste des capacités selon les versions : http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities

