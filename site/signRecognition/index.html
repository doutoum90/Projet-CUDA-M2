<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Mathias DIDIER">
  <link rel="shortcut icon" href="../favicon.ico">
  
  <title>Reconnaissance des signes - Développement d’applications sous processeur graphique (GPU / CUDA).</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Reconnaissance des signes";
    var mkdocs_page_input_path = "signRecognition.md";
    var mkdocs_page_url = "/signRecognition/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Développement d’applications sous processeur graphique (GPU / CUDA).</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../introduction/">Introduction</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../tk1/">Jetson TK1</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../connexions/">Connexions</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../cpu_gpu/">CPU/GPU</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../architecture/">Architectures</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../cuda/">CUDA</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../frameworks/">Frameworks</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../tensorflow/">Tensorflow</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../applications/">Applications</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../traitementImage/">Traitement d'image</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../optimisation/">Optimisation GPU</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../reduction/">Optimisation/Reduction</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Reconnaissance des signes</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#langage-des-signes">Langage des signes</a></li>
                
                    <li><a class="toctree-l4" href="#premiere-iteration">Première itération</a></li>
                
            
                <li class="toctree-l3"><a href="#deuxieme-iteration">Deuxième itération</a></li>
                
                    <li><a class="toctree-l4" href="#traitements-dimage-utilises">Traitements d'image utilisés</a></li>
                
                    <li><a class="toctree-l4" href="#cascade-haar-viola-and-jones">Cascade Haar / Viola and Jones</a></li>
                
                    <li><a class="toctree-l4" href="#definition-du-besoin">Définition du besoin</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../ressources/">Ressources</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../outils/">Outils</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../about/">About</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../sceances/">Scéances</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../installations/">Installations</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Développement d’applications sous processeur graphique (GPU / CUDA).</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Reconnaissance des signes</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/matEhickey/Projet-CUDA-M2/edit/master/docs/signRecognition.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="langage-des-signes">Langage des signes</h1>
<p>Mise en place d'un classifieur d'images pour distinguer les différentes lettres du langages des signes francais, et en réaliser une transcription automatique.</p>
<p>Il sera, dans un premier temps, classifié les lettres distinguables sans mouvements (le traitement d'images étant moins complexe que le traitement vidéo), et sans les mots/phrases qui possèdent un signe/mouvements chacuns</p>
<h2 id="premiere-iteration">Première itération</h2>
<h3 id="outils">Outils</h3>
<p>Tensorflow<br />
<a href="https://sites.google.com/site/autosignlan/source/image-data-set">Dataset</a><br />
Training : <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py">retrain.py</a><br />
Eval : <a href="https://github.com/llSourcell/tensorflow_image_classifier/blob/master/src/py/label_dir.py">label.py</a>  </p>
<h3 id="workflow">Workflow</h3>
<p>Utilisation de docker pour développement indépendant de la machine hôte (dans mon cas, depuis windows).<br />
Le probleme de docker est qu'il ne peut tirer parti du GPU (sur windows!, sur les systemes compatibles, il existe <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a>, à tester sur la Jetson donc.)  </p>
<h4 id="docker-quick-start-terminal">Docker (quick start terminal):</h4>
<pre><code>docker run -it gcr.io/tensorflow/tensorflow:latest-devel   # Crée un container docker qui contient la derniere version de tensorflow/tensorflow  
mkdir /tf_files   # Crée un dossier pour récuperer les fichiers utiles  
exit   # Quite la vm pour y lier les tf_files

docker run -it -v $HOME/tf_files:/tf_files gcr.io/tensorflow/tensorflow:latest-devel   # lance le container avec /tf_files en commun avec le pc

# ouverture du terminal du container (si besoin):  
# docker attach &lt;name&gt;

# Lance l'entrainement sur les fichiers jpg du dossier /tf_files/signSplit      
cd /tensorflow
python tensorflow/examples/image_retraining/retrain.py \
    --bottleneck_dir=/tf_files/bottlenecks \
    --how_many_training_steps 4000 \
    --model_dir=/tf_files/inception \
    --output_graph=/tf_files/retrained_graph.pb \
    --output_labels=/tf_files/retrained_labels.txt \
    --image_dir /tf_files/signSplit

# Lance la prédiction
python /tf_files/label.py /tf_files/sign/&lt;number&gt;.jpg
</code></pre>
<h4 id="contenu-de-tf_files">Contenu de tf_files</h4>
<pre><code>sign/  
    A/  
    B/  
    C/  
signSplit/  
    A/  
    B/  
    C/
label_image.py  
modif.py   # Renomme les fichiers de train  
splitData.py   # Coupe les données de train
</code></pre>
<p>convertion images ppm en jpg</p>
<pre><code># install imagemagick 
sudo apt-get install imagemagick
convert *.ppm %d.jpg
</code></pre>
<h3 id="premiers-resultats">Premiers Résultats</h3>
<p>Une classification sur 3 signes distincts avec environ 100 images pour chaque cas d'entrainement mene a une exactitude (final test accuracy) de 94%  </p>
<p>D'autres campagnes de tests automatisés seronts à réaliser pour produire des schémas de performances de ce modeles, pour essayer d'en trouver les faiblesses dans notre contexte de la reconnaissance du langages des signes, pour enfin essayer d'augmenter les performances de classification. 
Les tests s'avèrents relativememnt bons sur le meme dataset (mais des images qu'il n'as pas appris), cependant, apres un test sur une image de l'exterieur du dataset, les resultats sont médiocres.<br />
Nous avons mis aux point un script de generation de dataset a l'aide de la webcam (linux uniquement, paquets streamer et tkinter necéssaires).<br />
A tester donc.</p>
<h3 id="a-voir-pour-la-deuxieme-experimentation">A voir pour la deuxieme expérimentation</h3>
<p><a href="https://github.com/NVIDIA/nvidia-docker/wiki/Why%20NVIDIA%20Docker">Nvidia docker</a><br />
Installation de docker sur ubuntu <a href="https://docs.docker.com/engine/installation/linux/ubuntulinux/">3 versions</a> ou <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-16-04">16.04</a><br />
<a href="https://github.com/NVIDIA/nvidia-docker">Installation de nvidia-docker</a><br />
<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker">Tensorflow image on nvidia-docker</a>  </p>
<h1 id="deuxieme-iteration">Deuxième itération</h1>
<p>Nos premiers essais de classifications des signes étaient un echec, cependant nous nous attendions a ce résultat, mais il fallait commencer par la pour avancer.<br />
L'idée était de mettre en place un classifier d'image quelquonque (nous avons utilisé le (...retail?) de tensorflow) sur un dataset trouvé sur internet comportant des images de signes.<br />
Nous avons fait des essais sur des données d'apprentissages variés, mais peu nombreuses (50aine de photos, séparés en 3 signes distincts.)
En séparant le corpus en corpus de tests et d'apprentissage, les résultats semblait plutot bons (~60%), cependant, les essais réels était médiocre (non mesurés, mais je dirai maximum 5 % de bonnes classifications).<br />
Nous avons déduit de cette expérience que les données d'apprentissages importe énormement sur la classification, et que les conditions/environements de capture sont très influents.<br />
Il nous faudra donc faire du traitement d'image, pour, a minima retirer le maximum de fond de l'image, et si possible, couper les parties de l'image des mains pour les isoler, et y appliquer des traitements, pour le training, et la classification.<br />
On espere aussi réduire la taille des données en entrée du systeme pour permetre des calculs plus rapide.</p>
<h2 id="traitements-dimage-utilises">Traitements d'image utilisés</h2>
<h3 id="grayscale">Grayscale</h3>
<p>Le passage des images en niveau de gris permet de grandement diminuer la taille de celle-ci, en revanche ce traitement est destructif pour des informations qui peuvent être capitale.<br />
On appliquera ainsi ce traitement, mais pas forcément au début de la chaine de traitement.  </p>
<h3 id="blur-gauss">Blur-Gauss</h3>
<p>Application d'un filtre qui permet de flouter legerement l'image. Cela nous permet de diminuer les détails bruités de l'images, pour se concentrer sur les formes plus importantes.<br />
<img alt="Blur" src="../img/blur.jpg" /></p>
<h3 id="bg-remove">BG-Remove</h3>
<p>Ce traitement permet d'extraire l'information qui n'est pas immobile sur l'image.<br />
Elle se base sur la capture de plusieurs trames, et une soustraction des pixels qui n'ont pas changé, avec un certain controle par des parametres de réglages.<br />
Cette technique consomme relativement peu de ressource par rapport au gain d'espace de travail utile.<br />
L'utilisation de cette technique permet également un affinage en fonction du temps. En effet, lorsque l'utilisateur arrive dans l'image, la zone analysé est assez grande, mais celle ci se reduit très vite car l'utilisateur bouge peu ( relativement a la main qu'il présente a la caméra )  </p>
<h3 id="contoursconvex-hullboundingbox">Contours/Convex Hull/Boundingbox</h3>
<p>L'algorithme de recherche des contours permet de trouver des zones des couleurs semblables, et tracer leurs coutours.  </p>
<p><img alt="Contours" src="../img/contours.png" /></p>
<p>A partir de ces resultats, on peut extraire les convex-hull. Il s'agit de détourer au mieux une forme, en retirant ainsi beaucoup d'information (les coutours détaillés) en conservant celle qui nous intéressents.<br />
<img alt="ConvexHull" src="../img/convexHull.png" />  </p>
<p>A partir de ces zones sont facilement extrayable les boudings-box (zones rectangle ou éllipsoïdales), qui bornent les zones qui nous interressents. En dilattant les convex hull, ou les bounding-box, on permet d'etre plus large sur l'image que l'on va scrop.  </p>
<p><img alt="BoundingBox" src="../img/Bounding1.png" />
<img alt="BoundingBox2" src="../img/Bounding2.png" />  </p>
<h2 id="cascade-haar-viola-and-jones">Cascade Haar / Viola and Jones</h2>
<p>L'agorithme de detection en cascade est une technique de reconnaissance d'objet dans une image.<br />
Cet algorithme est utilisé depuis 2001 pour la localisation de visages dans les images. Cependant, il peut apprendre a localiser n'importe quel type d'objets, et de trouver les bounding boxes des objets qu'on lui aura fait apprendre.  </p>
<p>Elle se base sur le calcul des zones de couleurs d'ondelettes, et un entrainement permettant de déduire un modele de vraisemblance pour ces ondelettes.<br />
Le calcul semble a premiere vu énorme, mais en convertissant dans un format addapté pour ce calcul, il se fait en 4 opérations basiques.  </p>
<p>Ainsi il nous permettrait, soit de remplacer notre modele précédent (bg remove/convexhull, meme si aucun traitement concernant les mains n'était encore incorporé), soit le renforcer (avant ou apres, ou en prenant une décision par rapport aux réponses des 2 systemes.)  </p>
<p>De plus, les versions a l'état de l'art l'utilisent de facon cascade (Cascade Classifier), ce qui nous permettrait aussi de faire la reconnaissance à proprement parler des signes de la main.  </p>
<p><img alt="Viola1" src="../img/viola.png" /> 
<img alt="Viola2" src="../img/viola2.png" /> 
<img alt="Viola3" src="../img/viola3.png" /> </p>
<h3 id="implementation-python-et-gpu">Implémentation Python et GPU:</h3>
<p>Avec les librairiesPython <a href="https://github.com/Itseez/opencv">OpenCV</a> et <a href="https://github.com/alexanderkoumis/opencv-gpu-py">OpenCV GPU</a>, détecter des objets avec un fichier d'entrainement est très facile:  </p>
<pre><code>if cv2gpu.is_cuda_compatible():  
    cv2gpu.init_gpu_detector(cascade_file_gpu)  # training file

detections = cv2gpu.find_faces(image_file)
</code></pre>
<h2 id="definition-du-besoin">Définition du besoin</h2>
<p>Il nous maintenant faire des essais de la meilleure combinaison de systemes.<br />
L'impact du sens d'application des traitements étant importants et peu prévisible, cela demanderais trop de temps et nous allons faire un maximum d'essais, en fonction de nos prévisions.<br />
Cependant rien ne nous indique que nous optiendrons le meilleur modèle.  </p>
<p>Il faut enfin créer un programme de création de dataset a l'aide de webcam, pour obtenir plus de données d'entrainement.<br />
Ce programme n'étant pas a priori compliqué, il faut néanmoins qu'il soit facilement parametrable pour créer des datasets de tailles variables (pour nos tests, et pour un déployement imaginable).<br />
On pourrait nottament penser a une interface de parametrage par l'utilisateur de ces datasets. Il pourrait ainsi ajouter lui meme des signes dont nous ignorons l'existence.</p>
<p>Il nous faut définir des campagnes de tests adaptés pour juger nos systèmes, et nos datasets, cependant, je ne pense pas qu'on ai le temps de tester les datasets ( j'imagine qu'en comparant la vraisemblance avec les modeles déja appris on pourrais filtrer les cas ou l'utilisateur s'es trompé, mais on sors du  cadre du projet). En revanche, il faut qu'on puisse tester automatiquement les systemes. </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../ressources/" class="btn btn-neutral float-right" title="Ressources">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../reduction/" class="btn btn-neutral" title="Optimisation/Reduction"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/matEhickey/Projet-CUDA-M2" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../reduction/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../ressources/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
