{
    "docs": [
        {
            "location": "/", 
            "text": "Le but de ce projet est de d\u00e9couvrir le pipeline de d\u00e9velopement d'application sur GPU, si possible pour des applications embarqu\u00e9s,\navec la carte Nvidia TK1, et proposer des projets r\u00e9alisables exploitant au mieux nos nouvelles connaissances.\n\n\nIntitul\u00e9 du projet\n\n\nIntitul\u00e9 du projet\n\n\nLa diff\u00e9rence de performance entre les CPU et les GPU est due \u00e0 la parall\u00e9lisation des calculs.\n\n\nLes programmes s'\u00e9xecutant de maniere classique, cad par un processeurs, se font majoritairement\n\n\nde mani\u00e8re s\u00e9quencielle, malgr\u00e9 les efforts des processeurs multi-coeurs et des programmes multithread\u00e9s.\n\n\nLe GPU r\u00e9pond a ce probl\u00e8me en proposant un parall\u00e9lisme extr\u00eame, passant par un nouveau\n\n\nparadigme de programmation, car d\u00e9pendant de l'impl\u00e9mentation mat\u00e9rielle de la carte de calcul.\n\n\nAinsi, un GP peut \u00eatre repr\u00e9sent\u00e9 par un grand nombre de processeurs coll\u00e9 les uns \u00e0 cot\u00e9 des autres,\n\n\nfaisant des calculs simple, mais de mani\u00e8re totalement parall\u00e8le.\n  \n\n\nL\u2019objectif final du projet est de tendre vers l\u2019\u00e9tude de la faisabilit\u00e9 et la\n\n\npertinence d \u2019un portage C/C++ vers le langage CUDA. (Simulation vers Syst\u00e8me).\n\n\nYou \ncan\n combine bold and italic\n\nezfezfzefz", 
            "title": "Home"
        }, 
        {
            "location": "/#intitule-du-projet", 
            "text": "", 
            "title": "Intitul\u00e9 du projet"
        }, 
        {
            "location": "/#intitule-du-projet_1", 
            "text": "La diff\u00e9rence de performance entre les CPU et les GPU est due \u00e0 la parall\u00e9lisation des calculs.  Les programmes s'\u00e9xecutant de maniere classique, cad par un processeurs, se font majoritairement  de mani\u00e8re s\u00e9quencielle, malgr\u00e9 les efforts des processeurs multi-coeurs et des programmes multithread\u00e9s.  Le GPU r\u00e9pond a ce probl\u00e8me en proposant un parall\u00e9lisme extr\u00eame, passant par un nouveau  paradigme de programmation, car d\u00e9pendant de l'impl\u00e9mentation mat\u00e9rielle de la carte de calcul.  Ainsi, un GP peut \u00eatre repr\u00e9sent\u00e9 par un grand nombre de processeurs coll\u00e9 les uns \u00e0 cot\u00e9 des autres,  faisant des calculs simple, mais de mani\u00e8re totalement parall\u00e8le.     L\u2019objectif final du projet est de tendre vers l\u2019\u00e9tude de la faisabilit\u00e9 et la  pertinence d \u2019un portage C/C++ vers le langage CUDA. (Simulation vers Syst\u00e8me).  You  can  combine bold and italic \nezfezfzefz", 
            "title": "Intitul\u00e9 du projet"
        }, 
        {
            "location": "/introduction/", 
            "text": "Introduction\n\n\nLa carte NVIDIA TK1 est une carte \u00e9lectronique embarqu\u00e9, poss\u00e9dant une carte graphique puissante pour des applications requ\u00e9rant beaucoup de calculs, dans un temps minime.\n\n\nLes possibilit\u00e9s sont nombreuses, car la capacit\u00e9 de calcul de ce type de cartes permet des applications, notamment dans le domaine du traitement\net de la reconnaissance d'image dans la robotique (en temps r\u00e9el). Des applications en traitement de la parole ou en traductions, en simulations physiques etc..,\nsont aussi envisageable, tant ces domaines n\u00e9cessitent de ressources.", 
            "title": "Introduction"
        }, 
        {
            "location": "/introduction/#introduction", 
            "text": "La carte NVIDIA TK1 est une carte \u00e9lectronique embarqu\u00e9, poss\u00e9dant une carte graphique puissante pour des applications requ\u00e9rant beaucoup de calculs, dans un temps minime.  Les possibilit\u00e9s sont nombreuses, car la capacit\u00e9 de calcul de ce type de cartes permet des applications, notamment dans le domaine du traitement\net de la reconnaissance d'image dans la robotique (en temps r\u00e9el). Des applications en traitement de la parole ou en traductions, en simulations physiques etc..,\nsont aussi envisageable, tant ces domaines n\u00e9cessitent de ressources.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tk1/", 
            "text": "Nvidia Jetson TK1\n\n\nCarte semblable au cartes RaspberryPi, a l'exception que la TK1 contient une carte graphique (GPU), un port SATA, mini-PCIe et un ventilateur.\n\nLa Jetson TK1 mesure 5\" x 5\" (127mm x 127mm).  \n\n\nSa puissance lui vient de son composant CPU/GPU/ISP: Le Tegra K1 SOC.\n\nCe dernier contient un GPU NVIDIA Kepler avec 192 c\u0153urs NVIDIA CUDA, et un processeur (CPU) quad-core NVIDIA 4-Plus-1\u2122 ARM  Cortex-A15.\n\nIl possede une m\u00e9moire DRAM de Go en DDR3L.\n\nIl est aliment\u00e9 en 12V DC, pour une consomation d'\u00e9nergie moyenne comprise entre 1 et 5 W.  \n\n\n\n\nFront panel\u00a0:\n Il contr\u00f4le les boutons d'alimentation et de r\u00e9initialisation et les LED qui permettent d'obtenir des informations sur l'\u00e9tat de l'ordinateur et l'utilisation du disque dur.\n\n\nVert =\n  Ce sont les LED d'alimentation qui indiquent si la jetson TK1 est allum\u00e9, \u00e9teinte ou en veille.\n\n\nOranges =\n Ce sont les LED du disque dur, elles indiquent si le disque dur est en mode \u00e9criture ou lecture\n\n\nRouge =\n  Contr\u00f4le le bouton d'alimentation qui permet d'allumer ou \u00e9teindre la jetson TK1\n\n\nBleu =\n G\u00e8re le bouton de r\u00e9initialisation pour red\u00e9marrer l'ordinateur\n\n\nDB-9\u00a0:\n Le connecteur DB-9 est une prise analogique comportant 9 broches. Il sert essentiellement dans les liaisons permettant la transmission de donn\u00e9es asynchrone. Il \u00e9tait beaucoup utilis\u00e9 pour des p\u00e9riph\u00e9riques comme les claviers et les souris. Ce type de port a \u00e9t\u00e9 remplac\u00e9 par les ports PS/2 et USB de nos jours.\n\n\nHDMI\u00a0:\n Abr\u00e9viation de High Definition Multimedia Interface (Interface Multim\u00e9dia Haute D\u00e9finition) est un connecteur et un c\u00e2ble capable de transmettre des flux vid\u00e9os et audios de haute qualit\u00e9 et \u00e0 bande passante \u00e9lev\u00e9e Il permet notamment de relier une source vid\u00e9o/audio comme un ordinateur \u00e0 un dispositif compatible comme un \u00e9cran HD.\n\n\nUSB\u00a0:\n Abr\u00e9viation de Universal Serial Bus (Bus Universel en S\u00e9rie) est une interface de type plug-and-play qui permet \u00e0 la jestson TK1 de communiquer avec d'autres p\u00e9riph\u00e8riques. Les p\u00e9riph\u00e9riques USB peuvent \u00eatre une souris, un clavier, une cl\u00e9 USB...\n\n\nPCIe Ethernet\u00a0:\n C'est le port qui permet de brancher un c\u00e2ble RJ45 pour pouvoir acc\u00e9der \u00e0 internet.\n\n\nHeadphone/microphone\u00a0:\n C'est une prise jack qui est le connecteur le plus utilis\u00e9 pour la connectique des petits \u00e9quipements audio comme les casques et micros.\n\n\nUSB micro-B recovery port\u00a0:\n C'est un port micro USB qui permet de connecter la jetson TK1 au PC h\u00f4te pour la lancer en recovery mode.\n\n\nMini PCIe\u00a0:\n Le port mini PCIe est une version plus petite du port PCIe, il est souvent utilis\u00e9 pour les ordinateurs portables et autres p\u00e9riph\u00e9riques portables.\n\n\nJTAG\u00a0:\n Ce port peut \u00eatre utilis\u00e9 pour connecter une sonde \u00e0 la carte pour identifier les d\u00e9fauts, ce qui permet \u00e0 un d\u00e9veloppeur de manipuler la carte. Les d\u00e9veloppeurs peuvent \u00e9galement l'utiliser pour copier le firmware de la carte.\n\n\nExpansion I/O\u00a0:\n Ce sont les ports GPIO (General Purpose Input/Output, litt\u00e9ralement Entr\u00e9e/Sortie \npour un Usage G\u00e9n\u00e9ral), ils permettent de pouvoir communiquer avec d'autres circuits \u00e9lectroniques.\n\n\nSD card\u00a0:\n C'est un port pour les cartes SD (Abr\u00e9g\u00e9 de Secure Digital) qui sont des cartes m\u00e9moires de stockage de donn\u00e9es num\u00e9riques.\n\n\nPower\u00a0:\n C'est la prise d'alimentation 12 volts pour alimenter la carte.\n\n\nSATA\u00a0:\n C'est le port SATA (Abr\u00e9g\u00e9 de Serial Advanced Technology Attachment), c'est une norme qui sp\u00e9cifie notamment un format de transfert de donn\u00e9es et un format de c\u00e2ble. Ce type de port permet notamment de brancher un disque dur. Celui-ci n'est pas hot-plug c'est \u00e0 dire qu'il faut que la carte soit \u00e9teinte pour brancher le p\u00e9riph\u00e8rique.\n\n\nSoft power\u00a0:\n Bouton pour d\u00e9marrer la carte.\n\n\nReset\u00a0:\n Bouton pour r\u00e9initialiser la carte.\n\n\nForce recovery\u00a0:\n Bouton pour forcer la carte \u00e0 se lancer en recovery mode.\n\n\nSp\u00e9cifications GPU:\n\n\n\nresult of deviceQuery on jetson tk1 \n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \nGK20A\n\n  CUDA Driver Version / Runtime Version          6.5 / 6.5\n  CUDA Capability Major/Minor version number:    3.2\n  Total amount of global memory:                 1892 MBytes (1984385024 bytes)\n  ( 1) Multiprocessors, (192) CUDA Cores/MP:     192 CUDA Cores\n  GPU Clock rate:                                852 MHz (0.85 GHz)\n  Memory Clock rate:                             924 Mhz\n  Memory Bus Width:                              64-bit\n  L2 Cache Size:                                 131072 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 32768\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n  Run time limit on kernels:                     No\n  Integrated GPU sharing Host Memory:            Yes\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device PCI Bus ID / PCI location ID:           0 / 0\n  Compute Mode:\n     \n Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) \n\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GK20A\nResult = PASS\n\n\n\n\nSource : \nWiki Jetson TK1", 
            "title": "Jetson TK1"
        }, 
        {
            "location": "/tk1/#nvidia-jetson-tk1", 
            "text": "Carte semblable au cartes RaspberryPi, a l'exception que la TK1 contient une carte graphique (GPU), un port SATA, mini-PCIe et un ventilateur. \nLa Jetson TK1 mesure 5\" x 5\" (127mm x 127mm).    Sa puissance lui vient de son composant CPU/GPU/ISP: Le Tegra K1 SOC. \nCe dernier contient un GPU NVIDIA Kepler avec 192 c\u0153urs NVIDIA CUDA, et un processeur (CPU) quad-core NVIDIA 4-Plus-1\u2122 ARM  Cortex-A15. \nIl possede une m\u00e9moire DRAM de Go en DDR3L. \nIl est aliment\u00e9 en 12V DC, pour une consomation d'\u00e9nergie moyenne comprise entre 1 et 5 W.     Front panel\u00a0:  Il contr\u00f4le les boutons d'alimentation et de r\u00e9initialisation et les LED qui permettent d'obtenir des informations sur l'\u00e9tat de l'ordinateur et l'utilisation du disque dur.  Vert =   Ce sont les LED d'alimentation qui indiquent si la jetson TK1 est allum\u00e9, \u00e9teinte ou en veille.  Oranges =  Ce sont les LED du disque dur, elles indiquent si le disque dur est en mode \u00e9criture ou lecture  Rouge =   Contr\u00f4le le bouton d'alimentation qui permet d'allumer ou \u00e9teindre la jetson TK1  Bleu =  G\u00e8re le bouton de r\u00e9initialisation pour red\u00e9marrer l'ordinateur  DB-9\u00a0:  Le connecteur DB-9 est une prise analogique comportant 9 broches. Il sert essentiellement dans les liaisons permettant la transmission de donn\u00e9es asynchrone. Il \u00e9tait beaucoup utilis\u00e9 pour des p\u00e9riph\u00e9riques comme les claviers et les souris. Ce type de port a \u00e9t\u00e9 remplac\u00e9 par les ports PS/2 et USB de nos jours.  HDMI\u00a0:  Abr\u00e9viation de High Definition Multimedia Interface (Interface Multim\u00e9dia Haute D\u00e9finition) est un connecteur et un c\u00e2ble capable de transmettre des flux vid\u00e9os et audios de haute qualit\u00e9 et \u00e0 bande passante \u00e9lev\u00e9e Il permet notamment de relier une source vid\u00e9o/audio comme un ordinateur \u00e0 un dispositif compatible comme un \u00e9cran HD.  USB\u00a0:  Abr\u00e9viation de Universal Serial Bus (Bus Universel en S\u00e9rie) est une interface de type plug-and-play qui permet \u00e0 la jestson TK1 de communiquer avec d'autres p\u00e9riph\u00e8riques. Les p\u00e9riph\u00e9riques USB peuvent \u00eatre une souris, un clavier, une cl\u00e9 USB...  PCIe Ethernet\u00a0:  C'est le port qui permet de brancher un c\u00e2ble RJ45 pour pouvoir acc\u00e9der \u00e0 internet.  Headphone/microphone\u00a0:  C'est une prise jack qui est le connecteur le plus utilis\u00e9 pour la connectique des petits \u00e9quipements audio comme les casques et micros.  USB micro-B recovery port\u00a0:  C'est un port micro USB qui permet de connecter la jetson TK1 au PC h\u00f4te pour la lancer en recovery mode.  Mini PCIe\u00a0:  Le port mini PCIe est une version plus petite du port PCIe, il est souvent utilis\u00e9 pour les ordinateurs portables et autres p\u00e9riph\u00e9riques portables.  JTAG\u00a0:  Ce port peut \u00eatre utilis\u00e9 pour connecter une sonde \u00e0 la carte pour identifier les d\u00e9fauts, ce qui permet \u00e0 un d\u00e9veloppeur de manipuler la carte. Les d\u00e9veloppeurs peuvent \u00e9galement l'utiliser pour copier le firmware de la carte.  Expansion I/O\u00a0:  Ce sont les ports GPIO (General Purpose Input/Output, litt\u00e9ralement Entr\u00e9e/Sortie \npour un Usage G\u00e9n\u00e9ral), ils permettent de pouvoir communiquer avec d'autres circuits \u00e9lectroniques.  SD card\u00a0:  C'est un port pour les cartes SD (Abr\u00e9g\u00e9 de Secure Digital) qui sont des cartes m\u00e9moires de stockage de donn\u00e9es num\u00e9riques.  Power\u00a0:  C'est la prise d'alimentation 12 volts pour alimenter la carte.  SATA\u00a0:  C'est le port SATA (Abr\u00e9g\u00e9 de Serial Advanced Technology Attachment), c'est une norme qui sp\u00e9cifie notamment un format de transfert de donn\u00e9es et un format de c\u00e2ble. Ce type de port permet notamment de brancher un disque dur. Celui-ci n'est pas hot-plug c'est \u00e0 dire qu'il faut que la carte soit \u00e9teinte pour brancher le p\u00e9riph\u00e8rique.  Soft power\u00a0:  Bouton pour d\u00e9marrer la carte.  Reset\u00a0:  Bouton pour r\u00e9initialiser la carte.  Force recovery\u00a0:  Bouton pour forcer la carte \u00e0 se lancer en recovery mode.", 
            "title": "Nvidia Jetson TK1"
        }, 
        {
            "location": "/tk1/#specifications-gpu", 
            "text": "result of deviceQuery on jetson tk1 \n\nDetected 1 CUDA Capable device(s)\n\nDevice 0:  GK20A \n  CUDA Driver Version / Runtime Version          6.5 / 6.5\n  CUDA Capability Major/Minor version number:    3.2\n  Total amount of global memory:                 1892 MBytes (1984385024 bytes)\n  ( 1) Multiprocessors, (192) CUDA Cores/MP:     192 CUDA Cores\n  GPU Clock rate:                                852 MHz (0.85 GHz)\n  Memory Clock rate:                             924 Mhz\n  Memory Bus Width:                              64-bit\n  L2 Cache Size:                                 131072 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 32768\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n  Run time limit on kernels:                     No\n  Integrated GPU sharing Host Memory:            Yes\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device PCI Bus ID / PCI location ID:           0 / 0\n  Compute Mode:\n       Default (multiple host threads can use ::cudaSetDevice() with device simultaneously)  \n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GK20A\nResult = PASS  Source :  Wiki Jetson TK1", 
            "title": "Sp\u00e9cifications GPU:"
        }, 
        {
            "location": "/connexions/", 
            "text": "Connexions \u00e0 la carte\n\n\nPour le d\u00e9veloppement, nous utilisions la carte avec un \u00e9cran et un clavier mat\u00e9riel.\n\nCependant, comme nous travaillions des fois \u00e0 plusieurs, nous utilisions diff\u00e9rents moyens d'acc\u00e9der a la carte \u00e0 distance.  \n\n\nSSH\n\n\nSSH est un type de connexion s\u00e9curis\u00e9, pour utiliser la carte depuis son terminal.\n\nCela permet de compiler/executer des programmes textuels sans probl\u00e8mes.\n\nPour les applications graphiques, l'option de connexion -X permet d'avoir l'affichage des interfaces sur le pc client, sauf dans certains cas, ou un \u00e9cran physique est requis.\n\nPour transferer des fichiers, nous utilisions scp.\n\nA noter que l'IP par default sur la Jetson TK1 sous L4T_OS est \ntegra-ubuntu\n. Ce qui \u00e9vite d'avoir a scanner son r\u00e9seau, ou a utiliser l'interface pour determiner l'IP.  \n\n\nExemples\n  \n\n\n# Simple connexion SSH\nssh -X ubuntu@tegra-ubuntu\n\n# Connexion SSH avec ( si possible ) une interface graphique\nssh -X ubuntu@tegra-ubuntu\n\n# Upload\nscp file ipDistant:path\n# Download\nscp ipDistant:file path\n\n\n\nWiki SSH\n\n\nHow to SCP\n\n\nVNC\n\n\nVNC est un type de connexion qui permet l'affichage de  l'\u00e9cran de l'ordinateur h\u00f4te.\n\nIl permet de lancer les applications graphiques que ssh -X ne permet pas d'\u00e9xecuter.\n\nCependant, comme les images de l'\u00e9cran sont envoy\u00e9s par le r\u00e9seau, cela peut \u00eatre lent, ou gourmand en ressources pour le pc hote.\n\nCertains clients VNC fournissents des options pour limiter la r\u00e9solution de l'echange, permettant un meilleur framerate, mais une qualit\u00e9 d'image amoindris.\n\nIl est possible de prendre controle du pc distant totalement, ou si on prefere, cela peut se faire en parralele, et il peut donc y avoir plusieurs utilisateurs en m\u00eame temps. \nUne solution qui fonctionne bien est \nx11vnc\n\n\nWiki VNC\n  \n\n\nSerial\n\n\nIl est \u00e9galement possible de communiquer avec la carte par le port serial.\n\nCeci permet un acces a la caret pour le developement, ou la communication entre diff\u00e9rentes machines. \n\n\nVoir\n\n\nWiki Serial Communication\n  \n\n\nProgrammation distribu\u00e9e / M\u00e9thodes distantes\n\n\nLa carte Jetson TK1, au m\u00eame titre que le Raspberry, peut \u00eatre utilis\u00e9 comme n'importe quel ordinateur.\nAinsi, elle peut executer des programmes distribu\u00e9s, quelque soit la configuration. La carte peut \u00eatre un serveur, ou le client d'appels \u00e0 des m\u00e9thodes dites \"distantes\", comme Java RMI, les requetes RPC, ou d'autres \noutils similaires\n.  \n\n\nLien vers les exemples (lien non fonctionel)", 
            "title": "Connexions"
        }, 
        {
            "location": "/connexions/#connexions-a-la-carte", 
            "text": "Pour le d\u00e9veloppement, nous utilisions la carte avec un \u00e9cran et un clavier mat\u00e9riel. \nCependant, comme nous travaillions des fois \u00e0 plusieurs, nous utilisions diff\u00e9rents moyens d'acc\u00e9der a la carte \u00e0 distance.", 
            "title": "Connexions \u00e0 la carte"
        }, 
        {
            "location": "/connexions/#ssh", 
            "text": "SSH est un type de connexion s\u00e9curis\u00e9, pour utiliser la carte depuis son terminal. \nCela permet de compiler/executer des programmes textuels sans probl\u00e8mes. \nPour les applications graphiques, l'option de connexion -X permet d'avoir l'affichage des interfaces sur le pc client, sauf dans certains cas, ou un \u00e9cran physique est requis. \nPour transferer des fichiers, nous utilisions scp. \nA noter que l'IP par default sur la Jetson TK1 sous L4T_OS est  tegra-ubuntu . Ce qui \u00e9vite d'avoir a scanner son r\u00e9seau, ou a utiliser l'interface pour determiner l'IP.    Exemples     # Simple connexion SSH\nssh -X ubuntu@tegra-ubuntu\n\n# Connexion SSH avec ( si possible ) une interface graphique\nssh -X ubuntu@tegra-ubuntu\n\n# Upload\nscp file ipDistant:path\n# Download\nscp ipDistant:file path  Wiki SSH  How to SCP", 
            "title": "SSH"
        }, 
        {
            "location": "/connexions/#vnc", 
            "text": "VNC est un type de connexion qui permet l'affichage de  l'\u00e9cran de l'ordinateur h\u00f4te. \nIl permet de lancer les applications graphiques que ssh -X ne permet pas d'\u00e9xecuter. \nCependant, comme les images de l'\u00e9cran sont envoy\u00e9s par le r\u00e9seau, cela peut \u00eatre lent, ou gourmand en ressources pour le pc hote. \nCertains clients VNC fournissents des options pour limiter la r\u00e9solution de l'echange, permettant un meilleur framerate, mais une qualit\u00e9 d'image amoindris. \nIl est possible de prendre controle du pc distant totalement, ou si on prefere, cela peut se faire en parralele, et il peut donc y avoir plusieurs utilisateurs en m\u00eame temps. \nUne solution qui fonctionne bien est  x11vnc  Wiki VNC", 
            "title": "VNC"
        }, 
        {
            "location": "/connexions/#serial", 
            "text": "Il est \u00e9galement possible de communiquer avec la carte par le port serial. \nCeci permet un acces a la caret pour le developement, ou la communication entre diff\u00e9rentes machines.   Voir  Wiki Serial Communication", 
            "title": "Serial"
        }, 
        {
            "location": "/connexions/#programmation-distribuee-methodes-distantes", 
            "text": "La carte Jetson TK1, au m\u00eame titre que le Raspberry, peut \u00eatre utilis\u00e9 comme n'importe quel ordinateur.\nAinsi, elle peut executer des programmes distribu\u00e9s, quelque soit la configuration. La carte peut \u00eatre un serveur, ou le client d'appels \u00e0 des m\u00e9thodes dites \"distantes\", comme Java RMI, les requetes RPC, ou d'autres  outils similaires .    Lien vers les exemples (lien non fonctionel)", 
            "title": "Programmation distribu\u00e9e / M\u00e9thodes distantes"
        }, 
        {
            "location": "/cpu_gpu/", 
            "text": "CPU/GPU\n\n\nLa diff\u00e9rence de performance entre les CPU et les GPU est due \u00e0 la parall\u00e9lisation des calculs.\n\nLes programmes s'\u00e9xecutant de maniere classique, cad par un processeurs, se font majoritairement de mani\u00e8re s\u00e9quencielle, malgr\u00e9 les efforts des processeurs multi-coeurs et des programmes multithread\u00e9s.\n\nLe GPU r\u00e9pond a ce probl\u00e8me en proposant un parall\u00e9lisme extr\u00eame, passant par un nouveau paradigme de programmation, car d\u00e9pendant de l'impl\u00e9mentation mat\u00e9rielle de la carte de calcul.\n\nAinsi, un GP peut \u00eatre repr\u00e9sent\u00e9 par un grand nombre de processeurs coll\u00e9 les uns \u00e0 cot\u00e9 des autres, faisant des calculs simple, mais de mani\u00e8re totalement parall\u00e8le.\n\n\n\n\nLes CPU incluent un nombre restreint de c\u0153urs optimis\u00e9s pour le traitement en s\u00e9rie, alors que  \nles GPU int\u00e8grent des milliers de c\u0153urs con\u00e7us pour traiter efficacement de nombreuses t\u00e2ches simultan\u00e9es.\n\n\n\n\nLien", 
            "title": "CPU/GPU"
        }, 
        {
            "location": "/cpu_gpu/#cpugpu", 
            "text": "La diff\u00e9rence de performance entre les CPU et les GPU est due \u00e0 la parall\u00e9lisation des calculs. \nLes programmes s'\u00e9xecutant de maniere classique, cad par un processeurs, se font majoritairement de mani\u00e8re s\u00e9quencielle, malgr\u00e9 les efforts des processeurs multi-coeurs et des programmes multithread\u00e9s. \nLe GPU r\u00e9pond a ce probl\u00e8me en proposant un parall\u00e9lisme extr\u00eame, passant par un nouveau paradigme de programmation, car d\u00e9pendant de l'impl\u00e9mentation mat\u00e9rielle de la carte de calcul. \nAinsi, un GP peut \u00eatre repr\u00e9sent\u00e9 par un grand nombre de processeurs coll\u00e9 les uns \u00e0 cot\u00e9 des autres, faisant des calculs simple, mais de mani\u00e8re totalement parall\u00e8le.   Les CPU incluent un nombre restreint de c\u0153urs optimis\u00e9s pour le traitement en s\u00e9rie, alors que  \nles GPU int\u00e8grent des milliers de c\u0153urs con\u00e7us pour traiter efficacement de nombreuses t\u00e2ches simultan\u00e9es.  Lien", 
            "title": "CPU/GPU"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architectures GPU Nvidia\n\n\nVoici une liste des architecture de GPU de Nvidia, de la plus ancienne \u00e0 la plus r\u00e9cente.\n\n\n\n\n\n\nTesla\n : 2006\n\nPremier implementation du \n\"Unified Shading Architecture\"\n\n\n\n\n\n\nFermi\n  : 2010\n\nPremier v\u00e9ritable systeme GPU.\n\n\n\n\n\n\nKepler\n  : 2012\n\nMoins couteux en energie, plus performant, et plus facilement programmable\n\n\n\n\n\n\nMaxwell\n : 2014\n\nAmeliorations des Streaming Multiprocessor, et gain en ratio performance/energie\n\n\n\n\n\n\nPascal\n : 2016\n\nAmeliorations des Streaming Multiprocessor, et Graphics Processor Cluster, \n\n\n\n\n\n\nVolta\n  : A venir\n\nDevrait figurer \nHigh Bandwidth Memory\n, \nUnified Memory\n, and \nNVLink\n\n\n\n\n\n\nLes gains de performance les plus importants constat\u00e9s entre deux architectures sont quand il y a une diminution de la finesse de gravure, plus la finesse de gravure est basse (16 nm \u00e0 partir de Pascal) plus les cartes poss\u00e8dent de transistors sur une m\u00eame surface. Une baisse ddee la finesse de gravure engendre souvent une baisse de la consommation.\nL'autre point qui permet d'am\u00e9liorer grandement les performances est le type de m\u00e9moire utilis\u00e9 et la fr\u00e9quence d'utilisation.\n\n\nProcesseurs SoC\n\n\nLes processeurs \nSoC\n sont bas\u00e9 sur un processeur tout en un, comprennant donc plus qu'un simple processeur (en fonction des besoins).\n\nCes derniers sont destin\u00e9 aux appareils mobiles, ou embarqu\u00e9s.\n\nNous ne rentrerons en d\u00e9tails dans cette gamme que dans le cas de la carte Jetson TK1\n\n\nTegra\n\n\nLe processeur Tegra est le composant Nvidia SoC: il contient CPU, GPU, northbridge, southbridge, et une m\u00e9moire primaire.  Les premi\u00e8res g\u00e9n\u00e9rations apparaissent en 2008.\n\n\nTegra K1\n\n\nLe processeur TK1 est l'unit\u00e9 centrale de la carte Jetson TK1. \nIl se base sur l'architecture Kepler. \n\n\nBas\u00e9 sur 4 CPU ARM Cortex-A15 MPCore \u00e0 2,3 GHz et grav\u00e9 en 28 nm en utilisant la technologie trois portes FinFET, il contient 32 ko de cache L1 et supporte jusqu'\u00e0 8 Gio de RAM DDR3. Il contient \u00e9galement un GPU Kepler contenant 192 c\u0153urs CUDA supportant OpenGL ES 3.0 et DirectX 11, et pour la premi\u00e8re fois dans un SoC ARM, OpenGL, en version 4.4. Il supporte une d\u00e9finition 4K Ultra HDTV et un appareil photo jusqu'\u00e0 100MPx.\n  \n\n\nLectures:\n\n\n\n\nFonctionnement interne GPU\n  \n\n\nArchitectures modernes GPU", 
            "title": "Architectures"
        }, 
        {
            "location": "/architecture/#architectures-gpu-nvidia", 
            "text": "Voici une liste des architecture de GPU de Nvidia, de la plus ancienne \u00e0 la plus r\u00e9cente.    Tesla  : 2006 \nPremier implementation du  \"Unified Shading Architecture\"    Fermi   : 2010 \nPremier v\u00e9ritable systeme GPU.    Kepler   : 2012 \nMoins couteux en energie, plus performant, et plus facilement programmable    Maxwell  : 2014 \nAmeliorations des Streaming Multiprocessor, et gain en ratio performance/energie    Pascal  : 2016 \nAmeliorations des Streaming Multiprocessor, et Graphics Processor Cluster,     Volta   : A venir \nDevrait figurer  High Bandwidth Memory ,  Unified Memory , and  NVLink    Les gains de performance les plus importants constat\u00e9s entre deux architectures sont quand il y a une diminution de la finesse de gravure, plus la finesse de gravure est basse (16 nm \u00e0 partir de Pascal) plus les cartes poss\u00e8dent de transistors sur une m\u00eame surface. Une baisse ddee la finesse de gravure engendre souvent une baisse de la consommation.\nL'autre point qui permet d'am\u00e9liorer grandement les performances est le type de m\u00e9moire utilis\u00e9 et la fr\u00e9quence d'utilisation.", 
            "title": "Architectures GPU Nvidia"
        }, 
        {
            "location": "/architecture/#processeurs-soc", 
            "text": "Les processeurs  SoC  sont bas\u00e9 sur un processeur tout en un, comprennant donc plus qu'un simple processeur (en fonction des besoins). \nCes derniers sont destin\u00e9 aux appareils mobiles, ou embarqu\u00e9s. \nNous ne rentrerons en d\u00e9tails dans cette gamme que dans le cas de la carte Jetson TK1", 
            "title": "Processeurs SoC"
        }, 
        {
            "location": "/architecture/#tegra", 
            "text": "Le processeur Tegra est le composant Nvidia SoC: il contient CPU, GPU, northbridge, southbridge, et une m\u00e9moire primaire.  Les premi\u00e8res g\u00e9n\u00e9rations apparaissent en 2008.", 
            "title": "Tegra"
        }, 
        {
            "location": "/architecture/#tegra-k1", 
            "text": "Le processeur TK1 est l'unit\u00e9 centrale de la carte Jetson TK1. \nIl se base sur l'architecture Kepler.   Bas\u00e9 sur 4 CPU ARM Cortex-A15 MPCore \u00e0 2,3 GHz et grav\u00e9 en 28 nm en utilisant la technologie trois portes FinFET, il contient 32 ko de cache L1 et supporte jusqu'\u00e0 8 Gio de RAM DDR3. Il contient \u00e9galement un GPU Kepler contenant 192 c\u0153urs CUDA supportant OpenGL ES 3.0 et DirectX 11, et pour la premi\u00e8re fois dans un SoC ARM, OpenGL, en version 4.4. Il supporte une d\u00e9finition 4K Ultra HDTV et un appareil photo jusqu'\u00e0 100MPx.", 
            "title": "Tegra K1"
        }, 
        {
            "location": "/architecture/#lectures", 
            "text": "Fonctionnement interne GPU     Architectures modernes GPU", 
            "title": "Lectures:"
        }, 
        {
            "location": "/cuda/", 
            "text": "CUDA\n\n\n\nCUDA (Compute Unified Device Architecture), est une technologie d\u00e9velopp\u00e9 par NVIDIA pour ses cartes graphiques en 2007.\n\nElle permet d'acc\u00e9der a la puissance de la carte graphique via une API C, ou des bindings dans des langages plus haut niveau (python-cuda), et a des frameworks(Tensorflow, Caffee).  \n\n\nLes concurrents de CUDA sont OpenCL, Larrabee, ATI Stream etc.. \n\n\nCoeurs CUDA\n\n\nLes coeurs CUDA sont des unit\u00e9s de calculs pas si diff\u00e9rentes que celles sur les cartes AMD (stream processor). Ces deux types d'unit\u00e9s de calculs excellent dans l'\u00e9x\u00e9cution de programmes parall\u00e8les.\n\n\nLes diff\u00e9rences notables sont que les coeurs CUDA sont plus gros, plus complexe et tourne sur une fr\u00e9quence plus \u00e9lev\u00e9. Ainsi on ne peut pas comparer le nombre de coeur entre une carte NVIDIA et AMD car il faut plus d'unit\u00e9s de calculs dans les cartes AMD pour \u00e9galer les performances d'une carte NVIDIA.\n\n\nLe compilateur CUDA fait moins d'optimisations et laisse la carte NVIDIA assign\u00e9 les coeurs dont aura besoin le programme.\nUn des points important en faveur de CUDA est le support qu'apporte NVIDIA, c'est pour cela qu'il y a un grand nombre de librairies disponibles pour CUDA.\n\n\nBlock/Thread\n\n\nUn programme (s'ex\u00e9cutant sur un CPU) peut avoir besoin d'appeler des calculs qui seront r\u00e9alis\u00e9s par des threads sur le GPU.\n\nPour cela il doit s'arranger pour donner a la carte les zones m\u00e9moires ou sont stock\u00e9s les donn\u00e9es en input, et ou \u00e9crire les donn\u00e9es r\u00e9sultantes.\n\nEnsuite, la fonction est ex\u00e9cut\u00e9 par autant de threads que n\u00e9cessaire.\n\nLes threads sont num\u00e9rot\u00e9s, et ils peuvent acc\u00e9der a leur identifiant. Ainsi, il peut savoir quelle partie de la m\u00e9moire il peut manipuler.\n\nLes threads sont r\u00e9partis dans des blocks CUDA. Cela sert a partag\u00e9 la m\u00e9moire d'une fa\u00e7on plus efficace. En effet, il existe une hi\u00e9rarchie d'acc\u00e8s \u00e0 la m\u00e9moire.\n\nLa m\u00e9moire propre du thread, la m\u00e9moire partag\u00e9 par tout les blocks du thread, et enfin la m\u00e9moire globale.  \n\n\nExemple de code CUDA\n\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \ncuda.h\n\n#include \ncuda_runtime.h\n\n\n__global__\nvoid mykernel(float *A1, float *A2, float *R)\n{\n    int p = threadIdx.x;\n    R[p] = A1[p] + A2[p];\n}\n\nint main()\n{\n    float A1[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n    float A2[] = { 10, 20, 30, 40, 50, 60, 70, 80, 90 };\n    float R[9];\n    int taille_mem = sizeof(float) * 9;\n\n    // on alloue de la memoire sur la carte graphique\n    float *a1_device;\n    float *a2_device;\n    float *r_device;\n    cudaMalloc((void**) \na1_device, taille_mem);\n    cudaMalloc((void**) \na2_device, taille_mem);\n    cudaMalloc((void**) \nr_device, taille_mem);\n\n    // on copie les donnees sur la carte\n    cudaMemcpy(a1_device, A1, taille_mem, cudaMemcpyHostToDevice);\n    cudaMemcpy(a2_device, A2, taille_mem, cudaMemcpyHostToDevice);\n\n    //9 additions, aucune boucle !\n    mykernel\n1, 9\n(a1_device, a2_device, r_device);\n\n    // on recupere le resultat\n    cudaMemcpy(R, r_device, taille_mem, cudaMemcpyDeviceToHost);\n    // sortie \u00e0 l'ecran\n    for(int i = 0; i \n 9; i++) {\n        printf(\"%f\\n\", R[i]);\n    }\n}\n\n\n\nEn D\u00e9tail\n\n\nUn programme CUDA est compos\u00e9 de deux composants primaire : un h\u00f4te (host) et un device (le GPU). \nLe code Host tourne sur le CPU, tandis que les fonctions kernel tournent sur le GPU.\nL'execution du kernel peut \u00eatre r\u00e9alis\u00e9e totalement ind\u00e9pendament de l'execution de l'host.\n\n\n \n\n\nUne application commence par executer le concernant l'host CPU, ensuite l'host invoque un kernel GPU sur le device GPU.Le GPu execute ce kernel en parallel en utilisant de multiples threads.Lorsque le kernel completeson ex\u00e9cution, le CPU reprend son execution du programme d'origine.\n(il est possible d'executer plusieur kernel ou bien de ne pas attendre sa fin pour continuer l'exeuction du host, nous d\u00e9taillerons ceci dans la partie Stream et synchronisation).\n\n\nKernel\n\n\nCUDA C h\u00e9rite du C en permettant aux programmers de d\u00e9finir des fonction C, appel\u00e9es kernels, qui lorsqu'elle sont appell\u00e9es, sont execut\u00e9es N fois en parallele par N diff\u00e9rents threads CUDA, contrainrement \u00e0 une fois seulement en C.\n\n\nD\u00e9finir un kernel\n\n\nUn kenerl est d\u00e9finit en utilisant le mot-cl\u00e9 \nglobal\n.\n\n\n\n// Definition du Kernel\n__global__ void VecAdd(float* A, float* B, float* C)\n{\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\n\n\n\n\nInvoquer un Kernel\n\n\nInvoquer un kernel GPU est tr\u00e8s similaire \u00e0 l'appel d'une fonction. CUDA utilise une synthaxe \u00e0 base de chevron afin de configurer et d'exectuer un kernel. \nIl faut donc lui indiquer le nombre de blocs et le nombre de threads \u00e0 utiliser.\n\n\n\nint main()\n{\n    ...\n    // Invocation de Kernel avec N threads sur 1 bloc\n    VecAdd\n1, N\n(A, B, C);\n    ...\n}\n\n\n\n\n\nIci chaqu'un des N threads executant VecAdd() performent une addition.\n\n\nGrid\n\n\nLa grid (grille) est le nom d'un kernel actif. Lorsque l'on lance plusieurs kernels, chacun \u00e0 son propre espace de travail, sa grid\n\n\n\n\nBlocs\n\n\nUn bloc est un ensemble limit\u00e9 de threads (multiple de 2).\nLes blocs composant une grid sont totalement ind\u00e9pendants. Diff\u00e9rents blocs sont assign\u00e9s \u00e0 diff\u00e9rents microprocesseurs (SM pour streaming multiprocesors).\nPlusieurs blocs peuvent \u00eatre pr\u00e9sent sur le m\u00eame SM, mais un bloc ne peut pas \u00eatre distribu\u00e9 sur plusieurs SM en m\u00eame temps.\nLes threads utilis\u00e9s dans un bloc peuvent :\n    \nSe synchroniser ( \u00e0 l'aide de la fonction __syncthreads qui agit comme une barriere tant que tout les threads du m\u00eame bloc ne l'ont pas atteinte)\n    \nPartager de la m\u00e9moire (shared memory)\n    *Communiquer\n\n\nParral\u00e9llisme\n\n\nL'ind\u00e9pendance des blocs et des kernel permet \u00e0 CUDA d'\u00eatre plus \u00e9volutifs et plus flexibles. CUDA peut donc supporter plusieurs formes de parral\u00e9lisme tel que :\n\nParral\u00e9llisme au niveau des threads : Diff\u00e9rents threads exectuent diff\u00e9rentes t\u00e2ches.\n\nParral\u00e9llisme au niveau des blocs et des grids : Diff\u00e9rents blocs ou grids executent diff\u00e9rentes t\u00e2ches.\n*Parral\u00e9llisme de donn\u00e9es : Diff\u00e9rents threads and blocks travaillent sur diff\u00e9rentes parties de la m\u00e9moire.\n\n\nThread ID\n\n\nChaque bloc et chaque thread disposent d'un index qui leur est propre, uniquement accesible depuis un kernel actif.\nLes variables blockIdx et threadIdx contiennent ces index.\nthreadIdx est un vecteur \u00e0 trois dimension, tandis que blockIdx est un vecteur \u00e0 2 dimensions. L'index threadIdx est donc lui-m\u00eame le bloc de threads.\nLa taille de la grid et d'un bloc est d\u00e9finie respectivement par les variables gridDim et blockDim.\n\n\nWarp\n\n\nLes warps sont des groupes de threads cons\u00e9cutifs. Un warp est execut\u00e9 par un seul coeur CUDA.\nA l'execution, un bloc de threads est divis\u00e9 en un nombre de warp \u00e9gal au nombre de coeur CUDA dans le SM.\nLa taille des warps d\u00e9pend du mat\u00e9riel, par exemple pour la jetson TK1 chaque warp contient 32 threads.\nCe sont ces warps qui nous permettent d'obtenir un style d'execution SMT (Single Instruction Multiple-Thread).\nContrairement au grids et aux blocs, l'impl\u00e9mentation des warps n'est pas accesible aux programmeurs. Cependant, ce mod\u00e8le d'execution influe sur les performances (plus de d\u00e9tail dans la section Optimisation).\n\n\nHierarchie de la m\u00e9moire\n\n\nLes threads CUDA ont acc\u00e8s \u00e0 diff\u00e9rents espaces de la m\u00e9moire lors de leurs \u00e9xecutions. \nChaque thread dispose d'un espace m\u00e9moire priv\u00e9 qui lui est propre.\nChaque thread peut aussi acc\u00e9der \u00e0 un espace de m\u00e9moire partag\u00e9e avec tout les threads du m\u00eame bloc ainsi qu'\u00e0 la m\u00e9moire globale (partag\u00e9e par tout les kernels).\n\n\n\n\nL'hote et le device maintiennent leurs propres espaces m\u00e9moires en DRAM, r\u00e9ff\u00e9r\u00e9s en temps qu'host memory et device memory.\n\n\nThrust vector\n\n\nThrust est une librarie bas\u00e9e sur la STL (Standard Template Library). Elle offre une interface haut-niveau permettant de programmer plus facilement des applications CUDA.\n\n\nThrust fournit deux containeurs vectoriels : host_vector (stock\u00e9 sur la m\u00e9moir\u00e9 de l'host) et device_vector (stock\u00e9 sur la m\u00e9moire du GPU).\nCes vecteurs fonctionnent de la m\u00eame mani\u00e8re que les std::vector disponibles en C++ STL : \nce sont des conteneurs g\u00e9neriques (tout type de donn\u00e9e) pouvant \u00eatre redimensionn\u00e9s dynamiquement.\nLorsqu'une fonction Thrust est appell\u00e9e il v\u00e9rifie automatique le type d'it\u00e9rateur utilis\u00e9 pour d\u00e9terminer si il doit utiliser une impl\u00e9mentation host ou device.\nCe processus est connu sous le nom de \"static dispatching\" car  l\"impl\u00e9mentation host/device est determin\u00e9e durant la compilation.\n\n\nNous pouvons utiliser des raw_pointers pour acc\u00e8der aux donn\u00e9es sans utiliser les it\u00e9rateurs.\n\n\nCapacit\u00e9 de calcul\n\n\nLa capacit\u00e9 de calcul d'un appareil est repr\u00e9sent\u00e9 par son num\u00e9ro de version (parfois appell\u00e9 \"SM version\").\nCe num\u00e9ro de version permet d'identifier le nombre de feature support\u00e9 par le hardware GPU et est utilis\u00e9 par les application \u00e0 l'\u00e9xecution pour d\u00e9terminer les outils et instructions diposnibles pour le GPU.\n\n\nLes appareils ayant la m\u00eame architecture diposent du m\u00eame niveau de revision.\nListe des capacit\u00e9s selon les versions : http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n\n\nAlternatives\n\n\n\n\nOpenCL\n\nCombinaisons d'API CPU multi-coeurs / GPU\n\n\nSite Nvidia", 
            "title": "CUDA"
        }, 
        {
            "location": "/cuda/#cuda", 
            "text": "CUDA (Compute Unified Device Architecture), est une technologie d\u00e9velopp\u00e9 par NVIDIA pour ses cartes graphiques en 2007. \nElle permet d'acc\u00e9der a la puissance de la carte graphique via une API C, ou des bindings dans des langages plus haut niveau (python-cuda), et a des frameworks(Tensorflow, Caffee).    Les concurrents de CUDA sont OpenCL, Larrabee, ATI Stream etc..", 
            "title": "CUDA"
        }, 
        {
            "location": "/cuda/#coeurs-cuda", 
            "text": "Les coeurs CUDA sont des unit\u00e9s de calculs pas si diff\u00e9rentes que celles sur les cartes AMD (stream processor). Ces deux types d'unit\u00e9s de calculs excellent dans l'\u00e9x\u00e9cution de programmes parall\u00e8les.  Les diff\u00e9rences notables sont que les coeurs CUDA sont plus gros, plus complexe et tourne sur une fr\u00e9quence plus \u00e9lev\u00e9. Ainsi on ne peut pas comparer le nombre de coeur entre une carte NVIDIA et AMD car il faut plus d'unit\u00e9s de calculs dans les cartes AMD pour \u00e9galer les performances d'une carte NVIDIA.  Le compilateur CUDA fait moins d'optimisations et laisse la carte NVIDIA assign\u00e9 les coeurs dont aura besoin le programme.\nUn des points important en faveur de CUDA est le support qu'apporte NVIDIA, c'est pour cela qu'il y a un grand nombre de librairies disponibles pour CUDA.", 
            "title": "Coeurs CUDA"
        }, 
        {
            "location": "/cuda/#blockthread", 
            "text": "Un programme (s'ex\u00e9cutant sur un CPU) peut avoir besoin d'appeler des calculs qui seront r\u00e9alis\u00e9s par des threads sur le GPU. \nPour cela il doit s'arranger pour donner a la carte les zones m\u00e9moires ou sont stock\u00e9s les donn\u00e9es en input, et ou \u00e9crire les donn\u00e9es r\u00e9sultantes. \nEnsuite, la fonction est ex\u00e9cut\u00e9 par autant de threads que n\u00e9cessaire. \nLes threads sont num\u00e9rot\u00e9s, et ils peuvent acc\u00e9der a leur identifiant. Ainsi, il peut savoir quelle partie de la m\u00e9moire il peut manipuler. \nLes threads sont r\u00e9partis dans des blocks CUDA. Cela sert a partag\u00e9 la m\u00e9moire d'une fa\u00e7on plus efficace. En effet, il existe une hi\u00e9rarchie d'acc\u00e8s \u00e0 la m\u00e9moire. \nLa m\u00e9moire propre du thread, la m\u00e9moire partag\u00e9 par tout les blocks du thread, et enfin la m\u00e9moire globale.", 
            "title": "Block/Thread"
        }, 
        {
            "location": "/cuda/#exemple-de-code-cuda", 
            "text": "#include  stdio.h \n#include  stdlib.h \n#include  cuda.h \n#include  cuda_runtime.h \n\n__global__\nvoid mykernel(float *A1, float *A2, float *R)\n{\n    int p = threadIdx.x;\n    R[p] = A1[p] + A2[p];\n}\n\nint main()\n{\n    float A1[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n    float A2[] = { 10, 20, 30, 40, 50, 60, 70, 80, 90 };\n    float R[9];\n    int taille_mem = sizeof(float) * 9;\n\n    // on alloue de la memoire sur la carte graphique\n    float *a1_device;\n    float *a2_device;\n    float *r_device;\n    cudaMalloc((void**)  a1_device, taille_mem);\n    cudaMalloc((void**)  a2_device, taille_mem);\n    cudaMalloc((void**)  r_device, taille_mem);\n\n    // on copie les donnees sur la carte\n    cudaMemcpy(a1_device, A1, taille_mem, cudaMemcpyHostToDevice);\n    cudaMemcpy(a2_device, A2, taille_mem, cudaMemcpyHostToDevice);\n\n    //9 additions, aucune boucle !\n    mykernel 1, 9 (a1_device, a2_device, r_device);\n\n    // on recupere le resultat\n    cudaMemcpy(R, r_device, taille_mem, cudaMemcpyDeviceToHost);\n    // sortie \u00e0 l'ecran\n    for(int i = 0; i   9; i++) {\n        printf(\"%f\\n\", R[i]);\n    }\n}", 
            "title": "Exemple de code CUDA"
        }, 
        {
            "location": "/cuda/#en-detail", 
            "text": "Un programme CUDA est compos\u00e9 de deux composants primaire : un h\u00f4te (host) et un device (le GPU). \nLe code Host tourne sur le CPU, tandis que les fonctions kernel tournent sur le GPU.\nL'execution du kernel peut \u00eatre r\u00e9alis\u00e9e totalement ind\u00e9pendament de l'execution de l'host.     Une application commence par executer le concernant l'host CPU, ensuite l'host invoque un kernel GPU sur le device GPU.Le GPu execute ce kernel en parallel en utilisant de multiples threads.Lorsque le kernel completeson ex\u00e9cution, le CPU reprend son execution du programme d'origine.\n(il est possible d'executer plusieur kernel ou bien de ne pas attendre sa fin pour continuer l'exeuction du host, nous d\u00e9taillerons ceci dans la partie Stream et synchronisation).", 
            "title": "En D\u00e9tail"
        }, 
        {
            "location": "/cuda/#kernel", 
            "text": "CUDA C h\u00e9rite du C en permettant aux programmers de d\u00e9finir des fonction C, appel\u00e9es kernels, qui lorsqu'elle sont appell\u00e9es, sont execut\u00e9es N fois en parallele par N diff\u00e9rents threads CUDA, contrainrement \u00e0 une fois seulement en C.", 
            "title": "Kernel"
        }, 
        {
            "location": "/cuda/#definir-un-kernel", 
            "text": "Un kenerl est d\u00e9finit en utilisant le mot-cl\u00e9  global .  \n// Definition du Kernel\n__global__ void VecAdd(float* A, float* B, float* C)\n{\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}", 
            "title": "D\u00e9finir un kernel"
        }, 
        {
            "location": "/cuda/#invoquer-un-kernel", 
            "text": "Invoquer un kernel GPU est tr\u00e8s similaire \u00e0 l'appel d'une fonction. CUDA utilise une synthaxe \u00e0 base de chevron afin de configurer et d'exectuer un kernel. \nIl faut donc lui indiquer le nombre de blocs et le nombre de threads \u00e0 utiliser.  \nint main()\n{\n    ...\n    // Invocation de Kernel avec N threads sur 1 bloc\n    VecAdd 1, N (A, B, C);\n    ...\n}  Ici chaqu'un des N threads executant VecAdd() performent une addition.", 
            "title": "Invoquer un Kernel"
        }, 
        {
            "location": "/cuda/#grid", 
            "text": "La grid (grille) est le nom d'un kernel actif. Lorsque l'on lance plusieurs kernels, chacun \u00e0 son propre espace de travail, sa grid", 
            "title": "Grid"
        }, 
        {
            "location": "/cuda/#blocs", 
            "text": "Un bloc est un ensemble limit\u00e9 de threads (multiple de 2).\nLes blocs composant une grid sont totalement ind\u00e9pendants. Diff\u00e9rents blocs sont assign\u00e9s \u00e0 diff\u00e9rents microprocesseurs (SM pour streaming multiprocesors).\nPlusieurs blocs peuvent \u00eatre pr\u00e9sent sur le m\u00eame SM, mais un bloc ne peut pas \u00eatre distribu\u00e9 sur plusieurs SM en m\u00eame temps.\nLes threads utilis\u00e9s dans un bloc peuvent :\n     Se synchroniser ( \u00e0 l'aide de la fonction __syncthreads qui agit comme une barriere tant que tout les threads du m\u00eame bloc ne l'ont pas atteinte)\n     Partager de la m\u00e9moire (shared memory)\n    *Communiquer", 
            "title": "Blocs"
        }, 
        {
            "location": "/cuda/#parralellisme", 
            "text": "L'ind\u00e9pendance des blocs et des kernel permet \u00e0 CUDA d'\u00eatre plus \u00e9volutifs et plus flexibles. CUDA peut donc supporter plusieurs formes de parral\u00e9lisme tel que : Parral\u00e9llisme au niveau des threads : Diff\u00e9rents threads exectuent diff\u00e9rentes t\u00e2ches. Parral\u00e9llisme au niveau des blocs et des grids : Diff\u00e9rents blocs ou grids executent diff\u00e9rentes t\u00e2ches.\n*Parral\u00e9llisme de donn\u00e9es : Diff\u00e9rents threads and blocks travaillent sur diff\u00e9rentes parties de la m\u00e9moire.", 
            "title": "Parral\u00e9llisme"
        }, 
        {
            "location": "/cuda/#thread-id", 
            "text": "Chaque bloc et chaque thread disposent d'un index qui leur est propre, uniquement accesible depuis un kernel actif.\nLes variables blockIdx et threadIdx contiennent ces index.\nthreadIdx est un vecteur \u00e0 trois dimension, tandis que blockIdx est un vecteur \u00e0 2 dimensions. L'index threadIdx est donc lui-m\u00eame le bloc de threads.\nLa taille de la grid et d'un bloc est d\u00e9finie respectivement par les variables gridDim et blockDim.", 
            "title": "Thread ID"
        }, 
        {
            "location": "/cuda/#warp", 
            "text": "Les warps sont des groupes de threads cons\u00e9cutifs. Un warp est execut\u00e9 par un seul coeur CUDA.\nA l'execution, un bloc de threads est divis\u00e9 en un nombre de warp \u00e9gal au nombre de coeur CUDA dans le SM.\nLa taille des warps d\u00e9pend du mat\u00e9riel, par exemple pour la jetson TK1 chaque warp contient 32 threads.\nCe sont ces warps qui nous permettent d'obtenir un style d'execution SMT (Single Instruction Multiple-Thread).\nContrairement au grids et aux blocs, l'impl\u00e9mentation des warps n'est pas accesible aux programmeurs. Cependant, ce mod\u00e8le d'execution influe sur les performances (plus de d\u00e9tail dans la section Optimisation).", 
            "title": "Warp"
        }, 
        {
            "location": "/cuda/#hierarchie-de-la-memoire", 
            "text": "Les threads CUDA ont acc\u00e8s \u00e0 diff\u00e9rents espaces de la m\u00e9moire lors de leurs \u00e9xecutions. \nChaque thread dispose d'un espace m\u00e9moire priv\u00e9 qui lui est propre.\nChaque thread peut aussi acc\u00e9der \u00e0 un espace de m\u00e9moire partag\u00e9e avec tout les threads du m\u00eame bloc ainsi qu'\u00e0 la m\u00e9moire globale (partag\u00e9e par tout les kernels).   L'hote et le device maintiennent leurs propres espaces m\u00e9moires en DRAM, r\u00e9ff\u00e9r\u00e9s en temps qu'host memory et device memory.", 
            "title": "Hierarchie de la m\u00e9moire"
        }, 
        {
            "location": "/cuda/#thrust-vector", 
            "text": "Thrust est une librarie bas\u00e9e sur la STL (Standard Template Library). Elle offre une interface haut-niveau permettant de programmer plus facilement des applications CUDA.  Thrust fournit deux containeurs vectoriels : host_vector (stock\u00e9 sur la m\u00e9moir\u00e9 de l'host) et device_vector (stock\u00e9 sur la m\u00e9moire du GPU).\nCes vecteurs fonctionnent de la m\u00eame mani\u00e8re que les std::vector disponibles en C++ STL : \nce sont des conteneurs g\u00e9neriques (tout type de donn\u00e9e) pouvant \u00eatre redimensionn\u00e9s dynamiquement.\nLorsqu'une fonction Thrust est appell\u00e9e il v\u00e9rifie automatique le type d'it\u00e9rateur utilis\u00e9 pour d\u00e9terminer si il doit utiliser une impl\u00e9mentation host ou device.\nCe processus est connu sous le nom de \"static dispatching\" car  l\"impl\u00e9mentation host/device est determin\u00e9e durant la compilation.  Nous pouvons utiliser des raw_pointers pour acc\u00e8der aux donn\u00e9es sans utiliser les it\u00e9rateurs.", 
            "title": "Thrust vector"
        }, 
        {
            "location": "/cuda/#capacite-de-calcul", 
            "text": "La capacit\u00e9 de calcul d'un appareil est repr\u00e9sent\u00e9 par son num\u00e9ro de version (parfois appell\u00e9 \"SM version\").\nCe num\u00e9ro de version permet d'identifier le nombre de feature support\u00e9 par le hardware GPU et est utilis\u00e9 par les application \u00e0 l'\u00e9xecution pour d\u00e9terminer les outils et instructions diposnibles pour le GPU.  Les appareils ayant la m\u00eame architecture diposent du m\u00eame niveau de revision.\nListe des capacit\u00e9s selon les versions : http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities", 
            "title": "Capacit\u00e9 de calcul"
        }, 
        {
            "location": "/cuda/#alternatives", 
            "text": "OpenCL \nCombinaisons d'API CPU multi-coeurs / GPU  Site Nvidia", 
            "title": "Alternatives"
        }, 
        {
            "location": "/frameworks/", 
            "text": "Frameworks\n\n\nIl existe de nombreux langages , librairies et frameworks avec lequels on peut utiliser la puissance des GPU en fonction de ses besoins.\n\n\n\n\nBindings\n\n\nCUDA C\n\n\nLibrairies bas niveau pour l'utilisation des threads CUDA\n\n\nCUDA C Programmming\n\n\nPyCUDA\n\n\nBindings Python pour les librairies CUDA. Permet d'integrer des kernels CUDA dans du code python\n\n\nPyCUDA\n\n\n\n\nLibrairies\n\n\nAmgX\n\n\nResolveur de probl\u00e8mes utilis\u00e9 pour les simulations physique nottament, dans les domaines de l'industrie.\n\n\nAmgX\n\n\nCuDNN\n\n\nLibrairies d'impl\u00e9mentations de r\u00e9seau neuronnaux de type Deep-Leearning.\nPeut \u00eatre utilis\u00e9 pour acc\u00e9lerer le travail d'autre frameworks de machine learning tel que Caffe, Tensorflow, Theano, Torch, CNTK etc..\n\n\nCompatibles\n\n\nCuDNN\n\n\nCuFFT\n\n\nTransformation de Fourrier rapide via GPU\n\n\nCuFFT\n\n\nIndeX\n\n\nVisualisation de donn\u00e9es m\u00e9t\u00e9orologiques.\n\n\nIndex\n\n\nNvGRAPH\n\n\nLibrairies regroupant des algorythmes d'analyse et traitements de graphes contenant plus de 2 milliars d'arretes.\n\n\nIndex\n\n\nCUDA Maths library\n\n\nEnsemble de fonctions math\u00e9matiques optimis\u00e9 pour GPU\n\n[CUDA Maths library](https://developer.nvidia.com/cuda-math-library\n\n\nOpenCV\n\n\nLibrairie Open Source de Vision par ordinateur (Computer Vision), de traitement d'images, et de machine learning\n\n\nOpenCV\n\n\n\n\nFrameworks\n\n\nNous avons \u00e9tudi\u00e9 diff\u00e9rents frameworks GPU pour d\u00e9terminer nos besoins r\u00e9els pour notre application finale.\n\nNotre choix se portait initialement sur Tensorflow, qui est le cador actuel dans le domaine de l'IA, et qui nous a \u00e9t\u00e9 fortement conseill\u00e9, cependant nous voulions tester ses concurents pour justifier l'utilisation de Tensorflow.\n\nPour la rapidit\u00e9 des diff\u00e9rents essais, ainsi que la pr\u00e9servation du systeme, nous avons d\u00e9cid\u00e9 d'utilis\u00e9 la plateforme Docker, qui nous permet l'installation de mini-vm pr\u00e9configur\u00e9 avec les logiciels n\u00e9cessaire a nos besoins.\n\nCependant, ces tests se faisant sur les heures personelles, nous utilisions docker, et non nvidia-docker qui ne fonctionne pas sur Windows, ainsi, l'utilisation des GPU n'\u00e9tait pas possible, de toute facon, le but \u00e9tant de comparer les frameworks d'inteligence entre eux, les resultats de classifications (et la simplicit\u00e9 avec laquelle les programmes se codent) nous interessais plus que les temps d'\u00e9xecutions.\n\nNous avons estim\u00e9, que notre probleme (transcription des signes), est assez proche du probleme de classification d'image mnist, qui consiste a classifier des chiffres \u00e9crit \u00e0 la main.\n\nExemples de code pour r\u00e9soudre le probleme de classification de \nmnist\n\n\nCaffe\n\n\nDeep-learning framework, optimis\u00e9 pour GPU\n\n\nCaffe\n\n\nmnist caffee\n\n\ndocker\n  \n\n\nCNTK\n\n\nMicrosoft cognitive Toolkit\n\n\nCNTK\n\n\nmnist cntk\n\n\ndocker\n  \n\n\nTheano\n\n\nLibrairie de calcul math\u00e9matiques, notamment multidimensions, optimis\u00e9 pour les GPU.\n\n\nTheano\n\n\nmnist theano\n\n\ndocker\n  \n\n\nTorch\n\n\nFramework \u00e0 but scientifique qui supporte de nombreux algorythmes, en optimisant par GPU\n\n\nTorch\n\n\nmnist torch\n\n\ndocker\n  \n\n\nTensorflow\n\n\nTensorflow est le framework d'inteligence port\u00e9 par Google, il semble le framework le plus complet, avec des tas de fonctionnalit\u00e9s et domaines d'application.\n\nLa communaut\u00e9 est tr\u00e8s active et de nombreuses ressources sont d\u00e9ja disponibles.\n\nTensorflow est pr\u00e9vu pour le d\u00e9ployement sur de nombreux type de machines, comme les serveurs distribu\u00e9s, les ordinateurs personnels, ou encore les smartphones, que la machine dispose d'un GPU ou non.\n\n\nTensorflow\n\n\nmnist tensorflow\n\n\ndocker\n  \n\n\nKeras\n\n\nKeras est une surcouche utilisant ou Tensorflow, ou Theano, en simplifiant grandement l'\u00e9criture du code.\n\nEn effet, le paradygmes par flow et tenseur est assez compliqu\u00e9 a imaginer, et encore plus \u00e0 mettre en place.\n\nAvec Keras, la mise en place est simplifi\u00e9 au maximum, ne reste que les soucis de mod\u00e9lisation de notre probl\u00e8mes.\n\n\nKeras\n\n\nmnist keras\n\n\n\n\ndocker\n\n\nTflearn\n\n\nTFLearn fourni une interface \u00e0 tensorflow de mani\u00e8re simplif\u00e9.\n\nLe code est beaucoup lisible, compr\u00e9hensible, et est am\u00e9liorable avec du vrai code Tensorflow au besoin.  \n\n\nOpenCV\n\n\nApr\u00e8s les r\u00e9sultats, plutot mauvais que nous ayons obtenu avec les classifiers d'images simples pour nos datasets d'images de mains, nous avons d\u00e9cider d'am\u00e9liorer les images avec des pr\u00e9-pr\u00e9traitements, qui nous permettraient d'utiliser d'analyser des zones plus discriminantes de l'image, et avec des dimensions plus petites, ce qui devrait am\u00e9liorer nos r\u00e9sultats, et les vitesses d'entrainements/de classification.\n\nNous avons commencer par regarder les solutions que propose Nvidia (Nvidia Vision-works), mais nous avons trouv\u00e9s des r\u00e9sultats comparatifs (Fichier a retrouver!!!)[.] et Vision-works est pour la majorit\u00e9 des probl\u00e8mes plus lent que la version GPU de OpenCV, qui lui, est open-source, dispose d'une tr\u00e8s large communaut\u00e9, et poss\u00e8de des bindings dans de nombreux langages. \nOn voit sur ce document que Nvidia-VisionWorks est surtout fait pour d\u00e9tecter des angles mouveant sur des flux vid\u00e9os.\n\nJ'ai donc d\u00e9cid\u00e9 d'essayer les versions C++, Java(via processing), et Python de OpenCV.\n\nMon choix final se porte sur Python, car le groupe de projet maitrise bien ce langage, et que si nous utilisons Tensorflow, Tflearn, Keras etc, nous serions amener a le faire en Python.\n\nL'am\u00e9lioration de la vitesse des traitements via la version C++ \u00e9tant minime, je pr\u00e9fere l'\u00e9carter du au temps n\u00e9cessaire pour d\u00e9velopper.\n\nEnfin, la version Java \u00e9tait tr\u00e8s bien, surtout grace processing, qui simplifie grandement la pratique de Java pour des applications graphiques, ainsi que par sa communaut\u00e9.  \n\n\nNe distinguant aucune r\u00e9elle diff\u00e9rence dans les API en fonction du langage utilis\u00e9, j'ai d\u00e9cider pour d\u00e9finir le mod\u00e8le des traitements en processing, car les modifications de codes et tests sont les plus rapides, et lorsque le modele nous semblera fonctionnel, de le porter sur Python.  \n\n\nSee Also\n\n\nListe de librairies acc\u00e9l\u00e9r\u00e9es par GPU\n\n\nListe de frameworks\n\n\nComparaison entre diff\u00e9rents frameworks de Deep-Learning\n\n\nMindMap de diff\u00e9rents modeles d'algorythmes de Machine-Learning\n\n\nTF Learn", 
            "title": "Frameworks"
        }, 
        {
            "location": "/frameworks/#frameworks", 
            "text": "Il existe de nombreux langages , librairies et frameworks avec lequels on peut utiliser la puissance des GPU en fonction de ses besoins.", 
            "title": "Frameworks"
        }, 
        {
            "location": "/frameworks/#bindings", 
            "text": "", 
            "title": "Bindings"
        }, 
        {
            "location": "/frameworks/#cuda-c", 
            "text": "Librairies bas niveau pour l'utilisation des threads CUDA  CUDA C Programmming", 
            "title": "CUDA C"
        }, 
        {
            "location": "/frameworks/#pycuda", 
            "text": "Bindings Python pour les librairies CUDA. Permet d'integrer des kernels CUDA dans du code python  PyCUDA", 
            "title": "PyCUDA"
        }, 
        {
            "location": "/frameworks/#librairies", 
            "text": "", 
            "title": "Librairies"
        }, 
        {
            "location": "/frameworks/#amgx", 
            "text": "Resolveur de probl\u00e8mes utilis\u00e9 pour les simulations physique nottament, dans les domaines de l'industrie.  AmgX", 
            "title": "AmgX"
        }, 
        {
            "location": "/frameworks/#cudnn", 
            "text": "Librairies d'impl\u00e9mentations de r\u00e9seau neuronnaux de type Deep-Leearning.\nPeut \u00eatre utilis\u00e9 pour acc\u00e9lerer le travail d'autre frameworks de machine learning tel que Caffe, Tensorflow, Theano, Torch, CNTK etc..  Compatibles  CuDNN", 
            "title": "CuDNN"
        }, 
        {
            "location": "/frameworks/#cufft", 
            "text": "Transformation de Fourrier rapide via GPU  CuFFT", 
            "title": "CuFFT"
        }, 
        {
            "location": "/frameworks/#index", 
            "text": "Visualisation de donn\u00e9es m\u00e9t\u00e9orologiques.  Index", 
            "title": "IndeX"
        }, 
        {
            "location": "/frameworks/#nvgraph", 
            "text": "Librairies regroupant des algorythmes d'analyse et traitements de graphes contenant plus de 2 milliars d'arretes.  Index", 
            "title": "NvGRAPH"
        }, 
        {
            "location": "/frameworks/#cuda-maths-library", 
            "text": "Ensemble de fonctions math\u00e9matiques optimis\u00e9 pour GPU \n[CUDA Maths library](https://developer.nvidia.com/cuda-math-library", 
            "title": "CUDA Maths library"
        }, 
        {
            "location": "/frameworks/#opencv", 
            "text": "Librairie Open Source de Vision par ordinateur (Computer Vision), de traitement d'images, et de machine learning  OpenCV", 
            "title": "OpenCV"
        }, 
        {
            "location": "/frameworks/#frameworks_1", 
            "text": "Nous avons \u00e9tudi\u00e9 diff\u00e9rents frameworks GPU pour d\u00e9terminer nos besoins r\u00e9els pour notre application finale. \nNotre choix se portait initialement sur Tensorflow, qui est le cador actuel dans le domaine de l'IA, et qui nous a \u00e9t\u00e9 fortement conseill\u00e9, cependant nous voulions tester ses concurents pour justifier l'utilisation de Tensorflow. \nPour la rapidit\u00e9 des diff\u00e9rents essais, ainsi que la pr\u00e9servation du systeme, nous avons d\u00e9cid\u00e9 d'utilis\u00e9 la plateforme Docker, qui nous permet l'installation de mini-vm pr\u00e9configur\u00e9 avec les logiciels n\u00e9cessaire a nos besoins. \nCependant, ces tests se faisant sur les heures personelles, nous utilisions docker, et non nvidia-docker qui ne fonctionne pas sur Windows, ainsi, l'utilisation des GPU n'\u00e9tait pas possible, de toute facon, le but \u00e9tant de comparer les frameworks d'inteligence entre eux, les resultats de classifications (et la simplicit\u00e9 avec laquelle les programmes se codent) nous interessais plus que les temps d'\u00e9xecutions. \nNous avons estim\u00e9, que notre probleme (transcription des signes), est assez proche du probleme de classification d'image mnist, qui consiste a classifier des chiffres \u00e9crit \u00e0 la main. \nExemples de code pour r\u00e9soudre le probleme de classification de  mnist", 
            "title": "Frameworks"
        }, 
        {
            "location": "/frameworks/#caffe", 
            "text": "Deep-learning framework, optimis\u00e9 pour GPU  Caffe  mnist caffee  docker", 
            "title": "Caffe"
        }, 
        {
            "location": "/frameworks/#cntk", 
            "text": "Microsoft cognitive Toolkit  CNTK  mnist cntk  docker", 
            "title": "CNTK"
        }, 
        {
            "location": "/frameworks/#theano", 
            "text": "Librairie de calcul math\u00e9matiques, notamment multidimensions, optimis\u00e9 pour les GPU.  Theano  mnist theano  docker", 
            "title": "Theano"
        }, 
        {
            "location": "/frameworks/#torch", 
            "text": "Framework \u00e0 but scientifique qui supporte de nombreux algorythmes, en optimisant par GPU  Torch  mnist torch  docker", 
            "title": "Torch"
        }, 
        {
            "location": "/frameworks/#tensorflow", 
            "text": "Tensorflow est le framework d'inteligence port\u00e9 par Google, il semble le framework le plus complet, avec des tas de fonctionnalit\u00e9s et domaines d'application. \nLa communaut\u00e9 est tr\u00e8s active et de nombreuses ressources sont d\u00e9ja disponibles. \nTensorflow est pr\u00e9vu pour le d\u00e9ployement sur de nombreux type de machines, comme les serveurs distribu\u00e9s, les ordinateurs personnels, ou encore les smartphones, que la machine dispose d'un GPU ou non.  Tensorflow  mnist tensorflow  docker", 
            "title": "Tensorflow"
        }, 
        {
            "location": "/frameworks/#keras", 
            "text": "Keras est une surcouche utilisant ou Tensorflow, ou Theano, en simplifiant grandement l'\u00e9criture du code. \nEn effet, le paradygmes par flow et tenseur est assez compliqu\u00e9 a imaginer, et encore plus \u00e0 mettre en place. \nAvec Keras, la mise en place est simplifi\u00e9 au maximum, ne reste que les soucis de mod\u00e9lisation de notre probl\u00e8mes.  Keras  mnist keras   docker", 
            "title": "Keras"
        }, 
        {
            "location": "/frameworks/#tflearn", 
            "text": "TFLearn fourni une interface \u00e0 tensorflow de mani\u00e8re simplif\u00e9. \nLe code est beaucoup lisible, compr\u00e9hensible, et est am\u00e9liorable avec du vrai code Tensorflow au besoin.", 
            "title": "Tflearn"
        }, 
        {
            "location": "/frameworks/#opencv_1", 
            "text": "Apr\u00e8s les r\u00e9sultats, plutot mauvais que nous ayons obtenu avec les classifiers d'images simples pour nos datasets d'images de mains, nous avons d\u00e9cider d'am\u00e9liorer les images avec des pr\u00e9-pr\u00e9traitements, qui nous permettraient d'utiliser d'analyser des zones plus discriminantes de l'image, et avec des dimensions plus petites, ce qui devrait am\u00e9liorer nos r\u00e9sultats, et les vitesses d'entrainements/de classification. \nNous avons commencer par regarder les solutions que propose Nvidia (Nvidia Vision-works), mais nous avons trouv\u00e9s des r\u00e9sultats comparatifs (Fichier a retrouver!!!)[.] et Vision-works est pour la majorit\u00e9 des probl\u00e8mes plus lent que la version GPU de OpenCV, qui lui, est open-source, dispose d'une tr\u00e8s large communaut\u00e9, et poss\u00e8de des bindings dans de nombreux langages. \nOn voit sur ce document que Nvidia-VisionWorks est surtout fait pour d\u00e9tecter des angles mouveant sur des flux vid\u00e9os. \nJ'ai donc d\u00e9cid\u00e9 d'essayer les versions C++, Java(via processing), et Python de OpenCV. \nMon choix final se porte sur Python, car le groupe de projet maitrise bien ce langage, et que si nous utilisons Tensorflow, Tflearn, Keras etc, nous serions amener a le faire en Python. \nL'am\u00e9lioration de la vitesse des traitements via la version C++ \u00e9tant minime, je pr\u00e9fere l'\u00e9carter du au temps n\u00e9cessaire pour d\u00e9velopper. \nEnfin, la version Java \u00e9tait tr\u00e8s bien, surtout grace processing, qui simplifie grandement la pratique de Java pour des applications graphiques, ainsi que par sa communaut\u00e9.    Ne distinguant aucune r\u00e9elle diff\u00e9rence dans les API en fonction du langage utilis\u00e9, j'ai d\u00e9cider pour d\u00e9finir le mod\u00e8le des traitements en processing, car les modifications de codes et tests sont les plus rapides, et lorsque le modele nous semblera fonctionnel, de le porter sur Python.", 
            "title": "OpenCV"
        }, 
        {
            "location": "/frameworks/#see-also", 
            "text": "Liste de librairies acc\u00e9l\u00e9r\u00e9es par GPU  Liste de frameworks  Comparaison entre diff\u00e9rents frameworks de Deep-Learning  MindMap de diff\u00e9rents modeles d'algorythmes de Machine-Learning  TF Learn", 
            "title": "See Also"
        }, 
        {
            "location": "/tensorflow/", 
            "text": "Introduction\n\n\nTensorFlow est un syst\u00e8me de programmation dans lequel les calculs sont repr\u00e9sent\u00e9s sous forme de graphiques. Les n\u0153uds du graphe sont appel\u00e9s ops (abr\u00e9viation de op\u00e9rations). Un op prend z\u00e9ro ou plus Tensors, effectue un certain calcul, et produit z\u00e9ro ou plus Tensors. Dans la terminologie TensorFlow, un Tensor est un tableau multidimensionnel typ\u00e9. Par exemple, on peut repr\u00e9senter un mini-lot d'images en tant que tableau 4-D de nombres en virgule flottante avec des dimensions [lot, hauteur, largeur, canaux].\n\n\nUn graphique TensorFlow est une description des calculs. Pour calculer quoi que ce soit, un graphique doit \u00eatre lanc\u00e9 dans une session. Une session place les op\u00e9rations graphiques sur les p\u00e9riph\u00e9riques, comme les CPU ou les GPU, et fournit des m\u00e9thodes pour les ex\u00e9cuter. Ces m\u00e9thodes renvoient des tenseurs produits par des ops en tant qu'objets numpy ndarray en Python, et comme des instances tensorflow :: Tensor en C et C ++.  \n\n\nStructure d'un programme\n\n\nLes programmes TensorFlow sont g\u00e9n\u00e9ralement structur\u00e9s en une phase de construction, qui assemble un graphe, et une phase d'ex\u00e9cution qui utilise une session pour ex\u00e9cuter des op\u00e9rations dans le graphe.\n\n\nPar exemple, il est courant de cr\u00e9er un graphe pour repr\u00e9senter et former un r\u00e9seau neuronal dans la phase de construction, puis ex\u00e9cuter \u00e0 plusieurs reprises un ensemble d'op\u00e9rations d'apprentissage dans le graphe dans la phase d'ex\u00e9cution.\n\n\nTensorFlow peut \u00eatre utilis\u00e9 \u00e0 partir de programmes C, C ++ et Python. Il est actuellement beaucoup plus facile d'utiliser la biblioth\u00e8que Python pour assembler des graphiques, car il fournit un grand nombre de fonctions auxiliaires non disponibles dans les biblioth\u00e8ques C et C ++.\n\n\nNous prenons un simple exemple de multiplication de deux matrices qui disponible sur le site de tensorflow. \n\n\nimport tensorflow as tf\n\n#construction du graphe de calcul\nmatrix1 = tf.constant([[3., 3.]])\nmatrix2 = tf.constant([[2.],[2.]])\nproduct = tf.matmul(matrix1, matrix2)\n\n#Lancement du graphique dans une session\nwith tf.Session() as sess:\n    print sess.run(product)\n\n#Lancement du graphique dans une session distribu\u00e9\nwith tf.Session(\ngrpc://localhost:2222\n) as sess:\n    print sess.run(product)", 
            "title": "Tensorflow"
        }, 
        {
            "location": "/tensorflow/#introduction", 
            "text": "TensorFlow est un syst\u00e8me de programmation dans lequel les calculs sont repr\u00e9sent\u00e9s sous forme de graphiques. Les n\u0153uds du graphe sont appel\u00e9s ops (abr\u00e9viation de op\u00e9rations). Un op prend z\u00e9ro ou plus Tensors, effectue un certain calcul, et produit z\u00e9ro ou plus Tensors. Dans la terminologie TensorFlow, un Tensor est un tableau multidimensionnel typ\u00e9. Par exemple, on peut repr\u00e9senter un mini-lot d'images en tant que tableau 4-D de nombres en virgule flottante avec des dimensions [lot, hauteur, largeur, canaux].  Un graphique TensorFlow est une description des calculs. Pour calculer quoi que ce soit, un graphique doit \u00eatre lanc\u00e9 dans une session. Une session place les op\u00e9rations graphiques sur les p\u00e9riph\u00e9riques, comme les CPU ou les GPU, et fournit des m\u00e9thodes pour les ex\u00e9cuter. Ces m\u00e9thodes renvoient des tenseurs produits par des ops en tant qu'objets numpy ndarray en Python, et comme des instances tensorflow :: Tensor en C et C ++.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tensorflow/#structure-dun-programme", 
            "text": "Les programmes TensorFlow sont g\u00e9n\u00e9ralement structur\u00e9s en une phase de construction, qui assemble un graphe, et une phase d'ex\u00e9cution qui utilise une session pour ex\u00e9cuter des op\u00e9rations dans le graphe.  Par exemple, il est courant de cr\u00e9er un graphe pour repr\u00e9senter et former un r\u00e9seau neuronal dans la phase de construction, puis ex\u00e9cuter \u00e0 plusieurs reprises un ensemble d'op\u00e9rations d'apprentissage dans le graphe dans la phase d'ex\u00e9cution.  TensorFlow peut \u00eatre utilis\u00e9 \u00e0 partir de programmes C, C ++ et Python. Il est actuellement beaucoup plus facile d'utiliser la biblioth\u00e8que Python pour assembler des graphiques, car il fournit un grand nombre de fonctions auxiliaires non disponibles dans les biblioth\u00e8ques C et C ++.  Nous prenons un simple exemple de multiplication de deux matrices qui disponible sur le site de tensorflow.   import tensorflow as tf\n\n#construction du graphe de calcul\nmatrix1 = tf.constant([[3., 3.]])\nmatrix2 = tf.constant([[2.],[2.]])\nproduct = tf.matmul(matrix1, matrix2)\n\n#Lancement du graphique dans une session\nwith tf.Session() as sess:\n    print sess.run(product)\n\n#Lancement du graphique dans une session distribu\u00e9\nwith tf.Session( grpc://localhost:2222 ) as sess:\n    print sess.run(product)", 
            "title": "Structure d'un programme"
        }, 
        {
            "location": "/applications/", 
            "text": "Nous avons travaill\u00e9 sur plusieurs facettes du d\u00e9veloppement GPU.\n\nNous avons dans un premier temps \u00e9tudier les applications d\u00e9ja existantes pour savoir ce qui \u00e9tait possible ou non, en fonction de notre exp\u00e9rience et budget.  \n\n\nApplications existantes\n\n\nUne liste d'applications en production ou en d\u00e9veloppement : \nLien\n\nNous remarquons dans cette liste la forte \u00e9tendue du champs d'apllication des GPU. De l'analyse de donn\u00e9ee, au traitement d'image, en passant par le militaire ou le m\u00e9dicale, la puissance apport\u00e9 par les cartes graphiques contribue a l'avanc\u00e9e dans de nombreux domaines.  \n\n\nApplications possibles\n\n\nNous listons ici nos diff\u00e9rents centre d'int\u00e9rets et de recherches.\n\n\nOptimisation Pure\n\n\nUne partie de notre travail de recherche a \u00e9t\u00e9 de pousser l'otimisation d'un maximale du temps d'\u00e9xecution d'un calcul g\u00e9n\u00e9rique.  \n\n\nTraitements d'image\n\n\nNous nous sommes amus\u00e9s a recr\u00e9er certains algorythmes de traitement d'images basiques, pour comparer leurs temps d'execution sur les versions processeurs et les versions GPU.  \n\n\nTranscription du LSF\n\n\nComme projet de d\u00e9monstration final, et avec les connaissances que nous avons acquise lors d ela premiere partie du projet, nous pensons \u00eatre capable de r\u00e9aliser un transcripteur du langage des signes francais, dans une version simplifi\u00e9. En effet, le traitement se faisant sur des images et non des vid\u00e9os, certains signes ne seronts compr\u00e9hensible par le systeme, dans sa version simple.\n\nAussi, n'ayant ni l'experience dans le langage des singes, ni le temps ou l'interet de l'apprendre, nous nous contenterons de traiter les signes correspondant a l'alphabet du LSF.", 
            "title": "Applications"
        }, 
        {
            "location": "/applications/#applications-existantes", 
            "text": "Une liste d'applications en production ou en d\u00e9veloppement :  Lien \nNous remarquons dans cette liste la forte \u00e9tendue du champs d'apllication des GPU. De l'analyse de donn\u00e9ee, au traitement d'image, en passant par le militaire ou le m\u00e9dicale, la puissance apport\u00e9 par les cartes graphiques contribue a l'avanc\u00e9e dans de nombreux domaines.", 
            "title": "Applications existantes"
        }, 
        {
            "location": "/applications/#applications-possibles", 
            "text": "Nous listons ici nos diff\u00e9rents centre d'int\u00e9rets et de recherches.", 
            "title": "Applications possibles"
        }, 
        {
            "location": "/applications/#optimisation-pure", 
            "text": "Une partie de notre travail de recherche a \u00e9t\u00e9 de pousser l'otimisation d'un maximale du temps d'\u00e9xecution d'un calcul g\u00e9n\u00e9rique.", 
            "title": "Optimisation Pure"
        }, 
        {
            "location": "/applications/#traitements-dimage", 
            "text": "Nous nous sommes amus\u00e9s a recr\u00e9er certains algorythmes de traitement d'images basiques, pour comparer leurs temps d'execution sur les versions processeurs et les versions GPU.", 
            "title": "Traitements d'image"
        }, 
        {
            "location": "/applications/#transcription-du-lsf", 
            "text": "Comme projet de d\u00e9monstration final, et avec les connaissances que nous avons acquise lors d ela premiere partie du projet, nous pensons \u00eatre capable de r\u00e9aliser un transcripteur du langage des signes francais, dans une version simplifi\u00e9. En effet, le traitement se faisant sur des images et non des vid\u00e9os, certains signes ne seronts compr\u00e9hensible par le systeme, dans sa version simple. \nAussi, n'ayant ni l'experience dans le langage des singes, ni le temps ou l'interet de l'apprendre, nous nous contenterons de traiter les signes correspondant a l'alphabet du LSF.", 
            "title": "Transcription du LSF"
        }, 
        {
            "location": "/traitementImage/", 
            "text": "Traitement D'image\n\n\nParmi les proof of concept, \u00e9tant donner que les autres travaillait avec CUDA en C++, j'ai d\u00e9cider de faire avec le langage Python, et la librairie \nPyCuda\n.\n\nPycuda permet d'executer des kernels materiels \u00e9crit en C++, mais dispose \u00e9galement d'un binding, que je n'ai pas (encore) utilis\u00e9: limiter le code C++ a la fonction materielle me suffisant, l'interet du python est alors de structurer simplement le reste du programme, nottament les interfaces.\n\nNos proof of concept ne se font pas sur des versions optimis\u00e9s des algorythmes, mais il se font cependant avec le m\u00eame calcul, juste le GPU execture le traitement en parralele.  \n\n\nNous avons appliqu\u00e9 les traitements du Threshold et de la d\u00e9tection de contours sur l'image suivante, et le retrait du fond avec nos webcams.  \n\n\n  \n\n\nThreshold.\n\n\nNous avons juste reinventer la roue pour implementer le principe du tresold d'abord en C++(utilisant la librairie Opencv) et ensuite en CUDA. \nLe principe est simple, nous comparons les valeurs des pixels d'image par rapport \u00e0 un seuil donn\u00e9. si la valeur est superieur au seuil le pixel est mis en blanc sinon en noir. l'image resultant est alors une image blanc-noire.\n\n\nImplementation CPU\n\n\nPour la partie C++ ce n'\u00e9tait pas trop compliqu\u00e9. Nous avons en effet charg\u00e9 l'image et ensuite compar\u00e9 les valeurs de chaque pixel au seuil (seuil = 128) et par la suite construit la nouvelle matrice de pixel et enfin enregistr\u00e9. Nous avons tr\u00e8s vite eu notre nouvelle image qui est bien blanc-noir.\n\n\nImpl\u00e9mentation GPU\n\n\nPour la partie Cuda nous avons eu \u00e0 g\u00e9rer plusieurs choses. Nous avons d'abord charg\u00e9 la matrice de pixels envoy\u00e9 vers le device.la comparaison du pixel avec le seuil et l'affection de la nouvelle de pixel se font dans le code du kernel. Nous enfin r\u00e9cup\u00e9rer la nouvelle matrice de pixel pour stocker l'image final. A chaque nous avons trait\u00e9 les exceptions moyennant le type d\u2019exception cudaError_t pred\u00e9finies dans Cuda. Voici quelques cas dans lesquels nous avons trait\u00e9 ces exceptions \n- pendant l'allocation de la matrice de pixel s'executant c\u00f4t\u00e9 mat\u00e9riel (cudaMalloc):\n\n\n\n\n\n\npendant la copie de la matrice (host -\ndevice) tout comme (device -\n host) (CudaMemcpy)\n\n\n\n\n\n\napr\u00e8s l'execution du kernel, nous nous assurons que l'ex\u00e9cution s'est bien pass\u00e9 \n(CudaGetLastError: m\u00e9thode qui retourne les erreurs s'ils en existe)\n\n\n\n\n\n\npour l'affichage de les exceptions, nous utilisons la fonction CudaGetErrorString() sur notre type d'erreur.\n\n\n\n\n\n\n  \n\n\nCoutours\n\n\nPour la detection de contours, j'ai d\u00e9cider d'utiliser PyCuda pour impl\u00e9menter le traitement, car nous utiliserons Python pour le projet final, et pycuda s'utilise comme Cuda C++ :\n\non \u00e9crit une fonction kernel, et on l'appel depuis python. Lors de la premiere execution, le kernel est compil\u00e9, et est r\u00e9utilis\u00e9 les fois d'apr\u00e8s.  \n\n\nImpl\u00e9mentation CPU\n\n\ndef contoursDetectCPU(pix):\n    seuil = 20\n\n    pixRes = np.zeros(pix.shape)\n    x = pix.shape[0]\n    y= pix.shape[1]\n\n    for j in range(1,y-1):\n        temp = []\n        for i in range(1,x-1):\n\n            diffX = sum(pix[i-1][j])-sum(pix[i+1][j])\n            diffY = sum(pix[i][j-1])-sum(pix[i][j+1])\n\n            if(diffX \n seuil)or(diffY \n seuil):\n                col = (0,0,0)\n            else:\n                col = (255,255,255)\n            pixRes[i][j] = col\n    return(pixRes)\n\n\n\n\nImpl\u00e9mentation GPU\n\n\n#fonction kernel\nmod = SourceModule(\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n__global__ void contour(float *im,float* res, int* x, int* y)\n{\nint seuil = 20;\nint size = int(x)*3;\nint begin = threadIdx.x*3;\n\nfor(int j = 1; j\n1000-1;j+=10){\n    for(int i = 3; i \n size-3 ;i+=10*3){\n        int idx = i+begin + (threadIdx.y*1000*3) + (j*1000*3);\n\n        int caseD = idx + 3; int caseG = idx - 3; int caseH = idx - (1000*3); int caseB = idx + (1000*3);\n\n        int diffX = abs(im[caseD]+im[caseD+1]+im[caseD+2]-(im[caseG]+im[caseG+1]+im[caseG+2]));\n        int diffY = abs(im[caseH]+im[caseH+1]+im[caseH+2]-(im[caseB]+im[caseB+1]+im[caseB+2]));\n\n        if((diffX\nseuil)\n(diffY\nseuil)){\n            res[idx] = 0;  res[idx+1] = 0; res[idx+2] = 0;\n        }\n        else{\n            res[idx] = 255; res[idx+1] = 255; res[idx+2] = 255;\n        }\n    }\n}\n\n\n\n}\n\n)\n\ndef contoursDetectCPU(pix):\n    pixRes = np.zeros(pix.shape)\n    x = np.int32(pix.shape[0])\n    y = np.int32(pix.shape[1])\n\n    # converti en float simple precision\n    pix = pix.astype(np.float32)\n\n    # alloue memoire gpu\n    im_gpu = cuda.mem_alloc(pix.nbytes)\n    res_gpu = cuda.mem_alloc(pix.nbytes)\n    x_gpu = cuda.mem_alloc(sys.getsizeof(x))\n    y_gpu = cuda.mem_alloc(sys.getsizeof(y))\n\n    # transfer array args\n    cuda.memcpy_htod(im_gpu, pix)\n    cuda.memcpy_htod(res_gpu, np.empty_like(pix))\n\n    func = mod.get_function(\ncontour\n)\n    func(im_gpu, res_gpu, x , y , block=(10,100,1))\n    result = np.empty_like(pix)\n\n    cuda.memcpy_dtoh(result, res_gpu)\n    return(result)\n\n\n\n\nR\u00e9sultats\n\n\nL'implementation du detecteur de contours s'\u00e9xecute en X ms sur CPU et Y ms sur GPU pour une image. Encore une fois la m\u00e9thode est basique, et nos nouvelles connaissances nous permettrait de faire mieux (Produit de convolution d'un certain filtre, le produit de convulution ayant surement des optimisations majeurs sur CPU et GPU. Nos r\u00e9sultats sont donc repr\u00e9sentatif par rapport a leurs impl\u00e9mentions seulement.)\n\n\n  \n\n\nBackgroud Remover\n\n\nImpl\u00e9mentation CPU\n\n\ndef bgRemoveCPU(bg,pix):\n    seuil = 10\n    pixRes = np.zeros(pix.shape)\n    x,y = (pix.shape[0],pix.shape[1])\n\n    for j in range(1,y):\n        for i in range(1,x):\n            diff = abs(sum(pix[i][j])/3-sum(bg[i][j]/3))\n\n            if(diff \n seuil):    col = pix[i][j]\n            else:                col = (255,255,255)\n\n            pixRes[i][j] = col\n\n    return(pixRes)\n\n\n\n\nImpl\u00e9mentation GPU\n\n\n#fonction kernel\nmod = SourceModule(\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n\n__global__ void bgremove(float *bg, float * im,float* res, int* x, int* y)\n{\nint seuil = 10;\nint size = int(x)*3;\nint begin = threadIdx.x*3;\n\nfor(int j = 1; j\n1000-1;j+=10){\n    for(int i = 3; i \n size-3 ;i+=10*3){\n        int idx = i+begin + (threadIdx.y*1000*3) + (j*1000*3);\n\n        int intIm = (im[idx]+im[idx+1]+im[idx+2])/3;\n        int intBG = (bg[idx]+bg[idx+1]+bg[idx+2])/3;\n\n        int diff = abs(intIm - intBG);\n\n        if(diff\nseuil){\n            res[idx] = im[idx]; res[idx+1] = im[idx+1]; res[idx+2] = im[idx+2];\n\n        }\n        else{\n            res[idx] = 255; res[idx+1] = 255; res[idx+2] = 255;\n        }\n    }\n}\n\n\n\n}\n\n)\n\ndef bgRemoveGPU(pixBG,pixIm):\n\n\n    print \ndebut algo gpu\n\n    t0 = time.time()\n\n    x = np.int32(pixBG.shape[0])\n    y = np.int32(pixBG.shape[1])\n\n    # converti en float simple precision\n    pixBG = pixBG.astype(np.float32)\n    pixIm = pixIm.astype(np.float32)\n\n    # alloue memoire gpu\n    BG_gpu = cuda.mem_alloc(pixBG.nbytes)\n    im_gpu = cuda.mem_alloc(pixIm.nbytes)\n    res_gpu = cuda.mem_alloc(pixBG.nbytes)\n    x_gpu = cuda.mem_alloc(sys.getsizeof(x))\n    y_gpu = cuda.mem_alloc(sys.getsizeof(y))\n\n    # transfer array args\n    cuda.memcpy_htod(BG_gpu, pixBG)\n    cuda.memcpy_htod(im_gpu, pixIm)\n    cuda.memcpy_htod(res_gpu, np.empty_like(pixBG))\n\n    func = mod.get_function(\nbgremove\n)\n    func(BG_gpu, im_gpu,res_gpu,x , y , block=(10,100,1))\n    result = np.empty_like(pixBG)\n\n    cuda.memcpy_dtoh(result, res_gpu)\n\n    tn = abs(t0-time.time())\n    print \nfin algo gpu en \n,tn, \n s\n\n    return(result)\n\n\n\n\nResultats\n\n\nMon impl\u00e9mentation s'\u00e9xecute en X ms sur CPU et Y ms sur GPU, cependant le r\u00e9sultat est moyen, et ne fonctionne pas en mode vi\u00e9deo pour une raison qui m'\u00e9chappe..  \n\n\nFond:\n\n\n  \n\n\nImage a traiter:\n\n\n  \n\n\nImage sans le fond:", 
            "title": "Traitement d'image"
        }, 
        {
            "location": "/traitementImage/#traitement-dimage", 
            "text": "Parmi les proof of concept, \u00e9tant donner que les autres travaillait avec CUDA en C++, j'ai d\u00e9cider de faire avec le langage Python, et la librairie  PyCuda . \nPycuda permet d'executer des kernels materiels \u00e9crit en C++, mais dispose \u00e9galement d'un binding, que je n'ai pas (encore) utilis\u00e9: limiter le code C++ a la fonction materielle me suffisant, l'interet du python est alors de structurer simplement le reste du programme, nottament les interfaces. \nNos proof of concept ne se font pas sur des versions optimis\u00e9s des algorythmes, mais il se font cependant avec le m\u00eame calcul, juste le GPU execture le traitement en parralele.    Nous avons appliqu\u00e9 les traitements du Threshold et de la d\u00e9tection de contours sur l'image suivante, et le retrait du fond avec nos webcams.", 
            "title": "Traitement D'image"
        }, 
        {
            "location": "/traitementImage/#threshold", 
            "text": "Nous avons juste reinventer la roue pour implementer le principe du tresold d'abord en C++(utilisant la librairie Opencv) et ensuite en CUDA. \nLe principe est simple, nous comparons les valeurs des pixels d'image par rapport \u00e0 un seuil donn\u00e9. si la valeur est superieur au seuil le pixel est mis en blanc sinon en noir. l'image resultant est alors une image blanc-noire.", 
            "title": "Threshold."
        }, 
        {
            "location": "/traitementImage/#implementation-cpu", 
            "text": "Pour la partie C++ ce n'\u00e9tait pas trop compliqu\u00e9. Nous avons en effet charg\u00e9 l'image et ensuite compar\u00e9 les valeurs de chaque pixel au seuil (seuil = 128) et par la suite construit la nouvelle matrice de pixel et enfin enregistr\u00e9. Nous avons tr\u00e8s vite eu notre nouvelle image qui est bien blanc-noir.", 
            "title": "Implementation CPU"
        }, 
        {
            "location": "/traitementImage/#implementation-gpu", 
            "text": "Pour la partie Cuda nous avons eu \u00e0 g\u00e9rer plusieurs choses. Nous avons d'abord charg\u00e9 la matrice de pixels envoy\u00e9 vers le device.la comparaison du pixel avec le seuil et l'affection de la nouvelle de pixel se font dans le code du kernel. Nous enfin r\u00e9cup\u00e9rer la nouvelle matrice de pixel pour stocker l'image final. A chaque nous avons trait\u00e9 les exceptions moyennant le type d\u2019exception cudaError_t pred\u00e9finies dans Cuda. Voici quelques cas dans lesquels nous avons trait\u00e9 ces exceptions \n- pendant l'allocation de la matrice de pixel s'executant c\u00f4t\u00e9 mat\u00e9riel (cudaMalloc):    pendant la copie de la matrice (host - device) tout comme (device -  host) (CudaMemcpy)    apr\u00e8s l'execution du kernel, nous nous assurons que l'ex\u00e9cution s'est bien pass\u00e9 \n(CudaGetLastError: m\u00e9thode qui retourne les erreurs s'ils en existe)    pour l'affichage de les exceptions, nous utilisons la fonction CudaGetErrorString() sur notre type d'erreur.", 
            "title": "Impl\u00e9mentation GPU"
        }, 
        {
            "location": "/traitementImage/#coutours", 
            "text": "Pour la detection de contours, j'ai d\u00e9cider d'utiliser PyCuda pour impl\u00e9menter le traitement, car nous utiliserons Python pour le projet final, et pycuda s'utilise comme Cuda C++ : \non \u00e9crit une fonction kernel, et on l'appel depuis python. Lors de la premiere execution, le kernel est compil\u00e9, et est r\u00e9utilis\u00e9 les fois d'apr\u00e8s.", 
            "title": "Coutours"
        }, 
        {
            "location": "/traitementImage/#implementation-cpu_1", 
            "text": "def contoursDetectCPU(pix):\n    seuil = 20\n\n    pixRes = np.zeros(pix.shape)\n    x = pix.shape[0]\n    y= pix.shape[1]\n\n    for j in range(1,y-1):\n        temp = []\n        for i in range(1,x-1):\n\n            diffX = sum(pix[i-1][j])-sum(pix[i+1][j])\n            diffY = sum(pix[i][j-1])-sum(pix[i][j+1])\n\n            if(diffX   seuil)or(diffY   seuil):\n                col = (0,0,0)\n            else:\n                col = (255,255,255)\n            pixRes[i][j] = col\n    return(pixRes)", 
            "title": "Impl\u00e9mentation CPU"
        }, 
        {
            "location": "/traitementImage/#implementation-gpu_1", 
            "text": "#fonction kernel\nmod = SourceModule( \n#include  stdio.h \n#include  stdlib.h \n__global__ void contour(float *im,float* res, int* x, int* y)\n{\nint seuil = 20;\nint size = int(x)*3;\nint begin = threadIdx.x*3;\n\nfor(int j = 1; j 1000-1;j+=10){\n    for(int i = 3; i   size-3 ;i+=10*3){\n        int idx = i+begin + (threadIdx.y*1000*3) + (j*1000*3);\n\n        int caseD = idx + 3; int caseG = idx - 3; int caseH = idx - (1000*3); int caseB = idx + (1000*3);\n\n        int diffX = abs(im[caseD]+im[caseD+1]+im[caseD+2]-(im[caseG]+im[caseG+1]+im[caseG+2]));\n        int diffY = abs(im[caseH]+im[caseH+1]+im[caseH+2]-(im[caseB]+im[caseB+1]+im[caseB+2]));\n\n        if((diffX seuil) (diffY seuil)){\n            res[idx] = 0;  res[idx+1] = 0; res[idx+2] = 0;\n        }\n        else{\n            res[idx] = 255; res[idx+1] = 255; res[idx+2] = 255;\n        }\n    }\n}\n\n\n\n} )\n\ndef contoursDetectCPU(pix):\n    pixRes = np.zeros(pix.shape)\n    x = np.int32(pix.shape[0])\n    y = np.int32(pix.shape[1])\n\n    # converti en float simple precision\n    pix = pix.astype(np.float32)\n\n    # alloue memoire gpu\n    im_gpu = cuda.mem_alloc(pix.nbytes)\n    res_gpu = cuda.mem_alloc(pix.nbytes)\n    x_gpu = cuda.mem_alloc(sys.getsizeof(x))\n    y_gpu = cuda.mem_alloc(sys.getsizeof(y))\n\n    # transfer array args\n    cuda.memcpy_htod(im_gpu, pix)\n    cuda.memcpy_htod(res_gpu, np.empty_like(pix))\n\n    func = mod.get_function( contour )\n    func(im_gpu, res_gpu, x , y , block=(10,100,1))\n    result = np.empty_like(pix)\n\n    cuda.memcpy_dtoh(result, res_gpu)\n    return(result)", 
            "title": "Impl\u00e9mentation GPU"
        }, 
        {
            "location": "/traitementImage/#resultats", 
            "text": "L'implementation du detecteur de contours s'\u00e9xecute en X ms sur CPU et Y ms sur GPU pour une image. Encore une fois la m\u00e9thode est basique, et nos nouvelles connaissances nous permettrait de faire mieux (Produit de convolution d'un certain filtre, le produit de convulution ayant surement des optimisations majeurs sur CPU et GPU. Nos r\u00e9sultats sont donc repr\u00e9sentatif par rapport a leurs impl\u00e9mentions seulement.)", 
            "title": "R\u00e9sultats"
        }, 
        {
            "location": "/traitementImage/#backgroud-remover", 
            "text": "", 
            "title": "Backgroud Remover"
        }, 
        {
            "location": "/traitementImage/#implementation-cpu_2", 
            "text": "def bgRemoveCPU(bg,pix):\n    seuil = 10\n    pixRes = np.zeros(pix.shape)\n    x,y = (pix.shape[0],pix.shape[1])\n\n    for j in range(1,y):\n        for i in range(1,x):\n            diff = abs(sum(pix[i][j])/3-sum(bg[i][j]/3))\n\n            if(diff   seuil):    col = pix[i][j]\n            else:                col = (255,255,255)\n\n            pixRes[i][j] = col\n\n    return(pixRes)", 
            "title": "Impl\u00e9mentation CPU"
        }, 
        {
            "location": "/traitementImage/#implementation-gpu_2", 
            "text": "#fonction kernel\nmod = SourceModule( \n#include  stdio.h \n#include  stdlib.h \n\n__global__ void bgremove(float *bg, float * im,float* res, int* x, int* y)\n{\nint seuil = 10;\nint size = int(x)*3;\nint begin = threadIdx.x*3;\n\nfor(int j = 1; j 1000-1;j+=10){\n    for(int i = 3; i   size-3 ;i+=10*3){\n        int idx = i+begin + (threadIdx.y*1000*3) + (j*1000*3);\n\n        int intIm = (im[idx]+im[idx+1]+im[idx+2])/3;\n        int intBG = (bg[idx]+bg[idx+1]+bg[idx+2])/3;\n\n        int diff = abs(intIm - intBG);\n\n        if(diff seuil){\n            res[idx] = im[idx]; res[idx+1] = im[idx+1]; res[idx+2] = im[idx+2];\n\n        }\n        else{\n            res[idx] = 255; res[idx+1] = 255; res[idx+2] = 255;\n        }\n    }\n}\n\n\n\n} )\n\ndef bgRemoveGPU(pixBG,pixIm):\n\n\n    print  debut algo gpu \n    t0 = time.time()\n\n    x = np.int32(pixBG.shape[0])\n    y = np.int32(pixBG.shape[1])\n\n    # converti en float simple precision\n    pixBG = pixBG.astype(np.float32)\n    pixIm = pixIm.astype(np.float32)\n\n    # alloue memoire gpu\n    BG_gpu = cuda.mem_alloc(pixBG.nbytes)\n    im_gpu = cuda.mem_alloc(pixIm.nbytes)\n    res_gpu = cuda.mem_alloc(pixBG.nbytes)\n    x_gpu = cuda.mem_alloc(sys.getsizeof(x))\n    y_gpu = cuda.mem_alloc(sys.getsizeof(y))\n\n    # transfer array args\n    cuda.memcpy_htod(BG_gpu, pixBG)\n    cuda.memcpy_htod(im_gpu, pixIm)\n    cuda.memcpy_htod(res_gpu, np.empty_like(pixBG))\n\n    func = mod.get_function( bgremove )\n    func(BG_gpu, im_gpu,res_gpu,x , y , block=(10,100,1))\n    result = np.empty_like(pixBG)\n\n    cuda.memcpy_dtoh(result, res_gpu)\n\n    tn = abs(t0-time.time())\n    print  fin algo gpu en  ,tn,   s \n    return(result)", 
            "title": "Impl\u00e9mentation GPU"
        }, 
        {
            "location": "/traitementImage/#resultats_1", 
            "text": "Mon impl\u00e9mentation s'\u00e9xecute en X ms sur CPU et Y ms sur GPU, cependant le r\u00e9sultat est moyen, et ne fonctionne pas en mode vi\u00e9deo pour une raison qui m'\u00e9chappe..    Fond:      Image a traiter:      Image sans le fond:", 
            "title": "Resultats"
        }, 
        {
            "location": "/optimisation/", 
            "text": "Optimisation\n\n\nDivergence de Threads\n\n\nLes threads d'un bloc sont regroup\u00e9s en warps de taille fixe pour l'ex\u00e9cution sur un noyau CUDA, et les threads dans un warp doivent suivre la m\u00eame trajectoire d'ex\u00e9cution. Tous les threads doivent ex\u00e9cuter la m\u00eame instruction en m\u00eame temps. En d'autres termes, les threads ne peuvent pas diverger.\n\n\nIF-THEN-ELSE\n\n\nLa construction de code la plus commune qui peut provoquer une divergence de thread sont les branchements conditionnels dans une instruction if-then-else. Si certains threads d'un seul warp sont \u00e9valu\u00e9s comme \u00abvrais\u00bb et d'autres comme \u00abfaux\u00bb, alors les threads \u00abvrai\u00bb et \u00abfaux\u00bb se ramifient \u00e0 des instructions diff\u00e9rentes. Certains threads voudront passer \u00e0 l'instruction 'then', tandis que d'autres 'else'.\n\n\nIntuitivement, nous pensons que les instructions seront ex\u00e9cut\u00e9s en parall\u00e8le. Cependant, les threads dans un block ne peut pas diverger,et donc la parrallelisation est donc impossible.CUDA a une solution de contournement qui corrige le probl\u00e8me, mais avec des cons\u00e9quences assez n\u00e9gatives sur la performance.\n\n\nLors de l'ex\u00e9cution de l'instruction if-then-else, CUDA ordonne au warp d'ex\u00e9cuter la premi\u00e8re partie puis de passer \u00e0 la partie else. Lors de l'ex\u00e9cution de la premiere partie (if), tous les threads qui sont \u00e9valu\u00e9s \u00e0 faux (else) sont d\u00e9sactiv\u00e9s. Lorsque l'ex\u00e9cution passe \u00e0 l'\u00e9tat else, la situation est invers\u00e9e. Les diff\u00e9rentes instructions ne sont donc pas ex\u00e9cut\u00e9es en parall\u00e8le, mais en s\u00e9rie. Cette s\u00e9rialisation peut entra\u00eener une perte de performances significative.\n\n\nExemple de dead_lock :\n\n\n\n//my_Func_then and my_Func_else are some device functions\nif (threadidx.x \n16)\n{\n    myFunc_then();\n    __syncthread();\n}else if (threadidx \n=16)\n{\n    myFunc_else();\n    __syncthread();\n}\n\n\n\n\n\nLa premi\u00e8re partie execute ses instruction puis attend que la seconde partie du block atteigne __syncthread(), cependant la seconde partie ne commence jamais et par cons\u00e9quent empeche la cloture de la premi\u00e8re instruction. (Nous sommes ici avec des warps de taille \n 16 threads).\n\n\nM\u00e9moire coalesc\u00e9e\n\n\nUn acc\u00e8s en m\u00e9moire est dit coalesc\u00e9 lorsque plusieurs threads d'un warp acc\u00e8dent en m\u00eame temps \u00e0 \u00e0 de la m\u00e9moire globale.\nLes conditions suivantes peuvent r\u00e9sulter en un acc\u00e8s non-coalesc\u00e9, le rendant donc s\u00e9rializ\u00e9 :\n\n\n\n\nLa m\u00e9moire n'est pas s\u00e9quentielle\n\n\nL'acc\u00e8s \u00e0 la m\u00e9moire est faible\n\n\nUn acc\u00e8s \u00e0 la m\u00e9moire d\u00e9salign\u00e9\n\n\n\n\nAcc\u00e8s s\u00e9quentiel et align\u00e9\n\n\nIci l'acc\u00e8s est s\u00e9quentiel et align\u00e9, il est donc coalesc\u00e9.\n\n\nAcc\u00e8s s\u00e9quentiel et align\u00e9: \n \n\n\nAcc\u00e8s align\u00e9 mais non s\u00e9quentiel\n\n\nIci l'acc\u00e8s n'est pas s\u00e9quentiel mais align\u00e9. Sur certaines architectures, principalement les plus anciennes ce genre d'ac\u00e8s ne peut pas \u00eatre effectu\u00e9 en une seule transaction.\n\n\nAcc\u00e8s align\u00e9 mais non s\u00e9quentiel: \n\n\nAcc\u00e8s m\u00e9moire non align\u00e9\n\n\nDans ce cas, l'acc\u00e8s \u00e0 la m\u00e9moire est bien s\u00e9quentiel mais non align\u00e9, du coup il nous faut une transaction de plus pour charger la derniere valeur si l'alignement m\u00e9moire est de 128 Bytes.\n\n\nL'alignement\n\n\nLes pointeurs allou\u00e9s par cudaMalloc ou cudaMallocPitch sont garanti d'avoir X bytes align\u00e9s (i.e l'adresse est un multiple de X) avec X le nombre de bytes indiqu\u00e9 par le \"Texture alignement\" (deviceQuery). Pour la Jetson TK1 notre alignement est de 512 bytes.\n\n\nExemple\n\n\n\nchar *ptr1, *ptr2;\n\nint bytes = 1;\n\ncudaMalloc((void**)\nptr1,bytes);\ncudaMalloc((void**)\nptr2,bytes);\n\n\n\n\n\nEn supossant que l'adresse retourn\u00e9 par ptr1 est un multiple de 512, alors l'adresse retourn\u00e9e dans ptr2 seras au minimun (ptr1 + 512).\nCette restriction est impos\u00e9 par le device sur lequel la m\u00e9moire est allou\u00e9, principalement pour des raisons de performances.\n\n\nAcc\u00e8s non align\u00e9: \n\n\nExemple\n\n\nUn acc\u00e8s en m\u00e9moire coalesc\u00e9 ressemble \u00e0 ce genre de code. \n\n\nshared_memory[threadIdx.x]= global_memory[blockIdx.x*blockDim.x + threadIdx.x];\n\n\n\n\nL'instruction suivante n'est quand \u00e0 elle pas coalesc\u00e9e.\n\n\nstride=4;\nshared_memory[threadIdx.x]= global_memory[stride*blockIdx.x*blockDim.x + threadIdx.x*stride];\n\n\n\n\nRappel\n\n\n\n\nLa m\u00e9moire globale est lente, la m\u00e9moire \"on-chip\" est beaucoup plus rapide\n\n\nEviter en tout moment la divergence de threads\n\n\nDes acc\u00e8s en m\u00e9moire coalesc\u00e9s permettent d'am\u00e9liorer grandement la performance \n\n\n\n\nDeroulage de boucles\n\n\nLe deroulage de boucle est une technique d'optimisation consistant \u00e0 am\u00e9liorer la vitesse d'execution d'un programme au d\u00e9pend de sa taille binaire\nCette am\u00e9lioration de vitesse s'obtient en r\u00e9duisant ou en \u00e9limant les actions controllant la boucle comme les pointeurs arithm\u00e9tiques et les tests de fin de boucle \u00e0 chaque it\u00e9ration,\nainsi qu'en \u00e9liminant les latences de lecture en m\u00e9moire.\n\n\nExemple de deroulage de boucle simple en C :\n\n\n\n//boucle normale\n int x;\n for (x = 0; x \n 100; x++)\n {\n     delete(x);\n }\n\n//boucle d\u00e9roul\u00e9e\n int x; \n for (x = 0; x \n 100; x += 5)\n {\n     delete(x);\n     delete(x + 1);\n     delete(x + 2);\n     delete(x + 3);\n     delete(x + 4);\n }\n\n\n\n\n\nApr\u00e8s cette modification le programme n'effectue que 20 it\u00e9rations au lieu des 100 pr\u00e9cedentes et seulement 20% des branches conditionelles seront prises.\n\n\nCe d\u00e9roulage doit \u00eatre effectu\u00e9 avec pr\u00e9caution afin que le controle de fin de boucle et le nombre d'op\u00e9rations \u00e0 l'int\u00e9rieur de la boucle concordent :\nPar exemple dans le cas ci dessus, si le nombre d'it\u00e9rations n'\u00e9tait pas divisible par 5, le programme ne fonctionnerait pas.", 
            "title": "Optimisation GPU"
        }, 
        {
            "location": "/optimisation/#optimisation", 
            "text": "", 
            "title": "Optimisation"
        }, 
        {
            "location": "/optimisation/#divergence-de-threads", 
            "text": "Les threads d'un bloc sont regroup\u00e9s en warps de taille fixe pour l'ex\u00e9cution sur un noyau CUDA, et les threads dans un warp doivent suivre la m\u00eame trajectoire d'ex\u00e9cution. Tous les threads doivent ex\u00e9cuter la m\u00eame instruction en m\u00eame temps. En d'autres termes, les threads ne peuvent pas diverger.", 
            "title": "Divergence de Threads"
        }, 
        {
            "location": "/optimisation/#if-then-else", 
            "text": "La construction de code la plus commune qui peut provoquer une divergence de thread sont les branchements conditionnels dans une instruction if-then-else. Si certains threads d'un seul warp sont \u00e9valu\u00e9s comme \u00abvrais\u00bb et d'autres comme \u00abfaux\u00bb, alors les threads \u00abvrai\u00bb et \u00abfaux\u00bb se ramifient \u00e0 des instructions diff\u00e9rentes. Certains threads voudront passer \u00e0 l'instruction 'then', tandis que d'autres 'else'.  Intuitivement, nous pensons que les instructions seront ex\u00e9cut\u00e9s en parall\u00e8le. Cependant, les threads dans un block ne peut pas diverger,et donc la parrallelisation est donc impossible.CUDA a une solution de contournement qui corrige le probl\u00e8me, mais avec des cons\u00e9quences assez n\u00e9gatives sur la performance.  Lors de l'ex\u00e9cution de l'instruction if-then-else, CUDA ordonne au warp d'ex\u00e9cuter la premi\u00e8re partie puis de passer \u00e0 la partie else. Lors de l'ex\u00e9cution de la premiere partie (if), tous les threads qui sont \u00e9valu\u00e9s \u00e0 faux (else) sont d\u00e9sactiv\u00e9s. Lorsque l'ex\u00e9cution passe \u00e0 l'\u00e9tat else, la situation est invers\u00e9e. Les diff\u00e9rentes instructions ne sont donc pas ex\u00e9cut\u00e9es en parall\u00e8le, mais en s\u00e9rie. Cette s\u00e9rialisation peut entra\u00eener une perte de performances significative.", 
            "title": "IF-THEN-ELSE"
        }, 
        {
            "location": "/optimisation/#exemple-de-dead_lock", 
            "text": "//my_Func_then and my_Func_else are some device functions\nif (threadidx.x  16)\n{\n    myFunc_then();\n    __syncthread();\n}else if (threadidx  =16)\n{\n    myFunc_else();\n    __syncthread();\n}  La premi\u00e8re partie execute ses instruction puis attend que la seconde partie du block atteigne __syncthread(), cependant la seconde partie ne commence jamais et par cons\u00e9quent empeche la cloture de la premi\u00e8re instruction. (Nous sommes ici avec des warps de taille   16 threads).", 
            "title": "Exemple de dead_lock :"
        }, 
        {
            "location": "/optimisation/#memoire-coalescee", 
            "text": "Un acc\u00e8s en m\u00e9moire est dit coalesc\u00e9 lorsque plusieurs threads d'un warp acc\u00e8dent en m\u00eame temps \u00e0 \u00e0 de la m\u00e9moire globale.\nLes conditions suivantes peuvent r\u00e9sulter en un acc\u00e8s non-coalesc\u00e9, le rendant donc s\u00e9rializ\u00e9 :   La m\u00e9moire n'est pas s\u00e9quentielle  L'acc\u00e8s \u00e0 la m\u00e9moire est faible  Un acc\u00e8s \u00e0 la m\u00e9moire d\u00e9salign\u00e9", 
            "title": "M\u00e9moire coalesc\u00e9e"
        }, 
        {
            "location": "/optimisation/#acces-sequentiel-et-aligne", 
            "text": "Ici l'acc\u00e8s est s\u00e9quentiel et align\u00e9, il est donc coalesc\u00e9.  Acc\u00e8s s\u00e9quentiel et align\u00e9:", 
            "title": "Acc\u00e8s s\u00e9quentiel et align\u00e9"
        }, 
        {
            "location": "/optimisation/#acces-aligne-mais-non-sequentiel", 
            "text": "Ici l'acc\u00e8s n'est pas s\u00e9quentiel mais align\u00e9. Sur certaines architectures, principalement les plus anciennes ce genre d'ac\u00e8s ne peut pas \u00eatre effectu\u00e9 en une seule transaction.  Acc\u00e8s align\u00e9 mais non s\u00e9quentiel:", 
            "title": "Acc\u00e8s align\u00e9 mais non s\u00e9quentiel"
        }, 
        {
            "location": "/optimisation/#acces-memoire-non-aligne", 
            "text": "Dans ce cas, l'acc\u00e8s \u00e0 la m\u00e9moire est bien s\u00e9quentiel mais non align\u00e9, du coup il nous faut une transaction de plus pour charger la derniere valeur si l'alignement m\u00e9moire est de 128 Bytes.", 
            "title": "Acc\u00e8s m\u00e9moire non align\u00e9"
        }, 
        {
            "location": "/optimisation/#lalignement", 
            "text": "Les pointeurs allou\u00e9s par cudaMalloc ou cudaMallocPitch sont garanti d'avoir X bytes align\u00e9s (i.e l'adresse est un multiple de X) avec X le nombre de bytes indiqu\u00e9 par le \"Texture alignement\" (deviceQuery). Pour la Jetson TK1 notre alignement est de 512 bytes.", 
            "title": "L'alignement"
        }, 
        {
            "location": "/optimisation/#exemple", 
            "text": "char *ptr1, *ptr2;\n\nint bytes = 1;\n\ncudaMalloc((void**) ptr1,bytes);\ncudaMalloc((void**) ptr2,bytes);  En supossant que l'adresse retourn\u00e9 par ptr1 est un multiple de 512, alors l'adresse retourn\u00e9e dans ptr2 seras au minimun (ptr1 + 512).\nCette restriction est impos\u00e9 par le device sur lequel la m\u00e9moire est allou\u00e9, principalement pour des raisons de performances.  Acc\u00e8s non align\u00e9:", 
            "title": "Exemple"
        }, 
        {
            "location": "/optimisation/#exemple_1", 
            "text": "Un acc\u00e8s en m\u00e9moire coalesc\u00e9 ressemble \u00e0 ce genre de code.   shared_memory[threadIdx.x]= global_memory[blockIdx.x*blockDim.x + threadIdx.x];  L'instruction suivante n'est quand \u00e0 elle pas coalesc\u00e9e.  stride=4;\nshared_memory[threadIdx.x]= global_memory[stride*blockIdx.x*blockDim.x + threadIdx.x*stride];", 
            "title": "Exemple"
        }, 
        {
            "location": "/optimisation/#rappel", 
            "text": "La m\u00e9moire globale est lente, la m\u00e9moire \"on-chip\" est beaucoup plus rapide  Eviter en tout moment la divergence de threads  Des acc\u00e8s en m\u00e9moire coalesc\u00e9s permettent d'am\u00e9liorer grandement la performance", 
            "title": "Rappel"
        }, 
        {
            "location": "/optimisation/#deroulage-de-boucles", 
            "text": "Le deroulage de boucle est une technique d'optimisation consistant \u00e0 am\u00e9liorer la vitesse d'execution d'un programme au d\u00e9pend de sa taille binaire\nCette am\u00e9lioration de vitesse s'obtient en r\u00e9duisant ou en \u00e9limant les actions controllant la boucle comme les pointeurs arithm\u00e9tiques et les tests de fin de boucle \u00e0 chaque it\u00e9ration,\nainsi qu'en \u00e9liminant les latences de lecture en m\u00e9moire.  Exemple de deroulage de boucle simple en C :  \n//boucle normale\n int x;\n for (x = 0; x   100; x++)\n {\n     delete(x);\n }\n\n//boucle d\u00e9roul\u00e9e\n int x; \n for (x = 0; x   100; x += 5)\n {\n     delete(x);\n     delete(x + 1);\n     delete(x + 2);\n     delete(x + 3);\n     delete(x + 4);\n }  Apr\u00e8s cette modification le programme n'effectue que 20 it\u00e9rations au lieu des 100 pr\u00e9cedentes et seulement 20% des branches conditionelles seront prises.  Ce d\u00e9roulage doit \u00eatre effectu\u00e9 avec pr\u00e9caution afin que le controle de fin de boucle et le nombre d'op\u00e9rations \u00e0 l'int\u00e9rieur de la boucle concordent :\nPar exemple dans le cas ci dessus, si le nombre d'it\u00e9rations n'\u00e9tait pas divisible par 5, le programme ne fonctionnerait pas.", 
            "title": "Deroulage de boucles"
        }, 
        {
            "location": "/reduction/", 
            "text": "Optimisation d'une Reduction Parall\u00e8le en CUDA\n\n\nLa reduction parrall\u00e8le consiste \u00e0 assembler chaque \u00e9l\u00e9ment d'un tableau afin de n'obtenir qu'un \u00e9l\u00e9ment final repr\u00e9sentant ce tableau (pour un tableau d'entier il s'agit d'une somme).\n\n\nNous allons ici nous servir d'une reduction parall\u00e8le d'un tableau comme exemple d'optimisation, un exemple classique dans la litt\u00e9rature concernant l'optimisation en CUDA, car cette r\u00e9duction est facile \u00e0 impl\u00e9menter.\nUne r\u00e9duction contient du parall\u00e8lisme mais est difficile \u00e0 exploiter car plus les calculs progressent et moins il y a de parall\u00e9lisme, et il y beaucoup d'acc\u00e8s aux donn\u00e9es et de calculs. \nLa r\u00e9duction est difficile de l'optimiser au maximun mais il est possible de d\u00e9couper cette optimisation \u00e9tape par \u00e9tape.\n\n\nL'impl\u00e9mentation se feras ici avec un tableau de taille cons\u00e9quante afin de pouvoir comparer les diff\u00e9rents niveaux d'optimisation et afin de pouvoir utiliser les m\u00e9caniques de blocs de CUDA.\nDe plus en utilisant une taille de tableau assez grande pour occuper la totalit\u00e9 des threads disponibles pour notre device, nous pouvons facilement observer les gains de performances caus\u00e9s par une diminution de threads peu voir non actifs.\n\n\nReduction 1 : Adressage \u00e0 intervalle\n\n\nUne approche de la r\u00e9duction parrall\u00e8le consiste \u00e0 utiliser un arbre : \n\n\n   \n\n\nAfin d'impl\u00e9menter cette r\u00e9presentation nous allons devoir utiliser plusieurs blocs, chacun r\u00e9duisant une partie du tableau.\nQuel choix effectuer pour la communication des r\u00e9sulatts entre les diffrents blocs ? \nEn synchronisant tout les threads de tout les blocs il serait facile de communiquer les r\u00e9sultats, il nous suffirait d'effectuer la reduction de mani\u00e8re recursive dans chaque blocs jusqu'\u00e0 atteindr eun r\u00e9sultat unique \u00e0 partager.\n\n\nProbl\u00e8me : comment effectuer une synchronisation globale\n\n\nCUDA n'impl\u00e9mente pas de synchronisation globale pour deux raisons :\n    - Pour \u00e9viter un certains nobmre de deadlocks possibles.\n    - Car cela c\u00f4ute cher \u00e0 cr\u00e9er au niveau hardware lorsque que le nombre de processeur est grand.\n\n\nNous allons donc devoir d\u00e9composer notre fonction en de multiples kernels en utilisant un premier thread comme point de synchronisation global.\nDans notre cas, celui d'une r\u00e9duction, nous pouvons utiliser le m\u00eame code pour chaque kenerl, notre invocation de kernel est donc r\u00e9cursive.\n\n\n    \n\n\nExemple d'un kernel basique de r\u00e9duction \n\n\n\n//g_idata est l'output et g_idata est l'input\n__global__ void reduce0(int *g_idata, int *g_odata, int size){\n\n    //vecteur de sortie partag\u00e9 avec tout les threads dans un bloc\n    extern __shared__ int sdata[];\n\n    //On recupere l'endroit ou l'on ecrit dans la memoire partag\u00e9 (tid) ainsi que la case que nous allons traiter (i)\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n    //Dans la cas o\u00f9 nous ne sommes pas dans le tableau nous renvoyons 0\n    sdata[tid] = 0;\n    if (i\nsize)\n        sdata[tid] = g_idata[i];\n\n    //__syncthreads permet de synchroniser tout les threads afin \n    __syncthreads();\n\n    for (unsigned int s = 1; s \n blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n\n            sdata[tid] += sdata[tid + s];\n\n        }\n        __syncthreads();\n    }\n\n    //On \u00e9crit le r\u00e9sultat de ce bloc en m\u00e9moire globale\n    if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}\n\n\n\n\n\n   \n\n\nExemple d'appel de ce kernel de mani\u00e8re r\u00e9cursive :\n\n\n\n//Nous passons notre kenerl en param\u00e8tre \u00e0 notre fonction de benchmarking ainsi que la taille du vecteur \u00e0 r\u00e9duire\nvoid benchmark(int size, void(*fptr)(int*, int*, int)){\n    //Le nombre de threads par blocs que nous allons utiliser\n    int threadsPerBlock = 1024;\n    //Le nombre totals de blocs que nous devons utiliser afin de r\u00e9duire l'enti\u00e9ret\u00e9 du vecteur\n    int totalBlocks = (size + (threadsPerBlock - 1)) / threadsPerBlock;\n\n    //Utilisation de la librairie thrust afin de cr\u00e9eer facilement des vecteurs host et devices permettant le transfert des donn\u00e9es d'input et d'ouput (cf doc)\n    thrust::host_vector\nint\n data_h_i(size, 1);\n    thrust::device_vector\nint\n data_v_i = data_h_i;\n    thrust::device_vector\nint\n data_v_o(totalBlocks);\n    thrust::device_vector\nint\n data_v_o_final(totalBlocks / threadsPerBlock);\n\n    int* output = thrust::raw_pointer_cast(data_v_o.data());\n    int* input = thrust::raw_pointer_cast(data_v_i.data());\n    int* ouput_f = thrust::raw_pointer_cast(data_v_o_final.data());\n\n    //Invocation du kernel\n\n    fptr \ntotalBlocks, threadsPerBlock, threadsPerBlock*sizeof(int) \n(input, output, size);\n    if (totalBlocks / threadsPerBlock \n 1){\n        //Apr\u00e8s le premier appel si nous utilisons plus de threads que disponible dans un seul bloc, nous devons rappeler le kernel afin de r\u00e9duire notre vecteur de r\u00e9sultat\n        fptr \n \ntotalBlocks / threadsPerBlock, threadsPerBlock, threadsPerBlock*sizeof(int) \n \n(output, ouput_f, totalBlocks);\n\n        fptr \n \n1, totalBlocks / threadsPerBlock, threadsPerBlock*sizeof(int) \n \n(ouput_f, input, totalBlocks);\n    }\n    else{\n        fptr \n \n1, threadsPerBlock, threadsPerBlock*sizeof(int) \n \n(output, input, totalBlocks);\n    }\n\n    //Nous attendons la fin de l'\u00e9xecution de chaque bloc, les kernels n'\u00e9tant pas bloquant pour le CPU\n    cudaDeviceSynchronize();\n\n    //Traitement des r\u00e9sultats\n    data_v_o[0] = data_v_i[0];\n    data_v_i.clear();\n    data_v_i.shrink_to_fit();\n\n    thrust::host_vector\nint\n data_h_o = data_v_o;\n\n    data_v_o.clear();\n    data_v_o.shrink_to_fit();\n\n    cout \n \nSomme : \n \n data_h_o[0] \n endl;\n}\n\n\n\n\n\nProbl\u00e8me : branchement divergents\n\n\nRappel : Les threads d'un bloc sont regroup\u00e9s en warps de taille fixe pour l'ex\u00e9cution sur un noyau CUDA, et les threads dans un warp doivent suivre la m\u00eame trajectoire d'ex\u00e9cution. Tous les threads doivent ex\u00e9cuter la m\u00eame instruction en m\u00eame temps. En d'autres termes, les threads ne peuvent pas diverger.\n\n\nReduction 2 : Kernel\n\n\n\n//g_idata est l'output et g_idata est l'input\n__global__ void reduce1(int *g_idata, int *g_odata, int size){\n\n    //vecteur de sortie partag\u00e9 avec tout les threads dans un bloc\n    extern __shared__ int sdata[];\n\n    //On recupere l'endroit ou l'on ecrit dans la memoire partag\u00e9 (tid) ainsi que la case que nous allons traiter (i)\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n    //Dans la cas o\u00f9 nous ne sommes pas dans le tableau nous renvoyons 0\n    sdata[tid] = 0;\n    if (i\nsize)\n        sdata[tid] = g_idata[i];\n\n    //__syncthreads permet de synchroniser tout les threads afin \n    __syncthreads();\n\n    for (unsigned int s = 1; s \n blockDim.x; s *= 2) {\n        int index = 2 * s * tid;\n\n        if (index \n blockDim.x){\n            sdata[index] += sdata[index + s];\n        }\n        __syncthreads();\n    }\n\n\n    //On \u00e9crit le r\u00e9sultat de ce bloc en m\u00e9moire globale\n    if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}\n\n\n\n\n\nProbl\u00e8me : Conflit des bancs de m\u00e9moires partag\u00e9es\n\n\nReduction 3 : Adressement S\u00e9quentiel\n\n\nNous remplacons l'index strided de la boucle par une boucle invers\u00e9 et un index bas\u00e9 sur l'id des threads.\n\n\n\n\n    for (unsigned int s =blockDim.x / 2; s\n0; s \n= 1) {\n        if(tid \n s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n\n\n\n\n\n \n\n\nProbl\u00e8me : Threads inactifs", 
            "title": "Optimisation/Reduction"
        }, 
        {
            "location": "/reduction/#optimisation-dune-reduction-parallele-en-cuda", 
            "text": "La reduction parrall\u00e8le consiste \u00e0 assembler chaque \u00e9l\u00e9ment d'un tableau afin de n'obtenir qu'un \u00e9l\u00e9ment final repr\u00e9sentant ce tableau (pour un tableau d'entier il s'agit d'une somme).  Nous allons ici nous servir d'une reduction parall\u00e8le d'un tableau comme exemple d'optimisation, un exemple classique dans la litt\u00e9rature concernant l'optimisation en CUDA, car cette r\u00e9duction est facile \u00e0 impl\u00e9menter.\nUne r\u00e9duction contient du parall\u00e8lisme mais est difficile \u00e0 exploiter car plus les calculs progressent et moins il y a de parall\u00e9lisme, et il y beaucoup d'acc\u00e8s aux donn\u00e9es et de calculs. \nLa r\u00e9duction est difficile de l'optimiser au maximun mais il est possible de d\u00e9couper cette optimisation \u00e9tape par \u00e9tape.  L'impl\u00e9mentation se feras ici avec un tableau de taille cons\u00e9quante afin de pouvoir comparer les diff\u00e9rents niveaux d'optimisation et afin de pouvoir utiliser les m\u00e9caniques de blocs de CUDA.\nDe plus en utilisant une taille de tableau assez grande pour occuper la totalit\u00e9 des threads disponibles pour notre device, nous pouvons facilement observer les gains de performances caus\u00e9s par une diminution de threads peu voir non actifs.", 
            "title": "Optimisation d'une Reduction Parall\u00e8le en CUDA"
        }, 
        {
            "location": "/reduction/#reduction-1-adressage-a-intervalle", 
            "text": "Une approche de la r\u00e9duction parrall\u00e8le consiste \u00e0 utiliser un arbre :        Afin d'impl\u00e9menter cette r\u00e9presentation nous allons devoir utiliser plusieurs blocs, chacun r\u00e9duisant une partie du tableau.\nQuel choix effectuer pour la communication des r\u00e9sulatts entre les diffrents blocs ? \nEn synchronisant tout les threads de tout les blocs il serait facile de communiquer les r\u00e9sultats, il nous suffirait d'effectuer la reduction de mani\u00e8re recursive dans chaque blocs jusqu'\u00e0 atteindr eun r\u00e9sultat unique \u00e0 partager.", 
            "title": "Reduction 1 : Adressage \u00e0 intervalle"
        }, 
        {
            "location": "/reduction/#probleme-comment-effectuer-une-synchronisation-globale", 
            "text": "CUDA n'impl\u00e9mente pas de synchronisation globale pour deux raisons :\n    - Pour \u00e9viter un certains nobmre de deadlocks possibles.\n    - Car cela c\u00f4ute cher \u00e0 cr\u00e9er au niveau hardware lorsque que le nombre de processeur est grand.  Nous allons donc devoir d\u00e9composer notre fonction en de multiples kernels en utilisant un premier thread comme point de synchronisation global.\nDans notre cas, celui d'une r\u00e9duction, nous pouvons utiliser le m\u00eame code pour chaque kenerl, notre invocation de kernel est donc r\u00e9cursive.        Exemple d'un kernel basique de r\u00e9duction   \n//g_idata est l'output et g_idata est l'input\n__global__ void reduce0(int *g_idata, int *g_odata, int size){\n\n    //vecteur de sortie partag\u00e9 avec tout les threads dans un bloc\n    extern __shared__ int sdata[];\n\n    //On recupere l'endroit ou l'on ecrit dans la memoire partag\u00e9 (tid) ainsi que la case que nous allons traiter (i)\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n    //Dans la cas o\u00f9 nous ne sommes pas dans le tableau nous renvoyons 0\n    sdata[tid] = 0;\n    if (i size)\n        sdata[tid] = g_idata[i];\n\n    //__syncthreads permet de synchroniser tout les threads afin \n    __syncthreads();\n\n    for (unsigned int s = 1; s   blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n\n            sdata[tid] += sdata[tid + s];\n\n        }\n        __syncthreads();\n    }\n\n    //On \u00e9crit le r\u00e9sultat de ce bloc en m\u00e9moire globale\n    if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}       Exemple d'appel de ce kernel de mani\u00e8re r\u00e9cursive :  \n//Nous passons notre kenerl en param\u00e8tre \u00e0 notre fonction de benchmarking ainsi que la taille du vecteur \u00e0 r\u00e9duire\nvoid benchmark(int size, void(*fptr)(int*, int*, int)){\n    //Le nombre de threads par blocs que nous allons utiliser\n    int threadsPerBlock = 1024;\n    //Le nombre totals de blocs que nous devons utiliser afin de r\u00e9duire l'enti\u00e9ret\u00e9 du vecteur\n    int totalBlocks = (size + (threadsPerBlock - 1)) / threadsPerBlock;\n\n    //Utilisation de la librairie thrust afin de cr\u00e9eer facilement des vecteurs host et devices permettant le transfert des donn\u00e9es d'input et d'ouput (cf doc)\n    thrust::host_vector int  data_h_i(size, 1);\n    thrust::device_vector int  data_v_i = data_h_i;\n    thrust::device_vector int  data_v_o(totalBlocks);\n    thrust::device_vector int  data_v_o_final(totalBlocks / threadsPerBlock);\n\n    int* output = thrust::raw_pointer_cast(data_v_o.data());\n    int* input = thrust::raw_pointer_cast(data_v_i.data());\n    int* ouput_f = thrust::raw_pointer_cast(data_v_o_final.data());\n\n    //Invocation du kernel\n\n    fptr  totalBlocks, threadsPerBlock, threadsPerBlock*sizeof(int)  (input, output, size);\n    if (totalBlocks / threadsPerBlock   1){\n        //Apr\u00e8s le premier appel si nous utilisons plus de threads que disponible dans un seul bloc, nous devons rappeler le kernel afin de r\u00e9duire notre vecteur de r\u00e9sultat\n        fptr    totalBlocks / threadsPerBlock, threadsPerBlock, threadsPerBlock*sizeof(int)    (output, ouput_f, totalBlocks);\n\n        fptr    1, totalBlocks / threadsPerBlock, threadsPerBlock*sizeof(int)    (ouput_f, input, totalBlocks);\n    }\n    else{\n        fptr    1, threadsPerBlock, threadsPerBlock*sizeof(int)    (output, input, totalBlocks);\n    }\n\n    //Nous attendons la fin de l'\u00e9xecution de chaque bloc, les kernels n'\u00e9tant pas bloquant pour le CPU\n    cudaDeviceSynchronize();\n\n    //Traitement des r\u00e9sultats\n    data_v_o[0] = data_v_i[0];\n    data_v_i.clear();\n    data_v_i.shrink_to_fit();\n\n    thrust::host_vector int  data_h_o = data_v_o;\n\n    data_v_o.clear();\n    data_v_o.shrink_to_fit();\n\n    cout    Somme :     data_h_o[0]   endl;\n}", 
            "title": "Probl\u00e8me : comment effectuer une synchronisation globale"
        }, 
        {
            "location": "/reduction/#probleme-branchement-divergents", 
            "text": "Rappel : Les threads d'un bloc sont regroup\u00e9s en warps de taille fixe pour l'ex\u00e9cution sur un noyau CUDA, et les threads dans un warp doivent suivre la m\u00eame trajectoire d'ex\u00e9cution. Tous les threads doivent ex\u00e9cuter la m\u00eame instruction en m\u00eame temps. En d'autres termes, les threads ne peuvent pas diverger.", 
            "title": "Probl\u00e8me : branchement divergents"
        }, 
        {
            "location": "/reduction/#reduction-2-kernel", 
            "text": "//g_idata est l'output et g_idata est l'input\n__global__ void reduce1(int *g_idata, int *g_odata, int size){\n\n    //vecteur de sortie partag\u00e9 avec tout les threads dans un bloc\n    extern __shared__ int sdata[];\n\n    //On recupere l'endroit ou l'on ecrit dans la memoire partag\u00e9 (tid) ainsi que la case que nous allons traiter (i)\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n    //Dans la cas o\u00f9 nous ne sommes pas dans le tableau nous renvoyons 0\n    sdata[tid] = 0;\n    if (i size)\n        sdata[tid] = g_idata[i];\n\n    //__syncthreads permet de synchroniser tout les threads afin \n    __syncthreads();\n\n    for (unsigned int s = 1; s   blockDim.x; s *= 2) {\n        int index = 2 * s * tid;\n\n        if (index   blockDim.x){\n            sdata[index] += sdata[index + s];\n        }\n        __syncthreads();\n    }\n\n\n    //On \u00e9crit le r\u00e9sultat de ce bloc en m\u00e9moire globale\n    if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}", 
            "title": "Reduction 2 : Kernel"
        }, 
        {
            "location": "/reduction/#probleme-conflit-des-bancs-de-memoires-partagees", 
            "text": "", 
            "title": "Probl\u00e8me : Conflit des bancs de m\u00e9moires partag\u00e9es"
        }, 
        {
            "location": "/reduction/#reduction-3-adressement-sequentiel", 
            "text": "Nous remplacons l'index strided de la boucle par une boucle invers\u00e9 et un index bas\u00e9 sur l'id des threads.  \n\n    for (unsigned int s =blockDim.x / 2; s 0; s  = 1) {\n        if(tid   s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }", 
            "title": "Reduction 3 : Adressement S\u00e9quentiel"
        }, 
        {
            "location": "/reduction/#probleme-threads-inactifs", 
            "text": "", 
            "title": "Probl\u00e8me : Threads inactifs"
        }, 
        {
            "location": "/signRecognition/", 
            "text": "Langage des signes\n\n\nMise en place d'un classifieur d'images pour distinguer les diff\u00e9rentes lettres du langages des signes francais, et en r\u00e9aliser une transcription automatique.\n\n\nIl sera, dans un premier temps, classifi\u00e9 les lettres distinguables sans mouvements (le traitement d'images \u00e9tant moins complexe que le traitement vid\u00e9o), et sans les mots/phrases qui poss\u00e8dent un signe/mouvements chacuns\n\n\nPremi\u00e8re it\u00e9ration\n\n\nOutils\n\n\nTensorflow\n\n\nDataset\n\nTraining : \nretrain.py\n\nEval : \nlabel.py\n  \n\n\nWorkflow\n\n\nUtilisation de docker pour d\u00e9veloppement ind\u00e9pendant de la machine h\u00f4te (dans mon cas, depuis windows).\n\nLe probleme de docker est qu'il ne peut tirer parti du GPU (sur windows!, sur les systemes compatibles, il existe \nnvidia-docker\n, \u00e0 tester sur la Jetson donc.)  \n\n\nDocker (quick start terminal):\n\n\ndocker run -it gcr.io/tensorflow/tensorflow:latest-devel   # Cr\u00e9e un container docker qui contient la derniere version de tensorflow/tensorflow  \nmkdir /tf_files   # Cr\u00e9e un dossier pour r\u00e9cuperer les fichiers utiles  \nexit   # Quite la vm pour y lier les tf_files\n\ndocker run -it -v $HOME/tf_files:/tf_files gcr.io/tensorflow/tensorflow:latest-devel   # lance le container avec /tf_files en commun avec le pc\n\n# ouverture du terminal du container (si besoin):  \n# docker attach \nname\n\n\n# Lance l'entrainement sur les fichiers jpg du dossier /tf_files/signSplit      \ncd /tensorflow\npython tensorflow/examples/image_retraining/retrain.py \\\n    --bottleneck_dir=/tf_files/bottlenecks \\\n    --how_many_training_steps 4000 \\\n    --model_dir=/tf_files/inception \\\n    --output_graph=/tf_files/retrained_graph.pb \\\n    --output_labels=/tf_files/retrained_labels.txt \\\n    --image_dir /tf_files/signSplit\n\n# Lance la pr\u00e9diction\npython /tf_files/label.py /tf_files/sign/\nnumber\n.jpg\n\n\n\nContenu de tf_files\n\n\nsign/  \n    A/  \n    B/  \n    C/  \nsignSplit/  \n    A/  \n    B/  \n    C/\nlabel_image.py  \nmodif.py   # Renomme les fichiers de train  \nsplitData.py   # Coupe les donn\u00e9es de train\n\n\n\nconvertion images ppm en jpg\n\n\n# install imagemagick \nsudo apt-get install imagemagick\nconvert *.ppm %d.jpg\n\n\n\nPremiers R\u00e9sultats\n\n\nUne classification sur 3 signes distincts avec environ 100 images pour chaque cas d'entrainement mene a une exactitude (final test accuracy) de 94%  \n\n\nD'autres campagnes de tests automatis\u00e9s seronts \u00e0 r\u00e9aliser pour produire des sch\u00e9mas de performances de ce modeles, pour essayer d'en trouver les faiblesses dans notre contexte de la reconnaissance du langages des signes, pour enfin essayer d'augmenter les performances de classification. \nLes tests s'av\u00e8rents relativememnt bons sur le meme dataset (mais des images qu'il n'as pas appris), cependant, apres un test sur une image de l'exterieur du dataset, les resultats sont m\u00e9diocres.\n\nNous avons mis aux point un script de generation de dataset a l'aide de la webcam (linux uniquement, paquets streamer et tkinter nec\u00e9ssaires).\n\nA tester donc.\n\n\nA voir pour la deuxieme exp\u00e9rimentation\n\n\nNvidia docker\n\nInstallation de docker sur ubuntu \n3 versions\n ou \n16.04\n\n\nInstallation de nvidia-docker\n\n\nTensorflow image on nvidia-docker\n  \n\n\nDeuxi\u00e8me it\u00e9ration\n\n\nNos premiers essais de classifications des signes \u00e9taient un echec, cependant nous nous attendions a ce r\u00e9sultat, mais il fallait commencer par la pour avancer.\n\nL'id\u00e9e \u00e9tait de mettre en place un classifier d'image quelquonque (nous avons utilis\u00e9 le (...retail?) de tensorflow) sur un dataset trouv\u00e9 sur internet comportant des images de signes.\n\nNous avons fait des essais sur des donn\u00e9es d'apprentissages vari\u00e9s, mais peu nombreuses (50aine de photos, s\u00e9par\u00e9s en 3 signes distincts.)\nEn s\u00e9parant le corpus en corpus de tests et d'apprentissage, les r\u00e9sultats semblait plutot bons (~60%), cependant, les essais r\u00e9els \u00e9tait m\u00e9diocre (non mesur\u00e9s, mais je dirai maximum 5 % de bonnes classifications).\n\nNous avons d\u00e9duit de cette exp\u00e9rience que les donn\u00e9es d'apprentissages importe \u00e9normement sur la classification, et que les conditions/environements de capture sont tr\u00e8s influents.\n\nIl nous faudra donc faire du traitement d'image, pour, a minima retirer le maximum de fond de l'image, et si possible, couper les parties de l'image des mains pour les isoler, et y appliquer des traitements, pour le training, et la classification.\n\nOn espere aussi r\u00e9duire la taille des donn\u00e9es en entr\u00e9e du systeme pour permetre des calculs plus rapide.\n\n\nTraitements d'image utilis\u00e9s\n\n\nGrayscale\n\n\nLe passage des images en niveau de gris permet de grandement diminuer la taille de celle-ci, en revanche ce traitement est destructif pour des informations qui peuvent \u00eatre capitale.\n\nOn appliquera ainsi ce traitement, mais pas forc\u00e9ment au d\u00e9but de la chaine de traitement.  \n\n\nBlur-Gauss\n\n\nApplication d'un filtre qui permet de flouter legerement l'image. Cela nous permet de diminuer les d\u00e9tails bruit\u00e9s de l'images, pour se concentrer sur les formes plus importantes.\n\n\n\n\nBG-Remove\n\n\nCe traitement permet d'extraire l'information qui n'est pas immobile sur l'image.\n\nElle se base sur la capture de plusieurs trames, et une soustraction des pixels qui n'ont pas chang\u00e9, avec un certain controle par des parametres de r\u00e9glages.\n\nCette technique consomme relativement peu de ressource par rapport au gain d'espace de travail utile.\n\nL'utilisation de cette technique permet \u00e9galement un affinage en fonction du temps. En effet, lorsque l'utilisateur arrive dans l'image, la zone analys\u00e9 est assez grande, mais celle ci se reduit tr\u00e8s vite car l'utilisateur bouge peu ( relativement a la main qu'il pr\u00e9sente a la cam\u00e9ra )  \n\n\nContours/Convex Hull/Boundingbox\n\n\nL'algorithme de recherche des contours permet de trouver des zones des couleurs semblables, et tracer leurs coutours.  \n\n\n\n\nA partir de ces resultats, on peut extraire les convex-hull. Il s'agit de d\u00e9tourer au mieux une forme, en retirant ainsi beaucoup d'information (les coutours d\u00e9taill\u00e9s) en conservant celle qui nous int\u00e9ressents.\n\n\n  \n\n\nA partir de ces zones sont facilement extrayable les boudings-box (zones rectangle ou \u00e9llipso\u00efdales), qui bornent les zones qui nous interressents. En dilattant les convex hull, ou les bounding-box, on permet d'etre plus large sur l'image que l'on va scrop.  \n\n\n\n\n  \n\n\nCascade Haar / Viola and Jones\n\n\nL'agorithme de detection en cascade est une technique de reconnaissance d'objet dans une image.\n\nCet algorithme est utilis\u00e9 depuis 2001 pour la localisation de visages dans les images. Cependant, il peut apprendre a localiser n'importe quel type d'objets, et de trouver les bounding boxes des objets qu'on lui aura fait apprendre.  \n\n\nElle se base sur le calcul des zones de couleurs d'ondelettes, et un entrainement permettant de d\u00e9duire un modele de vraisemblance pour ces ondelettes.\n\nLe calcul semble a premiere vu \u00e9norme, mais en convertissant dans un format addapt\u00e9 pour ce calcul, il se fait en 4 op\u00e9rations basiques.  \n\n\nAinsi il nous permettrait, soit de remplacer notre modele pr\u00e9c\u00e9dent (bg remove/convexhull, meme si aucun traitement concernant les mains n'\u00e9tait encore incorpor\u00e9), soit le renforcer (avant ou apres, ou en prenant une d\u00e9cision par rapport aux r\u00e9ponses des 2 systemes.)  \n\n\nDe plus, les versions a l'\u00e9tat de l'art l'utilisent de facon cascade (Cascade Classifier), ce qui nous permettrait aussi de faire la reconnaissance \u00e0 proprement parler des signes de la main.  \n\n\n \n\n \n\n \n\n\nImpl\u00e9mentation Python et GPU:\n\n\nAvec les librairiesPython \nOpenCV\n et \nOpenCV GPU\n, d\u00e9tecter des objets avec un fichier d'entrainement est tr\u00e8s facile:  \n\n\nif cv2gpu.is_cuda_compatible():  \n    cv2gpu.init_gpu_detector(cascade_file_gpu)  # training file\n\ndetections = cv2gpu.find_faces(image_file)\n\n\n\nD\u00e9finition du besoin\n\n\nIl nous maintenant faire des essais de la meilleure combinaison de systemes.\n\nL'impact du sens d'application des traitements \u00e9tant importants et peu pr\u00e9visible, cela demanderais trop de temps et nous allons faire un maximum d'essais, en fonction de nos pr\u00e9visions.\n\nCependant rien ne nous indique que nous optiendrons le meilleur mod\u00e8le.  \n\n\nIl faut enfin cr\u00e9er un programme de cr\u00e9ation de dataset a l'aide de webcam, pour obtenir plus de donn\u00e9es d'entrainement.\n\nCe programme n'\u00e9tant pas a priori compliqu\u00e9, il faut n\u00e9anmoins qu'il soit facilement parametrable pour cr\u00e9er des datasets de tailles variables (pour nos tests, et pour un d\u00e9ployement imaginable).\n\nOn pourrait nottament penser a une interface de parametrage par l'utilisateur de ces datasets. Il pourrait ainsi ajouter lui meme des signes dont nous ignorons l'existence.\n\n\nIl nous faut d\u00e9finir des campagnes de tests adapt\u00e9s pour juger nos syst\u00e8mes, et nos datasets, cependant, je ne pense pas qu'on ai le temps de tester les datasets ( j'imagine qu'en comparant la vraisemblance avec les modeles d\u00e9ja appris on pourrais filtrer les cas ou l'utilisateur s'es tromp\u00e9, mais on sors du  cadre du projet). En revanche, il faut qu'on puisse tester automatiquement les systemes.", 
            "title": "Reconnaissance des signes"
        }, 
        {
            "location": "/signRecognition/#langage-des-signes", 
            "text": "Mise en place d'un classifieur d'images pour distinguer les diff\u00e9rentes lettres du langages des signes francais, et en r\u00e9aliser une transcription automatique.  Il sera, dans un premier temps, classifi\u00e9 les lettres distinguables sans mouvements (le traitement d'images \u00e9tant moins complexe que le traitement vid\u00e9o), et sans les mots/phrases qui poss\u00e8dent un signe/mouvements chacuns", 
            "title": "Langage des signes"
        }, 
        {
            "location": "/signRecognition/#premiere-iteration", 
            "text": "", 
            "title": "Premi\u00e8re it\u00e9ration"
        }, 
        {
            "location": "/signRecognition/#outils", 
            "text": "Tensorflow  Dataset \nTraining :  retrain.py \nEval :  label.py", 
            "title": "Outils"
        }, 
        {
            "location": "/signRecognition/#workflow", 
            "text": "Utilisation de docker pour d\u00e9veloppement ind\u00e9pendant de la machine h\u00f4te (dans mon cas, depuis windows). \nLe probleme de docker est qu'il ne peut tirer parti du GPU (sur windows!, sur les systemes compatibles, il existe  nvidia-docker , \u00e0 tester sur la Jetson donc.)", 
            "title": "Workflow"
        }, 
        {
            "location": "/signRecognition/#docker-quick-start-terminal", 
            "text": "docker run -it gcr.io/tensorflow/tensorflow:latest-devel   # Cr\u00e9e un container docker qui contient la derniere version de tensorflow/tensorflow  \nmkdir /tf_files   # Cr\u00e9e un dossier pour r\u00e9cuperer les fichiers utiles  \nexit   # Quite la vm pour y lier les tf_files\n\ndocker run -it -v $HOME/tf_files:/tf_files gcr.io/tensorflow/tensorflow:latest-devel   # lance le container avec /tf_files en commun avec le pc\n\n# ouverture du terminal du container (si besoin):  \n# docker attach  name \n\n# Lance l'entrainement sur les fichiers jpg du dossier /tf_files/signSplit      \ncd /tensorflow\npython tensorflow/examples/image_retraining/retrain.py \\\n    --bottleneck_dir=/tf_files/bottlenecks \\\n    --how_many_training_steps 4000 \\\n    --model_dir=/tf_files/inception \\\n    --output_graph=/tf_files/retrained_graph.pb \\\n    --output_labels=/tf_files/retrained_labels.txt \\\n    --image_dir /tf_files/signSplit\n\n# Lance la pr\u00e9diction\npython /tf_files/label.py /tf_files/sign/ number .jpg", 
            "title": "Docker (quick start terminal):"
        }, 
        {
            "location": "/signRecognition/#contenu-de-tf_files", 
            "text": "sign/  \n    A/  \n    B/  \n    C/  \nsignSplit/  \n    A/  \n    B/  \n    C/\nlabel_image.py  \nmodif.py   # Renomme les fichiers de train  \nsplitData.py   # Coupe les donn\u00e9es de train  convertion images ppm en jpg  # install imagemagick \nsudo apt-get install imagemagick\nconvert *.ppm %d.jpg", 
            "title": "Contenu de tf_files"
        }, 
        {
            "location": "/signRecognition/#premiers-resultats", 
            "text": "Une classification sur 3 signes distincts avec environ 100 images pour chaque cas d'entrainement mene a une exactitude (final test accuracy) de 94%    D'autres campagnes de tests automatis\u00e9s seronts \u00e0 r\u00e9aliser pour produire des sch\u00e9mas de performances de ce modeles, pour essayer d'en trouver les faiblesses dans notre contexte de la reconnaissance du langages des signes, pour enfin essayer d'augmenter les performances de classification. \nLes tests s'av\u00e8rents relativememnt bons sur le meme dataset (mais des images qu'il n'as pas appris), cependant, apres un test sur une image de l'exterieur du dataset, les resultats sont m\u00e9diocres. \nNous avons mis aux point un script de generation de dataset a l'aide de la webcam (linux uniquement, paquets streamer et tkinter nec\u00e9ssaires). \nA tester donc.", 
            "title": "Premiers R\u00e9sultats"
        }, 
        {
            "location": "/signRecognition/#a-voir-pour-la-deuxieme-experimentation", 
            "text": "Nvidia docker \nInstallation de docker sur ubuntu  3 versions  ou  16.04  Installation de nvidia-docker  Tensorflow image on nvidia-docker", 
            "title": "A voir pour la deuxieme exp\u00e9rimentation"
        }, 
        {
            "location": "/signRecognition/#deuxieme-iteration", 
            "text": "Nos premiers essais de classifications des signes \u00e9taient un echec, cependant nous nous attendions a ce r\u00e9sultat, mais il fallait commencer par la pour avancer. \nL'id\u00e9e \u00e9tait de mettre en place un classifier d'image quelquonque (nous avons utilis\u00e9 le (...retail?) de tensorflow) sur un dataset trouv\u00e9 sur internet comportant des images de signes. \nNous avons fait des essais sur des donn\u00e9es d'apprentissages vari\u00e9s, mais peu nombreuses (50aine de photos, s\u00e9par\u00e9s en 3 signes distincts.)\nEn s\u00e9parant le corpus en corpus de tests et d'apprentissage, les r\u00e9sultats semblait plutot bons (~60%), cependant, les essais r\u00e9els \u00e9tait m\u00e9diocre (non mesur\u00e9s, mais je dirai maximum 5 % de bonnes classifications). \nNous avons d\u00e9duit de cette exp\u00e9rience que les donn\u00e9es d'apprentissages importe \u00e9normement sur la classification, et que les conditions/environements de capture sont tr\u00e8s influents. \nIl nous faudra donc faire du traitement d'image, pour, a minima retirer le maximum de fond de l'image, et si possible, couper les parties de l'image des mains pour les isoler, et y appliquer des traitements, pour le training, et la classification. \nOn espere aussi r\u00e9duire la taille des donn\u00e9es en entr\u00e9e du systeme pour permetre des calculs plus rapide.", 
            "title": "Deuxi\u00e8me it\u00e9ration"
        }, 
        {
            "location": "/signRecognition/#traitements-dimage-utilises", 
            "text": "", 
            "title": "Traitements d'image utilis\u00e9s"
        }, 
        {
            "location": "/signRecognition/#grayscale", 
            "text": "Le passage des images en niveau de gris permet de grandement diminuer la taille de celle-ci, en revanche ce traitement est destructif pour des informations qui peuvent \u00eatre capitale. \nOn appliquera ainsi ce traitement, mais pas forc\u00e9ment au d\u00e9but de la chaine de traitement.", 
            "title": "Grayscale"
        }, 
        {
            "location": "/signRecognition/#blur-gauss", 
            "text": "Application d'un filtre qui permet de flouter legerement l'image. Cela nous permet de diminuer les d\u00e9tails bruit\u00e9s de l'images, pour se concentrer sur les formes plus importantes.", 
            "title": "Blur-Gauss"
        }, 
        {
            "location": "/signRecognition/#bg-remove", 
            "text": "Ce traitement permet d'extraire l'information qui n'est pas immobile sur l'image. \nElle se base sur la capture de plusieurs trames, et une soustraction des pixels qui n'ont pas chang\u00e9, avec un certain controle par des parametres de r\u00e9glages. \nCette technique consomme relativement peu de ressource par rapport au gain d'espace de travail utile. \nL'utilisation de cette technique permet \u00e9galement un affinage en fonction du temps. En effet, lorsque l'utilisateur arrive dans l'image, la zone analys\u00e9 est assez grande, mais celle ci se reduit tr\u00e8s vite car l'utilisateur bouge peu ( relativement a la main qu'il pr\u00e9sente a la cam\u00e9ra )", 
            "title": "BG-Remove"
        }, 
        {
            "location": "/signRecognition/#contoursconvex-hullboundingbox", 
            "text": "L'algorithme de recherche des contours permet de trouver des zones des couleurs semblables, et tracer leurs coutours.     A partir de ces resultats, on peut extraire les convex-hull. Il s'agit de d\u00e9tourer au mieux une forme, en retirant ainsi beaucoup d'information (les coutours d\u00e9taill\u00e9s) en conservant celle qui nous int\u00e9ressents.      A partir de ces zones sont facilement extrayable les boudings-box (zones rectangle ou \u00e9llipso\u00efdales), qui bornent les zones qui nous interressents. En dilattant les convex hull, ou les bounding-box, on permet d'etre plus large sur l'image que l'on va scrop.", 
            "title": "Contours/Convex Hull/Boundingbox"
        }, 
        {
            "location": "/signRecognition/#cascade-haar-viola-and-jones", 
            "text": "L'agorithme de detection en cascade est une technique de reconnaissance d'objet dans une image. \nCet algorithme est utilis\u00e9 depuis 2001 pour la localisation de visages dans les images. Cependant, il peut apprendre a localiser n'importe quel type d'objets, et de trouver les bounding boxes des objets qu'on lui aura fait apprendre.    Elle se base sur le calcul des zones de couleurs d'ondelettes, et un entrainement permettant de d\u00e9duire un modele de vraisemblance pour ces ondelettes. \nLe calcul semble a premiere vu \u00e9norme, mais en convertissant dans un format addapt\u00e9 pour ce calcul, il se fait en 4 op\u00e9rations basiques.    Ainsi il nous permettrait, soit de remplacer notre modele pr\u00e9c\u00e9dent (bg remove/convexhull, meme si aucun traitement concernant les mains n'\u00e9tait encore incorpor\u00e9), soit le renforcer (avant ou apres, ou en prenant une d\u00e9cision par rapport aux r\u00e9ponses des 2 systemes.)    De plus, les versions a l'\u00e9tat de l'art l'utilisent de facon cascade (Cascade Classifier), ce qui nous permettrait aussi de faire la reconnaissance \u00e0 proprement parler des signes de la main.", 
            "title": "Cascade Haar / Viola and Jones"
        }, 
        {
            "location": "/signRecognition/#implementation-python-et-gpu", 
            "text": "Avec les librairiesPython  OpenCV  et  OpenCV GPU , d\u00e9tecter des objets avec un fichier d'entrainement est tr\u00e8s facile:    if cv2gpu.is_cuda_compatible():  \n    cv2gpu.init_gpu_detector(cascade_file_gpu)  # training file\n\ndetections = cv2gpu.find_faces(image_file)", 
            "title": "Impl\u00e9mentation Python et GPU:"
        }, 
        {
            "location": "/signRecognition/#definition-du-besoin", 
            "text": "Il nous maintenant faire des essais de la meilleure combinaison de systemes. \nL'impact du sens d'application des traitements \u00e9tant importants et peu pr\u00e9visible, cela demanderais trop de temps et nous allons faire un maximum d'essais, en fonction de nos pr\u00e9visions. \nCependant rien ne nous indique que nous optiendrons le meilleur mod\u00e8le.    Il faut enfin cr\u00e9er un programme de cr\u00e9ation de dataset a l'aide de webcam, pour obtenir plus de donn\u00e9es d'entrainement. \nCe programme n'\u00e9tant pas a priori compliqu\u00e9, il faut n\u00e9anmoins qu'il soit facilement parametrable pour cr\u00e9er des datasets de tailles variables (pour nos tests, et pour un d\u00e9ployement imaginable). \nOn pourrait nottament penser a une interface de parametrage par l'utilisateur de ces datasets. Il pourrait ainsi ajouter lui meme des signes dont nous ignorons l'existence.  Il nous faut d\u00e9finir des campagnes de tests adapt\u00e9s pour juger nos syst\u00e8mes, et nos datasets, cependant, je ne pense pas qu'on ai le temps de tester les datasets ( j'imagine qu'en comparant la vraisemblance avec les modeles d\u00e9ja appris on pourrais filtrer les cas ou l'utilisateur s'es tromp\u00e9, mais on sors du  cadre du projet). En revanche, il faut qu'on puisse tester automatiquement les systemes.", 
            "title": "D\u00e9finition du besoin"
        }, 
        {
            "location": "/ressources/", 
            "text": "Ressources\n\n\nCi-joint, une liste des ressources qui nous ont \u00e9t\u00e9 utiles:  \n\n\n\n\n\n\nNVIDIA\n  \n\n\n\n\nSite NVIDIA, contient de multiples documentations a propos des technologies CUDA, et CuDNN. \nContient \u00e9galement les liens vers les diff\u00e9rents outils qu'ils proposent.  \n\n\n\n\n\n\n\n\nCUDA Zone\n\n\n\n\nAccueil d\u00e9veloppeurs Nvidia CUDA  \n\n\n\n\n\n\n\n\nJetsonHacks\n  \n\n\n\n\nSite proposant \u00e9normement de contenu en relation avec les cartes Jetson et leurs applications.  \n\n\n\n\n\n\n\n\nSirajology \nYoutube\n / \nGithub\n  \n\n\n\n\nNombreuses vid\u00e9os concernant Tensorflow, et le machine-learning. Sources sur github  \n\n\n\n\n\n\n\n\nMachine/Deep Learning reading-list\n\n\n\n\nListe d'articles scientifiques sur diff\u00e9rents domaines du machine-learning ou deep-learning.", 
            "title": "Ressources"
        }, 
        {
            "location": "/ressources/#ressources", 
            "text": "Ci-joint, une liste des ressources qui nous ont \u00e9t\u00e9 utiles:      NVIDIA      Site NVIDIA, contient de multiples documentations a propos des technologies CUDA, et CuDNN. \nContient \u00e9galement les liens vers les diff\u00e9rents outils qu'ils proposent.       CUDA Zone   Accueil d\u00e9veloppeurs Nvidia CUDA       JetsonHacks      Site proposant \u00e9normement de contenu en relation avec les cartes Jetson et leurs applications.       Sirajology  Youtube  /  Github      Nombreuses vid\u00e9os concernant Tensorflow, et le machine-learning. Sources sur github       Machine/Deep Learning reading-list   Liste d'articles scientifiques sur diff\u00e9rents domaines du machine-learning ou deep-learning.", 
            "title": "Ressources"
        }, 
        {
            "location": "/outils/", 
            "text": "Outils\n\n\nCette page regroupe les diff\u00e9rentes instructions pour collaborer sur les outils dans les meilleurs conditions  \n\n\n\n\nGithub\n\n\nPlateforme de gestion de version\n\n\n\n\nInstallation\n\n\n\n\nsudo apt-get install git  \n\n\n\n\n\n\nAjouter le repo\n\n\n\n\ngit remote add upstream https://github.com/matEhickey/Projet-CUDA-M2  \ngit fetch upstream  \n\n\n\n\n\n\nR\u00e9cup\u00e9rer les (nouveaux) fichiers  \n\n\n\n\ngit pull https://github.com/matEhickey/Projet-CUDA-M2\n\n\n\n\n\n\nAjout des changements dans les fichier et dossiers de l'emplacement ou l'on se trouve\n\n\n\n\n    git add .\n\n\n\n\n\n\nCommit des changements (Titre DESCRIPTIF et CLAIR \nOBLIGATOIRE\n, descriptions d\u00e9taill\u00e9s appr\u00e9ci\u00e9s)\n\n\n\n\n    git commit -a\n\n\n\n\n\n\nEnvoi des changements\n\n\n\n\ngit push https://github.com/matEhickey/Projet-CUDA-M2\n\n\n\n\n\n\nMkdocs\n\n\nOutil de production de documentation\n  \n\n\n\n\nInstallation\n\n\n\n\npip install mkdocs\n\n\n\n\n\n\nNouveau projet\n\n\n\n\nmkdocs new projectName\n\n\n\n\n\n\nServe (serveur web de developemment, MAJ auto a partir des fichier .md)\n\n\n\n\nmkdocs serve\n\n\n\n\n\n\nBuild (g\u00e9n\u00e8re site web statique)\n\n\n\n\nmkdocs build\n\n\n\n\n\n\n\n\nMarkdown syntax\n\n\n\n\n\n\nMkdocs configuration\n\n\n\n\n\n\n\n\nSharelatex\n\n\nPlateforme de redaction d'articles collaborative bas\u00e9 sur Latex\n  \n\n\ncompte =\n projet.cuda@laposte.net\nmdp =\n projetcuda2016 \n\n\n\n\nSharelatex\n\n\nLatex syntax", 
            "title": "Outils"
        }, 
        {
            "location": "/outils/#outils", 
            "text": "Cette page regroupe les diff\u00e9rentes instructions pour collaborer sur les outils dans les meilleurs conditions", 
            "title": "Outils"
        }, 
        {
            "location": "/outils/#github", 
            "text": "Plateforme de gestion de version   Installation   sudo apt-get install git     Ajouter le repo   git remote add upstream https://github.com/matEhickey/Projet-CUDA-M2  \ngit fetch upstream     R\u00e9cup\u00e9rer les (nouveaux) fichiers     git pull https://github.com/matEhickey/Projet-CUDA-M2   Ajout des changements dans les fichier et dossiers de l'emplacement ou l'on se trouve       git add .   Commit des changements (Titre DESCRIPTIF et CLAIR  OBLIGATOIRE , descriptions d\u00e9taill\u00e9s appr\u00e9ci\u00e9s)       git commit -a   Envoi des changements   git push https://github.com/matEhickey/Projet-CUDA-M2", 
            "title": "Github"
        }, 
        {
            "location": "/outils/#mkdocs", 
            "text": "Outil de production de documentation      Installation   pip install mkdocs   Nouveau projet   mkdocs new projectName   Serve (serveur web de developemment, MAJ auto a partir des fichier .md)   mkdocs serve   Build (g\u00e9n\u00e8re site web statique)   mkdocs build    Markdown syntax    Mkdocs configuration", 
            "title": "Mkdocs"
        }, 
        {
            "location": "/outils/#sharelatex", 
            "text": "Plateforme de redaction d'articles collaborative bas\u00e9 sur Latex     compte =  projet.cuda@laposte.net\nmdp =  projetcuda2016   Sharelatex  Latex syntax", 
            "title": "Sharelatex"
        }, 
        {
            "location": "/about/", 
            "text": "About\n\n\nProjet d'\u00e9tudes r\u00e9alis\u00e9 \u00e0 l'Universit\u00e9 du Mans, Institut Claude CHAPPE de Octobre 2016 \u00e0 Janvier 2017\n\n\nEquipe\n\n\nMathias DIDIER, Benoit LETAY, Mahamat ANNOUR, Ronan YSEUX\n\n\nEncadrants\n\n\nFlorent Carlier\n\nVal\u00e9rie RENAULT\n\n\nRemerciements", 
            "title": "About"
        }, 
        {
            "location": "/about/#about", 
            "text": "Projet d'\u00e9tudes r\u00e9alis\u00e9 \u00e0 l'Universit\u00e9 du Mans, Institut Claude CHAPPE de Octobre 2016 \u00e0 Janvier 2017", 
            "title": "About"
        }, 
        {
            "location": "/about/#equipe", 
            "text": "Mathias DIDIER, Benoit LETAY, Mahamat ANNOUR, Ronan YSEUX", 
            "title": "Equipe"
        }, 
        {
            "location": "/about/#encadrants", 
            "text": "Florent Carlier \nVal\u00e9rie RENAULT", 
            "title": "Encadrants"
        }, 
        {
            "location": "/about/#remerciements", 
            "text": "", 
            "title": "Remerciements"
        }, 
        {
            "location": "/sceances/", 
            "text": "S\u00e9ance1: Mise en place des outils pour la gestion du Projet.\n\n\nMise en place de l'environnement\n- github\n- sharelatex\n- Trello\n- cr\u00e9ation de groupe facebook\n\n\nS\u00e9ance2:Installation de Ubuntu.\n\n\n\n\nt\u00e9l\u00e9chargement du fichier image depuis le site du telechargement d'ubuntu 14.04.* (version compatible jetpack le 14/08/16) :\nhttp://releases.ubuntu.com/trusty/ \n\n\n\n\nchoix de la version compatible avec le processeur ( amd64/i386)\n ubuntu-14.04.4-desktop-amd64.iso    \n\n ubuntu-14.04.4-desktop-i386.iso\n\n\n\n\n\n\non peut verifier si le fichier est bien t\u00e9l\u00e9charg\u00e9 (pas endomag\u00e9). Pour se faire, nous utilisons la commande md5 dans mac ou linux (Commande: sudo md5 nom_fichier_image). elle (la commande) traite l'entr\u00e9e et renvoie un num\u00e9ro qui est \u00e0 comparer avec le site de t\u00e9l\u00e9chargement.\n\n\n\n\navant de commencer l'installation assurez-vous de flasher ce fichier dans une cl\u00e9 Usb ou de le graver dans un cd ou dvd, pour pouvoir booter dessus(demarrer le syst\u00e8me)\n\n\nmodifier l'ordre de boot du syst\u00e9me dans le menu BIOS( mettre USB en premier).\n acc\u00e8s au bios (afin de changer le boot order pour mettre la cl\u00e9 usb avant les autres) F10 (la cl\u00e9 doit \u00eatre branch\u00e9e afin de la faire apparaitre dans le boot order)\n\n\n\n\nVoici quelques \u00e9tapes de l'installation\n- choix de la langue : anglais\n- install ubuntu\n- download update during installing (pas s\u00e9l\u00e9ctionner car internet nous faisait d\u00e9faut)\n- install third-party software\n- erase disk and install ubuntu\n- selectionner time-zone and keyboard configuration\n- ubuntu\n\n\nProbl\u00e9me1: probl\u00e8me d'installation de ubuntu.\n\n\nli\u00e9e au blockage de t\u00e9l\u00e9chargement des paquets par le serveur de l'universit\u00e9\nUtilisation des CheckSum pour verifer l'integrit\u00e9 du fichier Image.\n\n\n\nS\u00e9ance3:Telechargement de JetPackL4T et installation.\n\n\nProbl\u00e9me2: probl\u00e8me de t\u00e9l\u00e9chargement des paquets JetPack.\n\n\nce probl\u00e8me est li\u00e9 au probl\u00e8mes de proxy.\n\n\n\nS\u00e9ance4:Installation de JetPackL4T \n flashage de la carte.\n\n\nVoici quelques \u00e9tapes suivi durant l'installation de JetPackL4T.\n\n\n\n\n\n\nt\u00e9l\u00e9chargement du fichier d'installation du JETPACK depuis le site internet de nvidia : https://developer.nvidia.com/embedded/downloads\n    NB: Il faut avoir un compte pour pouvoir t\u00e9l\u00e9charger JetPack.\n\n\n\n\n\n\nA la base le fichier n'a pas de droit d'execution. Nous lui ajoutons donc le droit d'execution avec la Commande suivante: --- chmod +x nomfichier.run\n\n\n\n\n\n\nEt enfin nous lan\u00e7ons l'execution de ce fichier avec la commande suivante : \n\n\n\n\n./nomfichier.run \n    NB: soyez sur que la carte est branch\u00e9 \u00e0 la machine qui execute ce programme et qu'elle est \u00e9galement connect\u00e9 \u00e0 internet.\n\n\n\n\nProbl\u00e9me3: probl\u00e8me d'installation des paquets JetPack.\n\n\nLa connexion internet bloque syst\u00e9matiquement le t\u00e9l\u00e9chargement de paquets depuis internet si le proxy de la fac n'est pas correctement renseign\u00e9.\n\n\n\nl'installation ne marche pas. Pour se faire, nous avons les commandes suivantes \n- sudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com votre_n\u00b0_de_cl\u00e9\n- sudo apt-get update --fix-missing\n\n\npour garder certains dependence lorsque le upgrade les supprimes.\n- sudo apt-get dist-upgrade\n\n\nLes proxy ne marchent toujours pas. Il faut le for\u00e7er en utilisant les commandes ci-dessous.\n\n\n\n\ncd /etc/apt/apt.conf.d/\n\n\ntouch 95proxies\n\n\nAcquire::http::proxy \"adresse\";\n\n\nAcquire::ftp::proxy \"adresse\";\n\n\nAcquire::https::proxy \"adresse\";\n\n\n\n\nToutes ces tentatives n'aboutissent \u00e0 rien. Mais apr\u00e8s avoir regl\u00e9 le proxy, le t\u00e9l\u00e9chargement marche correctement. Ci - dessous les commande pour regler le proxy.\n\n\n\n\nsudo su \"on saisit le mot de passe\"\n\n\nexport http_proxy='adresse \u00e0 saisir'\n\n\nexport https_proxy='adresse \u00e0 saisir'\n\n\nexport ftp_proxy='adresse \u00e0 saisir'\n\n\n\n\nProbl\u00e8me4: probl\u00e8me de flash de la carte.\n\n\nla machine n'arrive pas \u00e0 donner une adresse ip \u00e0 la carte car celle-ci n'est pas connu par le r\u00e9seau. Nous avons essayer plusieurs techniques en vain. Il a fallu faire part \u00e0 l'administrateur reseau pour r\u00e9soudre ce probleme. Ci-dessous quelques commande qu'on a utilis\u00e9 mais qui n'ont pas abouti.\n\n\n\nsudo dhclient eth0\nifconfig eth0 192.168.0.1\ngestion de l'adresse ip: enter cette commande sur un terminal dans la machine hote apr\u00e8s qu'elle ai \u00e9t\u00e9 demarr\u00e9.\nsetxkbmap fr\n\n\nseance 6: Installation et Flashage de la carte JETSON TK-1.\n\n\nNous avons recommencer les etapes de la s\u00e9ance 3 qui se sont interrompu\u00e9s \u00e0 cause du proxy et continuer. JETSON TK-1 telecharge plusieurs fichiers depuis internet pour son fonctionnement interne. Mais il s'arr\u00eate \u00e0 cause du proxy.\n\n\nNous \u00e9tions oblig\u00e9 de suspendre l'op\u00e9ration juste apr\u00e8s le flashage et de recommencer \u00e0 partir de la postInstallation.\n\u00e0 la fin du Flah, la machine donne \u00e0 la carte une adresse ip que voici 172.18.41.199.\nApr\u00e8s avoir Flasher la carte, nous allons ajouter les proxy de la fac en editant le fichier .bashrc.\nMaintenant, nous pouvons lancer la postInstallation qui va telecharger des paquets depuis internet. Toutesfois, certains telechargements n'ont pas march\u00e9. Nous avons fait \u00e7a manuellement dans la carte avant de pouvoir continuer.\nVoici entre autres quelques biblioth\u00e9ques t\u00e9l\u00e9charg\u00e9es par JETPACK L4T:\n\n\n\n\nCommon\n\n\nTegra Graphics Debugger\n\n\nTegra System Profiler\n\n\nDocumentation\n\n\nVisionWorks pack On Host\n\n\nVisionWorks on Host\n\n\nVisionWorks Plus On Target\n\n\nVisionWorks Objects Trackers on Host\n\n\nVisionWorks References\n\n\n\n\n\n\n\n\n\n\nGameWorks Samples \n\n\nFor Jetson TK1\n\n\nCUDA Toolkit for Ubuntu 14.04\n\n\nLinux for Tegra(TK1)\n\n\nCUDA Toolkit for L4T\n\n\nCompile GameWorks Sample\n\n\nCompile CUDA Sample\n\n\nPerfKit CuDNN Package\n\n\nOpenCV for Tegra\n\n\nVisionWorks Pack On TK1 Target\n\n\nVisionWorks On Target\n\n\nVisionWorks Plus(SFM) On Target\n\n\nVisionWorks Objects Tracking On Target\n\n\n\n\n\n\n\n\n\n\n\n\nProbl\u00e8me5: probl\u00e8me de flash de la carte (1).\n\n\nPour resoudre definitivement le probl\u00e8me de proxy, nous avons editer le fichier .bashrc et nous lui avons ajouter \u00e0 la fin les lignes suivantes.\n\n\n\nhttp_proxy \"adresse\"\nftp_proxy \"adresse\"\nhttps_proxy \"adresse\"\nApr\u00e8s avoir resolu ce probl\u00e8me l'installation ne s'interromp pas et le flashage est effectif.\n\n\nSeance 7:activation des 4 cpu de la carte.\n\n\nPar defaut un seul CPU est actif. Pour activer les 3 autres, nous utilisons les commandes ci-dessous.\n\n\necho 0 \n /sys/devices/system/cpu/cpuquiet/tegra_cpuquiet/enable\necho 1 \n /sys/devices/system/cpu/cpu0/online\necho 1 \n /sys/devices/system/cpu/cpu1/online\necho 1 \n /sys/devices/system/cpu/cpu2/online\necho 1 \n /sys/devices/system/cpu/cpu3/online\necho performance \n /sys/devices/system/cpu/cpu0/cpufreq/scaling_govern\n\n\nSeance 8:\n\n\nS\u00e9ance 4 \n 5: Redaction du cahier de charge\n\n\n\n\nLa redaction a \u00e9t\u00e9 faite sur Sharelatex.\n\n\nReunion avec les encadrants \n quelques recherches concernant le probl\u00e8me rencontr\u00e9.", 
            "title": "Sc\u00e9ances"
        }, 
        {
            "location": "/sceances/#seance1-mise-en-place-des-outils-pour-la-gestion-du-projet", 
            "text": "Mise en place de l'environnement\n- github\n- sharelatex\n- Trello\n- cr\u00e9ation de groupe facebook", 
            "title": "S\u00e9ance1: Mise en place des outils pour la gestion du Projet."
        }, 
        {
            "location": "/sceances/#seance2installation-de-ubuntu", 
            "text": "t\u00e9l\u00e9chargement du fichier image depuis le site du telechargement d'ubuntu 14.04.* (version compatible jetpack le 14/08/16) :\nhttp://releases.ubuntu.com/trusty/    choix de la version compatible avec le processeur ( amd64/i386)\n ubuntu-14.04.4-desktop-amd64.iso     \n ubuntu-14.04.4-desktop-i386.iso    on peut verifier si le fichier est bien t\u00e9l\u00e9charg\u00e9 (pas endomag\u00e9). Pour se faire, nous utilisons la commande md5 dans mac ou linux (Commande: sudo md5 nom_fichier_image). elle (la commande) traite l'entr\u00e9e et renvoie un num\u00e9ro qui est \u00e0 comparer avec le site de t\u00e9l\u00e9chargement.   avant de commencer l'installation assurez-vous de flasher ce fichier dans une cl\u00e9 Usb ou de le graver dans un cd ou dvd, pour pouvoir booter dessus(demarrer le syst\u00e8me)  modifier l'ordre de boot du syst\u00e9me dans le menu BIOS( mettre USB en premier).\n acc\u00e8s au bios (afin de changer le boot order pour mettre la cl\u00e9 usb avant les autres) F10 (la cl\u00e9 doit \u00eatre branch\u00e9e afin de la faire apparaitre dans le boot order)   Voici quelques \u00e9tapes de l'installation\n- choix de la langue : anglais\n- install ubuntu\n- download update during installing (pas s\u00e9l\u00e9ctionner car internet nous faisait d\u00e9faut)\n- install third-party software\n- erase disk and install ubuntu\n- selectionner time-zone and keyboard configuration\n- ubuntu", 
            "title": "S\u00e9ance2:Installation de Ubuntu."
        }, 
        {
            "location": "/sceances/#probleme1-probleme-dinstallation-de-ubuntu", 
            "text": "li\u00e9e au blockage de t\u00e9l\u00e9chargement des paquets par le serveur de l'universit\u00e9\nUtilisation des CheckSum pour verifer l'integrit\u00e9 du fichier Image.", 
            "title": "Probl\u00e9me1: probl\u00e8me d'installation de ubuntu."
        }, 
        {
            "location": "/sceances/#seance3telechargement-de-jetpackl4t-et-installation", 
            "text": "", 
            "title": "S\u00e9ance3:Telechargement de JetPackL4T et installation."
        }, 
        {
            "location": "/sceances/#probleme2-probleme-de-telechargement-des-paquets-jetpack", 
            "text": "ce probl\u00e8me est li\u00e9 au probl\u00e8mes de proxy.", 
            "title": "Probl\u00e9me2: probl\u00e8me de t\u00e9l\u00e9chargement des paquets JetPack."
        }, 
        {
            "location": "/sceances/#seance4installation-de-jetpackl4t-flashage-de-la-carte", 
            "text": "Voici quelques \u00e9tapes suivi durant l'installation de JetPackL4T.    t\u00e9l\u00e9chargement du fichier d'installation du JETPACK depuis le site internet de nvidia : https://developer.nvidia.com/embedded/downloads\n    NB: Il faut avoir un compte pour pouvoir t\u00e9l\u00e9charger JetPack.    A la base le fichier n'a pas de droit d'execution. Nous lui ajoutons donc le droit d'execution avec la Commande suivante: --- chmod +x nomfichier.run    Et enfin nous lan\u00e7ons l'execution de ce fichier avec la commande suivante :    ./nomfichier.run \n    NB: soyez sur que la carte est branch\u00e9 \u00e0 la machine qui execute ce programme et qu'elle est \u00e9galement connect\u00e9 \u00e0 internet.", 
            "title": "S\u00e9ance4:Installation de JetPackL4T &amp; flashage de la carte."
        }, 
        {
            "location": "/sceances/#probleme3-probleme-dinstallation-des-paquets-jetpack", 
            "text": "La connexion internet bloque syst\u00e9matiquement le t\u00e9l\u00e9chargement de paquets depuis internet si le proxy de la fac n'est pas correctement renseign\u00e9.  l'installation ne marche pas. Pour se faire, nous avons les commandes suivantes \n- sudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com votre_n\u00b0_de_cl\u00e9\n- sudo apt-get update --fix-missing  pour garder certains dependence lorsque le upgrade les supprimes.\n- sudo apt-get dist-upgrade  Les proxy ne marchent toujours pas. Il faut le for\u00e7er en utilisant les commandes ci-dessous.   cd /etc/apt/apt.conf.d/  touch 95proxies  Acquire::http::proxy \"adresse\";  Acquire::ftp::proxy \"adresse\";  Acquire::https::proxy \"adresse\";   Toutes ces tentatives n'aboutissent \u00e0 rien. Mais apr\u00e8s avoir regl\u00e9 le proxy, le t\u00e9l\u00e9chargement marche correctement. Ci - dessous les commande pour regler le proxy.   sudo su \"on saisit le mot de passe\"  export http_proxy='adresse \u00e0 saisir'  export https_proxy='adresse \u00e0 saisir'  export ftp_proxy='adresse \u00e0 saisir'", 
            "title": "Probl\u00e9me3: probl\u00e8me d'installation des paquets JetPack."
        }, 
        {
            "location": "/sceances/#probleme4-probleme-de-flash-de-la-carte", 
            "text": "la machine n'arrive pas \u00e0 donner une adresse ip \u00e0 la carte car celle-ci n'est pas connu par le r\u00e9seau. Nous avons essayer plusieurs techniques en vain. Il a fallu faire part \u00e0 l'administrateur reseau pour r\u00e9soudre ce probleme. Ci-dessous quelques commande qu'on a utilis\u00e9 mais qui n'ont pas abouti.  sudo dhclient eth0\nifconfig eth0 192.168.0.1\ngestion de l'adresse ip: enter cette commande sur un terminal dans la machine hote apr\u00e8s qu'elle ai \u00e9t\u00e9 demarr\u00e9.\nsetxkbmap fr", 
            "title": "Probl\u00e8me4: probl\u00e8me de flash de la carte."
        }, 
        {
            "location": "/sceances/#seance-6-installation-et-flashage-de-la-carte-jetson-tk-1", 
            "text": "Nous avons recommencer les etapes de la s\u00e9ance 3 qui se sont interrompu\u00e9s \u00e0 cause du proxy et continuer. JETSON TK-1 telecharge plusieurs fichiers depuis internet pour son fonctionnement interne. Mais il s'arr\u00eate \u00e0 cause du proxy.  Nous \u00e9tions oblig\u00e9 de suspendre l'op\u00e9ration juste apr\u00e8s le flashage et de recommencer \u00e0 partir de la postInstallation.\n\u00e0 la fin du Flah, la machine donne \u00e0 la carte une adresse ip que voici 172.18.41.199.\nApr\u00e8s avoir Flasher la carte, nous allons ajouter les proxy de la fac en editant le fichier .bashrc.\nMaintenant, nous pouvons lancer la postInstallation qui va telecharger des paquets depuis internet. Toutesfois, certains telechargements n'ont pas march\u00e9. Nous avons fait \u00e7a manuellement dans la carte avant de pouvoir continuer.\nVoici entre autres quelques biblioth\u00e9ques t\u00e9l\u00e9charg\u00e9es par JETPACK L4T:   Common  Tegra Graphics Debugger  Tegra System Profiler  Documentation  VisionWorks pack On Host  VisionWorks on Host  VisionWorks Plus On Target  VisionWorks Objects Trackers on Host  VisionWorks References      GameWorks Samples   For Jetson TK1  CUDA Toolkit for Ubuntu 14.04  Linux for Tegra(TK1)  CUDA Toolkit for L4T  Compile GameWorks Sample  Compile CUDA Sample  PerfKit CuDNN Package  OpenCV for Tegra  VisionWorks Pack On TK1 Target  VisionWorks On Target  VisionWorks Plus(SFM) On Target  VisionWorks Objects Tracking On Target", 
            "title": "seance 6: Installation et Flashage de la carte JETSON TK-1."
        }, 
        {
            "location": "/sceances/#probleme5-probleme-de-flash-de-la-carte-1", 
            "text": "Pour resoudre definitivement le probl\u00e8me de proxy, nous avons editer le fichier .bashrc et nous lui avons ajouter \u00e0 la fin les lignes suivantes.  http_proxy \"adresse\"\nftp_proxy \"adresse\"\nhttps_proxy \"adresse\"\nApr\u00e8s avoir resolu ce probl\u00e8me l'installation ne s'interromp pas et le flashage est effectif.", 
            "title": "Probl\u00e8me5: probl\u00e8me de flash de la carte (1)."
        }, 
        {
            "location": "/sceances/#seance-7activation-des-4-cpu-de-la-carte", 
            "text": "Par defaut un seul CPU est actif. Pour activer les 3 autres, nous utilisons les commandes ci-dessous.  echo 0   /sys/devices/system/cpu/cpuquiet/tegra_cpuquiet/enable\necho 1   /sys/devices/system/cpu/cpu0/online\necho 1   /sys/devices/system/cpu/cpu1/online\necho 1   /sys/devices/system/cpu/cpu2/online\necho 1   /sys/devices/system/cpu/cpu3/online\necho performance   /sys/devices/system/cpu/cpu0/cpufreq/scaling_govern", 
            "title": "Seance 7:activation des 4 cpu de la carte."
        }, 
        {
            "location": "/sceances/#seance-8", 
            "text": "", 
            "title": "Seance 8:"
        }, 
        {
            "location": "/sceances/#seance-4-5-redaction-du-cahier-de-charge", 
            "text": "La redaction a \u00e9t\u00e9 faite sur Sharelatex.  Reunion avec les encadrants   quelques recherches concernant le probl\u00e8me rencontr\u00e9.", 
            "title": "S\u00e9ance 4 &amp; 5: Redaction du cahier de charge"
        }, 
        {
            "location": "/installations/", 
            "text": "Machine Host\n\n\nsite du telechargement d'ubuntu 14.04.* (version compatible jetpack le 14/08/16) :\nhttp://releases.ubuntu.com/trusty/\n\n\nchoisir la version compatible avec le processeur ( amd64/i386)\n\n\nubuntu-14.04.4-desktop-amd64.iso    \n\nubuntu-14.04.4-desktop-i386.iso\n\n\nsite de telechargement jetpack derni\u00e8re version disponible \nhttps://developer.nvidia.com/embedded/downloads\n\n\ndocumentation concernant la cr\u00e9ation de la cl\u00e9 bootable  :\n\n\nla cl\u00e9 bootable qe nous avons utilis\u00e9 \u00e0 \u00e9t\u00e9 cr\u00e9\u00e9e sur ubuntu 16.04 avec l'utilitaire Startup Disk Creator.\nla cl\u00e9 \u00e0 \u00e9t\u00e9 format\u00e9e au format FAT32.  \n\n\ninstallation de l'host :\n\n\nSur la machine fournie qui nous sert d'host : \n-\n acc\u00e8s au bios (afin de changer le boot order pour mettre la cl\u00e9 usb avant les autres) F10\n(la cl\u00e9 doit \u00eatre branch\u00e9e afin de la faire apparaitre dans le boot order)\n\n\nchoix de la langue : anglais\n\n\ninstall ubuntu\n\n-download update during installing (pas s\u00e9l\u00e9ctionner car internet nous faisait d\u00e9faut)\n-install third-party software\n\n-erase disk and install ubuntu\n-selectionner time-zone and keyboard configuration\n-ubuntu\n\n\n\npremier lancement de la jetson tk1 voir le QuickStartGuide : aucun soucis\n\n\ninstallation ubuntu  \n\n\n14.04.5-desktop-amd64.iso\n\n\nfaire un checksum \u00e0 l'aid de la commande md5 et comparer le num\u00e9ro avec celui sur le site d'ubuntu\n\n\nformattage en FAT32, start up disk creator\n\n\ninstallation normale\n\n\nutilisateur : m2\npwd     :   toto\n\n\nJetson TK1\n\n\nFlashage a l'aide de Nvidia Jetpack\nL4T OS", 
            "title": "Installations"
        }, 
        {
            "location": "/installations/#machine-host", 
            "text": "site du telechargement d'ubuntu 14.04.* (version compatible jetpack le 14/08/16) :\nhttp://releases.ubuntu.com/trusty/  choisir la version compatible avec le processeur ( amd64/i386)  ubuntu-14.04.4-desktop-amd64.iso     \nubuntu-14.04.4-desktop-i386.iso  site de telechargement jetpack derni\u00e8re version disponible \nhttps://developer.nvidia.com/embedded/downloads  documentation concernant la cr\u00e9ation de la cl\u00e9 bootable  :  la cl\u00e9 bootable qe nous avons utilis\u00e9 \u00e0 \u00e9t\u00e9 cr\u00e9\u00e9e sur ubuntu 16.04 avec l'utilitaire Startup Disk Creator.\nla cl\u00e9 \u00e0 \u00e9t\u00e9 format\u00e9e au format FAT32.    installation de l'host :  Sur la machine fournie qui nous sert d'host : \n-  acc\u00e8s au bios (afin de changer le boot order pour mettre la cl\u00e9 usb avant les autres) F10\n(la cl\u00e9 doit \u00eatre branch\u00e9e afin de la faire apparaitre dans le boot order)  choix de la langue : anglais  install ubuntu\n\n-download update during installing (pas s\u00e9l\u00e9ctionner car internet nous faisait d\u00e9faut)\n-install third-party software\n\n-erase disk and install ubuntu\n-selectionner time-zone and keyboard configuration\n-ubuntu  premier lancement de la jetson tk1 voir le QuickStartGuide : aucun soucis  installation ubuntu    14.04.5-desktop-amd64.iso  faire un checksum \u00e0 l'aid de la commande md5 et comparer le num\u00e9ro avec celui sur le site d'ubuntu  formattage en FAT32, start up disk creator  installation normale  utilisateur : m2\npwd     :   toto", 
            "title": "Machine Host"
        }, 
        {
            "location": "/installations/#jetson-tk1", 
            "text": "Flashage a l'aide de Nvidia Jetpack\nL4T OS", 
            "title": "Jetson TK1"
        }
    ]
}