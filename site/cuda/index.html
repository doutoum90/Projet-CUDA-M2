<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Mathias DIDIER">
  <link rel="shortcut icon" href="../favicon.ico">
  
  <title>CUDA - Développement d’applications sous processeur graphique (GPU / CUDA).</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "CUDA";
    var mkdocs_page_input_path = "cuda.md";
    var mkdocs_page_url = "/cuda/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Développement d’applications sous processeur graphique (GPU / CUDA).</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../introduction/">Introduction</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../tk1/">Jetson TK1</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../connexions/">Connexions</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../cpu_gpu/">CPU/GPU</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../architecture/">Architectures</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">CUDA</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#cuda">CUDA</a></li>
                
            
                <li class="toctree-l3"><a href="#coeurs-cuda">Coeurs CUDA</a></li>
                
            
                <li class="toctree-l3"><a href="#blockthread">Block/Thread</a></li>
                
                    <li><a class="toctree-l4" href="#exemple-de-code-cuda">Exemple de code CUDA</a></li>
                
                    <li><a class="toctree-l4" href="#en-detail">En Détail</a></li>
                
            
                <li class="toctree-l3"><a href="#alternatives">Alternatives</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../frameworks/">Frameworks</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../tensorflow/">Tensorflow</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../applications/">Applications</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../traitementImage/">Traitement d'image</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../optimisation/">Optimisation GPU</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../reduction/">Optimisation/Reduction</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../signRecognition/">Reconnaissance des signes</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../ressources/">Ressources</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../outils/">Outils</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../about/">About</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../sceances/">Scéances</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../installations/">Installations</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Développement d’applications sous processeur graphique (GPU / CUDA).</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>CUDA</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/matEhickey/Projet-CUDA-M2/edit/master/docs/cuda.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="cuda">CUDA</h1>
<p><img alt="Nvidia CUDA logo" src="http://images.anandtech.com/doci/6839/nvidia-cuda2.png" /><br />
CUDA (Compute Unified Device Architecture), est une technologie développé par NVIDIA pour ses cartes graphiques en 2007.<br />
Elle permet d'accéder a la puissance de la carte graphique via une API C, ou des bindings dans des langages plus haut niveau (python-cuda), et a des frameworks(Tensorflow, Caffee).  </p>
<p>Les concurrents de CUDA sont OpenCL, Larrabee, ATI Stream etc.. </p>
<h1 id="coeurs-cuda">Coeurs CUDA</h1>
<p>Les coeurs CUDA sont des unités de calculs pas si différentes que celles sur les cartes AMD (stream processor). Ces deux types d'unités de calculs excellent dans l'éxécution de programmes parallèles.</p>
<p>Les différences notables sont que les coeurs CUDA sont plus gros, plus complexe et tourne sur une fréquence plus élevé. Ainsi on ne peut pas comparer le nombre de coeur entre une carte NVIDIA et AMD car il faut plus d'unités de calculs dans les cartes AMD pour égaler les performances d'une carte NVIDIA.</p>
<p>Le compilateur CUDA fait moins d'optimisations et laisse la carte NVIDIA assigné les coeurs dont aura besoin le programme.
Un des points important en faveur de CUDA est le support qu'apporte NVIDIA, c'est pour cela qu'il y a un grand nombre de librairies disponibles pour CUDA.</p>
<h1 id="blockthread">Block/Thread</h1>
<p>Un programme (s'exécutant sur un CPU) peut avoir besoin d'appeler des calculs qui seront réalisés par des threads sur le GPU.<br />
Pour cela il doit s'arranger pour donner a la carte les zones mémoires ou sont stockés les données en input, et ou écrire les données résultantes.<br />
Ensuite, la fonction est exécuté par autant de threads que nécessaire.<br />
Les threads sont numérotés, et ils peuvent accéder a leur identifiant. Ainsi, il peut savoir quelle partie de la mémoire il peut manipuler.<br />
Les threads sont répartis dans des blocks CUDA. Cela sert a partagé la mémoire d'une façon plus efficace. En effet, il existe une hiérarchie d'accès à la mémoire.<br />
La mémoire propre du thread, la mémoire partagé par tout les blocks du thread, et enfin la mémoire globale.  </p>
<h2 id="exemple-de-code-cuda">Exemple de code CUDA</h2>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;

__global__
void mykernel(float *A1, float *A2, float *R)
{
    int p = threadIdx.x;
    R[p] = A1[p] + A2[p];
}

int main()
{
    float A1[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 };
    float A2[] = { 10, 20, 30, 40, 50, 60, 70, 80, 90 };
    float R[9];
    int taille_mem = sizeof(float) * 9;

    // on alloue de la memoire sur la carte graphique
    float *a1_device;
    float *a2_device;
    float *r_device;
    cudaMalloc((void**) &amp;a1_device, taille_mem);
    cudaMalloc((void**) &amp;a2_device, taille_mem);
    cudaMalloc((void**) &amp;r_device, taille_mem);

    // on copie les donnees sur la carte
    cudaMemcpy(a1_device, A1, taille_mem, cudaMemcpyHostToDevice);
    cudaMemcpy(a2_device, A2, taille_mem, cudaMemcpyHostToDevice);

    //9 additions, aucune boucle !
    mykernel&lt;&lt;&lt;1, 9&gt;&gt;&gt;(a1_device, a2_device, r_device);

    // on recupere le resultat
    cudaMemcpy(R, r_device, taille_mem, cudaMemcpyDeviceToHost);
    // sortie à l'ecran
    for(int i = 0; i &lt; 9; i++) {
        printf("%f\n", R[i]);
    }
}
</code></pre>
<h2 id="en-detail">En Détail</h2>
<p>Un programme CUDA est composé de deux composants primaire : un hôte (host) et un device (le GPU). 
Le code Host tourne sur le CPU, tandis que les fonctions kernel tournent sur le GPU.
L'execution du kernel peut être réalisée totalement indépendament de l'execution de l'host.</p>
<p><img alt="" src="../img/host_device.png" title="Host and device" /> </p>
<p>Une application commence par executer le concernant l'host CPU, ensuite l'host invoque un kernel GPU sur le device GPU.Le GPu execute ce kernel en parallel en utilisant de multiples threads.Lorsque le kernel completeson exécution, le CPU reprend son execution du programme d'origine.
(il est possible d'executer plusieur kernel ou bien de ne pas attendre sa fin pour continuer l'exeuction du host, nous détaillerons ceci dans la partie Stream et synchronisation).</p>
<h3 id="kernel">Kernel</h3>
<p>CUDA C hérite du C en permettant aux programmers de définir des fonction C, appelées kernels, qui lorsqu'elle sont appellées, sont executées N fois en parallele par N différents threads CUDA, contrainrement à une fois seulement en C.</p>
<h4 id="definir-un-kernel">Définir un kernel</h4>
<p>Un kenerl est définit en utilisant le mot-clé <strong>global</strong>.</p>
<pre><code>
// Definition du Kernel
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

</code></pre>

<h4 id="invoquer-un-kernel">Invoquer un Kernel</h4>
<p>Invoquer un kernel GPU est très similaire à l'appel d'une fonction. CUDA utilise une synthaxe à base de chevron afin de configurer et d'exectuer un kernel. 
Il faut donc lui indiquer le nombre de blocs et le nombre de threads à utiliser.</p>
<pre><code>
int main()
{
    ...
    // Invocation de Kernel avec N threads sur 1 bloc
    VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);
    ...
}

</code></pre>

<p>Ici chaqu'un des N threads executant VecAdd() performent une addition.</p>
<h3 id="grid">Grid</h3>
<p>La grid (grille) est le nom d'un kernel actif. Lorsque l'on lance plusieurs kernels, chacun à son propre espace de travail, sa grid</p>
<p><img alt="" src="../img/grid.png" title="Grid" /></p>
<h3 id="blocs">Blocs</h3>
<p>Un bloc est un ensemble limité de threads (multiple de 2).
Les blocs composant une grid sont totalement indépendants. Différents blocs sont assignés à différents microprocesseurs (SM pour streaming multiprocesors).
Plusieurs blocs peuvent être présent sur le même SM, mais un bloc ne peut pas être distribué sur plusieurs SM en même temps.
Les threads utilisés dans un bloc peuvent :
    <em>Se synchroniser ( à l'aide de la fonction __syncthreads qui agit comme une barriere tant que tout les threads du même bloc ne l'ont pas atteinte)
    </em>Partager de la mémoire (shared memory)
    *Communiquer</p>
<h3 id="parralellisme">Parraléllisme</h3>
<p>L'indépendance des blocs et des kernel permet à CUDA d'être plus évolutifs et plus flexibles. CUDA peut donc supporter plusieurs formes de parralélisme tel que :
<em>Parraléllisme au niveau des threads : Différents threads exectuent différentes tâches.
</em>Parraléllisme au niveau des blocs et des grids : Différents blocs ou grids executent différentes tâches.
*Parraléllisme de données : Différents threads and blocks travaillent sur différentes parties de la mémoire.</p>
<h3 id="thread-id">Thread ID</h3>
<p>Chaque bloc et chaque thread disposent d'un index qui leur est propre, uniquement accesible depuis un kernel actif.
Les variables blockIdx et threadIdx contiennent ces index.
threadIdx est un vecteur à trois dimension, tandis que blockIdx est un vecteur à 2 dimensions. L'index threadIdx est donc lui-même le bloc de threads.
La taille de la grid et d'un bloc est définie respectivement par les variables gridDim et blockDim.</p>
<h3 id="warp">Warp</h3>
<p>Les warps sont des groupes de threads consécutifs. Un warp est executé par un seul coeur CUDA.
A l'execution, un bloc de threads est divisé en un nombre de warp égal au nombre de coeur CUDA dans le SM.
La taille des warps dépend du matériel, par exemple pour la jetson TK1 chaque warp contient 32 threads.
Ce sont ces warps qui nous permettent d'obtenir un style d'execution SMT (Single Instruction Multiple-Thread).
Contrairement au grids et aux blocs, l'implémentation des warps n'est pas accesible aux programmeurs. Cependant, ce modèle d'execution influe sur les performances (plus de détail dans la section Optimisation).</p>
<h3 id="hierarchie-de-la-memoire">Hierarchie de la mémoire</h3>
<p>Les threads CUDA ont accès à différents espaces de la mémoire lors de leurs éxecutions. 
Chaque thread dispose d'un espace mémoire privé qui lui est propre.
Chaque thread peut aussi accéder à un espace de mémoire partagée avec tout les threads du même bloc ainsi qu'à la mémoire globale (partagée par tout les kernels).</p>
<p><img alt="" src="../img/" /></p>
<p>L'hote et le device maintiennent leurs propres espaces mémoires en DRAM, réfférés en temps qu'host memory et device memory.</p>
<h3 id="thrust-vector">Thrust vector</h3>
<p>Thrust est une librarie basée sur la STL (Standard Template Library). Elle offre une interface haut-niveau permettant de programmer plus facilement des applications CUDA.</p>
<p>Thrust fournit deux containeurs vectoriels : host_vector (stocké sur la mémoiré de l'host) et device_vector (stocké sur la mémoire du GPU).
Ces vecteurs fonctionnent de la même manière que les std::vector disponibles en C++ STL : 
ce sont des conteneurs géneriques (tout type de donnée) pouvant être redimensionnés dynamiquement.
Lorsqu'une fonction Thrust est appellée il vérifie automatique le type d'itérateur utilisé pour déterminer si il doit utiliser une implémentation host ou device.
Ce processus est connu sous le nom de "static dispatching" car  l"implémentation host/device est determinée durant la compilation.</p>
<p>Nous pouvons utiliser des raw_pointers pour accèder aux données sans utiliser les itérateurs.</p>
<h3 id="capacite-de-calcul">Capacité de calcul</h3>
<p>La capacité de calcul d'un appareil est représenté par son numéro de version (parfois appellé "SM version").
Ce numéro de version permet d'identifier le nombre de feature supporté par le hardware GPU et est utilisé par les application à l'éxecution pour déterminer les outils et instructions diposnibles pour le GPU.</p>
<p>Les appareils ayant la même architecture diposent du même niveau de revision.
Liste des capacités selon les versions : http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities</p>
<h1 id="alternatives">Alternatives</h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/OpenCL">OpenCL</a><br />
Combinaisons d'API CPU multi-coeurs / GPU<br />
<a href="https://developer.nvidia.com/opencl">Site Nvidia</a></li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../frameworks/" class="btn btn-neutral float-right" title="Frameworks">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../architecture/" class="btn btn-neutral" title="Architectures"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/matEhickey/Projet-CUDA-M2" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../architecture/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../frameworks/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
