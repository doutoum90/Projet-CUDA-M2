<-- source : https://cvw.cac.cornell.edu/gpu -->

#Structure de CUDA 

##Programming Structure

Un programme CUDA est composé de deux composants primaire : un hôte (host) et un device (le GPU). 
Le code Host tourne sur le CPU, tandis que les fonctions kernel tournent sur le GPU.
L'execution du kernel peut être réalisée totalement indépendament de l'execution de l'host.

![](/img/host_device.png "Host and device")	

Une application commence par executer le concernant l'host CPU, ensuite l'host invoque un kernel GPU sur le device GPU.Le GPu execute ce kernel en parallel en utilisant de multiples threads.Lorsque le kernel completeson exécution, le CPU reprend son execution du programme d'origine.
(il est possible d'executer plusieur kernel ou bien de ne pas attendre sa fin pour continuer l'exeuction du host, nous détaillerons ceci dans la partie Stream et synchronisation).

##Kernel

CUDA C hérite du C en permettant aux programmers de définir des fonction C, appelées kernels, qui lorsqu'elle sont appellées, sont executées N fois en parallele par N différents threads CUDA, contrainrement à une fois seulement en C.

###Définir un kernel

Un kenerl est définit en utilisant le mot-clé __global__.

~~~

// Definition du Kernel
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

~~~

###Invoquer un Kernel
Invoquer un kernel GPU est très similaire à l'appel d'une fonction. CUDA utilise une synthaxe à base de chevron afin de configurer et d'exectuer un kernel. 
Il faut donc lui indiquer le nombre de blocs et le nombre de threads à utiliser.

~~~

int main()
{
    ...
    // Invocation de Kernel avec N threads sur 1 bloc
    VecAdd<<<1, N>>>(A, B, C);
    ...
}

~~~

Ici chaqu'un des N threads executant VecAdd() performent une addition.


##Grid

La grid (grille) est le nom d'un kernel actif. Lorsque l'on lance plusieurs kernels, chacun à son propre espace de travail, sa grid

![](/img/grid.png "Grid")

##Blocs
Un bloc est un ensemble limité de threads (multiple de 2).
Les blocs composant une grid sont totalement indépendants. Différents blocs sont assignés à différents microprocesseurs (SM pour streaming multiprocesors).
Plusieurs blocs peuvent être présent sur le même SM, mais un bloc ne peut pas être distribué sur plusieurs SM en même temps.
Les threads utilisés dans un bloc peuvent :
    *Se synchroniser ( à l'aide de la fonction __syncthreads qui agit comme une barriere tant que tout les threads du même bloc ne l'ont pas atteinte)
    *Partager de la mémoire (shared memory)
    *Communiquer

##Parraléllisme

L'indépendance des blocs et des kernel permet à CUDA d'être plus évolutifs et plus flexibles. CUDA peut donc supporter plusieurs formes de parralélisme tel que :
*Parraléllisme au niveau des threads : Différents threads exectuent différentes tâches.
*Parraléllisme au niveau des blocs et des grids : Différents blocs ou grids executent différentes tâches.
*Parraléllisme de données : Différents threads and blocks travaillent sur différentes parties de la mémoire.

##Thread ID

Chaque bloc et chaque hread disposent d'un index qui leur est propre, uniquement accesible depuis un kernel actif.
Les variables blockIdx et threadIdx contiennent ces index.
threadIdx est un vecteur à trois dimension, tandis que blockIdx est un vecteur à 2 dimensions. L'index threadIdx est donc lui-même le bloc de threads.
La taille de la grid et d'un bloc est définie respectivement par les variables gridDim et blockDim.

##Warp

Les warps sont des groupes de threads consécutifs. Un warp est executé par un seul coeur CUDA.
A l'execution, un bloc de threads est divisé en un nombre de warp égal au nombre de coeur CUDA dans le SM.
La taille des warps dépend du matériel, par exemple pour la jetson TK1 chaque warp contient 32 threads.
Ce sont ces warps qui nous permettent d'obtenir un style d'execution SMT (Single Instruction Multiple-Thread).
Contrairement au grids et aux blocs, l'implémentation des warps n'est pas accesible aux programmeurs. Cependant, ce modèle d'execution influe sur les performances (plus de détail dans la section Optimisation).




